\section{Method} \label{sec: method}



\subsection{Overall Framework} \label{sec: overall framework}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/imf_encoder.pdf}
	\vspace{10pt}
	\caption{An illustration of the encoder for our image compression method, based on integer matrix factorization.}
	\label{fig:imf_encoder}
\end{figure}

Figure \ref{fig:imf_encoder} illustrates an overview of the encoding pipeline for our proposed image compression method using integer matrix factorization (IMF). The encoder accepts an RGB image with dimensions $H \times W$ and a color depth of 8 bits, represented by the tensor $\bm{\mathcal{X}} \in \{0, \ldots, 255\}^{3 \times H \times W}$. Each step of encoding is described in the following.

\paragraph{Color Space Transformation.}
Analogous to the JPEG standard, the image is initially transformed into the YC\textsubscript{B}C\textsubscript{R} color space. Let $\bm{Y} \in [0, 255]^{H \times W}$ represent the \emph{luma} component, and $\bm{C}_B, \bm{C}_R \in [0, 255]^{\frac{H}{2} \times \frac{W}{2}}$ represent the blue-difference and red-difference \emph{chroma} components, respectively. Note that as a result of this transformation, the elements of the \emph{luma} ($\bm{Y}$) and \emph{chroma} ($\bm{C}_B$, $\bm{C}_R$) matrices are not limited to integers and can take any value within the interval $[0, 255]$.

%\begin{equation} \label{eq: ycbcr transform}
%	\begin{bmatrix}
	%		Y \\
	%		C_B \\
	%		C_R
	%	\end{bmatrix} \triangleq
%	\begin{bmatrix}
	%		0.299 & 0.587 & 0.114 \\
	%		-0.168736 & -0.331264 & 0.5 \\
	%		0.5 & -0.418688 & -0.081312
	%	\end{bmatrix}
%	\begin{bmatrix}
	%		R \\
	%		G \\
	%		B
	%	\end{bmatrix}
%	+
%	\begin{bmatrix}
	%		0 \\
	%		128 \\
	%		128
	%	\end{bmatrix},
%\end{equation}

\paragraph{Chroma Downsampling.} 
After conversion to the YC\textsubscript{B}C\textsubscript{R} color space, the \emph{chroma} components $\bm{C}_B$ and $\bm{C}_R$ are downsampled using average-pooling with a kernel size of $(2, 2)$ and a stride of $(2, 2)$, similar to the process used in JPEG. This downsampling exploits the fact that the human visual system perceives far more detail in brightness information (\emph{luma}) than in color saturation ({chroma}).

\paragraph{Patchification.}
Following \emph{chroma} downsampling, we have three components:  the \emph{luma} component $\bm{Y} \in [0, 255]^{H \times W}$ and the \emph{chroma} components $\bm{C}_B, \bm{C}_R \in [0, 255]^{\frac{H}{2} \times \frac{W}{2}}$. Each of the matrices is split into non-overlapping $8 \times 8$ patches. If a dimension of a matrix is not divisible by 8, the matrix is first padded to the nearest size divisible by 8 using reflection of the boundary values. These patches are then flattened into row vectors and stacked vertically to form matrices $\bm{X}_{Y} \in [0, 255]^{\frac{HW}{64} \times 64}$, $\bm{X}_{C_B} \in [0, 255]^{\frac{HW}{256} \times 64}$, and $\bm{X}_{C_R} \in [0, 255]^{\frac{HW}{256} \times 64}$. Later, these matrices will be low-rank approximated using integer matrix factorization (IMF). Note that this patchification technique differs from the block splitting in JPEG, where each block is subject to discrete cosine transform (DCT) and processed independently. This patchification technique not only captures the locality and spatial dependencies of neighboring pixels but also performs better with the matrix decomposition approach to image compression.

\paragraph{Low-rank approximation.} 
We now apply a low-rank approximation to the matrices $\bm{X}_{Y}$, $\bm{X}_{C_B}$, and $\bm{X}_{C_R}$, which is the core of our compression method that provides a lossy compressed representation of these matrices.  The low-rank approximation \citep{eckart1936approximation} aims to approximate a given matrix $ \mathbf{X} \in \mathbb{R}^{M \times N} $ by 
\begin{equation} \label{eq: lra}
	\bm{X} \approx \bm{U} \bm{V}^\mathsf{T} = \sum_{r=1}^{R} U_{:r} {V_{:r}}^\mathsf{T},
\end{equation} 
where $\bm{U} \in \mathbb{R}^{M \times R}$ and $\bm{V} \in \mathbb{R}^{N \times R}$ are \emph{factor matrices} (or simply \emph{factors}), and $R \leq \min(M,N)$ represents the \emph{rank}. We refer to $\bm{U}$ as the \emph{basis matrix} and $\bm{V}$ as the \emph{coefficient matrix}. By selecting a sufficiently small value for $R$, the factor matrices $\bm{U}$ and $\bm{V}$, with a combined total of $(M+N)R$ elements, offer a compact representation of the original matrix $\mathbf{X}$, which has $MN$ elements, capturing the most significant patterns in the image. Depending on the loss function used to measure the reconstruction error between $\mathbf{X}$ and the product $\bm{U} \bm{V}^\mathsf{T}$, as well as the constraints on the factor matrices $\bm{U}$ and $\bm{V}$, various formulations and variants have been proposed for different purposes. In Section \ref{sec: imf}, we introduce and elaborate on our variant, termed integer matrix factorization (IMF), and argue why it is well-suited and effective for image compression. 

\paragraph{Reshape factors.} 
IMF yields integers factor matrices $\bm{U}_{Y} \in \{0, \ldots, 255\}^{\frac{HW}{64} \times R}$ and $\bm{V}_{Y} \in \{0, \ldots, 255\}^{64 \times R}$; $\bm{U}_{C_B} \in \{0, \ldots, 255\}^{\frac{HW}{256} \times R}$ and $\bm{V}_{C_B} \in \{0, \ldots, 255\}^{64 \times R}$; and $\bm{U}_{C_R} \in \{0, \ldots, 255\}^{\frac{HW}{256} \times R}$ and $\bm{V}_{C_R} \in \{0, \ldots, 255\}^{64 \times R}$ that correspond to $\bm{X}_{Y}$, $\bm{X}_{C_B}$, and $\bm{X}_{C_R}$. We reshape these matrices by unfolding their first dimension to obtain $R$-channel 2D spatial maps, referred to as \emph{factor maps} and represented by the following tensors:
\begin{align} \label{eq: reshaped factors}
	\bm{\mathcal{U}}_{Y} & \in \{0, \ldots, 255\}^{R \times \frac{H}{8} \times \frac{W}{8}}, \nonumber \\
	\bm{\mathcal{U}}_{C_B}, \bm{\mathcal{U}}_{C_R} & \in \{0, \ldots, 255\}^{R \times \frac{H}{16} \times \frac{W}{16}}, \nonumber \\
	\bm{\mathcal{V}}_{Y}, \bm{\mathcal{V}}_{C_B}, \bm{\mathcal{V}}_{C_R} & \in \{0, \ldots, 255\}^{R \times 8 \times 8}.
\end{align}

\paragraph{Lossless image compression.}
Since \emph{factor maps} are already integer tensors, we do not need a quantization step, which is commonly present in other lossy image compression methods and adds extra complications. This is the main advantage of our approach. Moreover, each channel of a \emph{factor map} can be treated as a 8-bit grayscale image and therefore can be directly encoded by any standard lossless image compression method, such as PNG or WebP. For images with a resolution of $H, W \gg 64$, which are most common nowadays, the basis maps ($\bm{\mathcal{U}}$) are significantly larger than the coefficient maps ($\bm{\mathcal{V}}$), accounting for the majority of the storage space. Interestingly, in practice, the IMF basis maps turn out to be meaningful images, each capturing some visual semantic of the image (see Figure \ref{fig:imf_components} for an example). Therefore, our IMF approach can effectively leverage the power of existing lossless image compression algorithms, offering a significant advantage over current methods.


\subsection{Integer Matrix Factorization (IMF)} \label{sec: imf}

The main building block of our method is integer matrix factorization (IMF), which is responsible for lossy compression of matrices obtained through patchification. IMF can be framed as an optimization problem, aiming to minimize the reconstruction error between the original matrix $\bm{X} \in \mathbb{R}^{M \times N}$ and the product $\bm{U} \bm{V}^\mathsf{T}$, while ensuring that the elements of the factor matrices $\bm{U}$ and $\bm{V}$ are integers within a specified interval $[a,b]$. Formally, the IMF problem can be expressed as:
\begin{align} \label{eq: imf problem}
	\min_{\bm{U}, \bm{V}} & \ \| \bm{X} - \bm{U} \bm{V}^\mathsf{T} \|_\text{F}^2 \nonumber \\
	\text{s.t.}           & \ \bm{U} \in \mathbb{Z}_{[a,b]}^{M \times R}, \bm{V} \in \mathbb{Z}_{[a,b]}^{N \times R},
\end{align}
where $\|\cdot\|_\text{F}$ denotes the Frobenius norm; $R \leq \min(M,N)$ represents the \emph{rank}; and $\mathbb{Z}_{[a,b]} \triangleq [a, b] \cap \mathbb{Z}$ denotes the set of integers within $[a,b]$. Without constraints on the factors, the problem would have an analytic solution through the singular value decomposition (SVD), as addressed by the Eckart–Young–Mirsky theorem \cite{eckart1936approximation}. If only a nonnegativity constraint were applied (without integrality), variations of nonnegative matrix factorization (NMF) would emerge \cite{lee2000algorithms, gillis2020nonnegative}. The IMF problem \eqref{eq: imf problem} poses a challenging integer program, with finding its global minima known to be NP-hard \cite{dong2018integer, van1981another}. Only a few iterative algorithms \cite{dong2018integer, lin2005integer} have been proposed to find a ``good solution" for some IMF variants in contexts other than image compression. In Section \ref{sec: bcd}, we propose an efficient iterative algorithm for the IMF problem \eqref{eq: imf problem}.

The application of SVD and NMF in image compression is problematic mainly because the resulting factors contain continuous values that must be represented as arrays of floating-point numbers. This necessitates a quantization step that not only adds extra complications but also significantly degrades compression performance due to quantization errors (as demonstrated in Section \ref{sec: experiments}). Conversely, our IMF formulation produces integer factor matrices that can be directly stored and losslessly processed without incurring roundoff errors. The reason for limiting the feasible region to $[a,b]$ in our IMF formulation is to enable more compact storage of the factors using standard integral data types, such as \texttt{int8} and \texttt{int16}, supported by programming languages. Given that the elements of the input matrix $\bm{X}$ are in $[0, 255]$, we found the signed \texttt{int8} type, which represents integers from -128 to 127, suitable for image compression applications. As a result, our IMF formulation is well-suited for image compression, effectively integrating the factorization and quantization steps into a single, efficient compression process.


\subsection{Block Coordinate Descent Scheme for IMF} \label{sec: bcd}

Here we propose a simple yet efficient algorithm based on the block coordinate descent (BCD) scheme (aka alternating optimization), where the objective function is iteratively minimized only with respect to a single column of a factor while the other factor and the  is kept fixed at a time.



\begin{algorithm}[!t]
	\caption{Hierarchical alternating least squares (HALS) for NMF. \label{alg:lrf:hals for nmf}}
	\DontPrintSemicolon
	\SetAlgoLined
	\KwIn{Nonnegative matrix $ \bm{X} \in \mathbb{R}_{\ge 0}^{M \times N} $, factorization rank $ R $}
	\KwOut{Factor matrices $\bm{F} \in \mathbb{R}_{\ge 0}^{M \times R}$ and $\bm{G} \in \mathbb{R}_{\ge 0}^{N \times R}$; $\bm{X} \approx \bm{F} \bm{G}^\mathsf{T}$}
	Initialize $\bm{F}^{0} \in \mathbb{R}_{\ge 0}^{M \times R}$, $\bm{G}^{0} \in \mathbb{R}_{\ge 0}^{N \times R}$\;
	\While{stopping criterion not satisfied}{
		$\bm{A} \gets \bm{X} \bm{G}$,\ $\bm{B} \gets \bm{G}^\mathsf{T} \bm{G}$\;
		\For{$r = 1, \ldots, R$}{
			$\displaystyle F_{:r} \gets \max \left(0, \frac{A_{:r} - \sum_{s \neq r} B_{sr} F_{:s}}{\| G_{:r} \|^2} \right)$\;
		}
		$\bm{A} \gets \bm{X}^\mathsf{T} \bm{F}$; $\bm{B} \gets \bm{F}^\mathsf{T} \bm{F}$\;
		\For{$r = 1, \ldots, R$}{
			$\displaystyle G_{:r} \gets \max \left(0, \frac{A_{:r} - \sum_{s \neq r} B_{sr} G_{:s}}{\| G_{:r} \|^2} \right)$\;
		}
	}
	\Return $(\bm{F}, \bm{G})$
\end{algorithm}




\begin{theorem}  \label{the: bcd monotonicity}
The IMF cost function, $\| \bm{X} - \bm{U} \bm{V}^\mathsf{T} \|_\text{F}^2$, is monotonically nonincreasing under each of the multiplicative update rules.
\end{theorem}

\begin{proof}
See Appendix \ref{app: proof} for the proof.
\end{proof}


\subsection{Implementation Details} \label{sec: implementation details}