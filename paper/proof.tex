\section{Proof of Theorem \ref{the: bcd monotonicity}} \label{app: proof}

\begin{theorem}[Global convergence]
    Let $\seq{\bm U^k}[k\in\N]$ and $\seq{\bm V^k}[k\in\N]$ be sequences generated by the proposed Algorithm \ref{alg:lrf:hals for nmf}. Then both sequences are convergent to a locally optimal point of the optimization problem \eqref{eq: imf problem}.
\end{theorem}

\begin{proof}
    To study the convergence of the proposed Algorithm \ref{alg:lrf:hals for nmf}, we recast the optimization problem \eqref{eq: imf problem} to the following equivalent problem:
    \begin{align}
        \begin{split}
            \minimize_{\bm U \in \R^{M\times R}, \bm V \in \R^{N\times R}} \Psi(\bm U, \bm V) &\coloneqq H(\bm U, \bm V) + f(\bm U) + g(\bm V),\\
            \text{where} \quad~~~ H(\bm U, \bm V) &\coloneqq \|\bm X - \bm U \bm V^{\rm T} \|_{\rm F}^2,\\
            f(\bm U) &\coloneqq \delta_{[a,b]}(\bm U) + \delta_\Z(\bm U),\\
            g(\bm V) &\coloneqq \delta_{[a,b]}(\bm V) + \delta_\Z(\bm V),
        \end{split}
        \label{eq:IMF_surrogate}
    \end{align}
    with $\delta_\mathcal{B}(\cdot)$ as the indicator function of the nonempty set $\mathcal{B}$ where $\delta_\mathcal{B}(\bm x) = 0$ if $\bm x \in \mathcal{B}$ and $\delta_\mathcal{B}(\bm x) = +\infty$, otherwise. By the definition of functions above, it is easy to confirm that the problems \eqref{eq: imf problem} and \eqref{eq:IMF_surrogate} are equivalent.
    
    The unconstrained optimization problem \eqref{eq:IMF_surrogate} consists of the sum of a differentiable (smooth) convex function $H$ with nonsmooth nonconvex functions $f$ and $g$. This problem has been extensively studied in the literature under the class of unconstrained nonconvex nonsmooth minimization problems. 
    One of the common algorithms applied to such a problem class is the well-known forward-backward-splitting (FBS) []. 
    In Algorithm \ref{alg:lrf:hals for nmf}, the variables $\bm U$ and $\bm V$ are updated sequentially following block coordinate (BC) descent minimization algorithms, also often called Gauss-Seidel updates or alternating minimization.
    Hence, in this convergence study we are interested in the algorithms that allow BC-type updates for the nonconvex nonsmooth problem of \eqref{eq:IMF_surrogate} []. Specifically, we focus on the proximal alternating linearized minimization (PALM) algorithm, relating its convergence behavior to that of Algorithm \ref{alg:lrf:hals for nmf}. 
    To that end, we show that the updates of Algorithm \ref{alg:lrf:hals for nmf} are equivalent to the updates of PALM on the recast problem of \eqref{eq:IMF_surrogate}, and all the assumptions necessary for the convergence of PALM is satisfied by our problem setting. 

    The PALM algorithm [] can be summarized as follows:
    \begin{enumerate}
        \item Initialize $\bm U^0 \in \R^{M\times R}$, $\bm V^0 \in \R^{N\times R}$ 
        \item For each iteration $k=0,1,...$ 
        \begin{align}
            \begin{split}
                \mysubnumber\quad \bm U^{k+1} &\in \prox^f_{c_k} \left(\bm U^k - \frac{1}{c_k} \nabla_{\bm U} H(\bm U^k, \bm V^k)\right), ~\text{with}~ c_k > L_1(\bm V^k)\\
                \mysubnumber\quad \bm V^{k+1} &\in \prox^g_{d_k} \left(\bm V^k - \frac{1}{d_k} \nabla_{\bm V} H(\bm U^{k+1}, \bm V^k)\right), ~\text{with}~ d_k > L_2(\bm U^{k+1})
            \end{split}
            \label{eq:palm_updates}
        \end{align}
    \end{enumerate}
    where the proximal map for an extended proper lower semicontinuous (nonsmooth) function $\func{\varphi}{\R^n}{(-\infty,+\infty]}$ and $\gamma > 0$ is defined as $\prox^\varphi_\gamma(\bm x) \coloneqq \argmin_{\bm w\in\R^n}\left\{\varphi(\bm w) + \frac{\gamma}{2} \|\bm w - \bm x\|^2_2\right\}$, $\nabla_x$ is the partial derivative with respect to $x$, and $L_1 > 0$, $L_2 > 0$ are local Lipschitz moduli, defined in the following assumption II.

    The following proposition investigates the necessary assumptions for convergence of PALM and Algorithm \ref{alg:lrf:hals for nmf}.
    \begin{prop}[Meeting required assumptions]\label{prop:assumptions}
        The assumptions necessary for the convergence of iterations in \eqref{eq:palm_updates} are satisfied by the functions involved in the problem \eqref{eq:IMF_surrogate}, specifically:
        \begin{enumerate}
            \item The indicator functions $\delta_{[a,b]}$ and $\delta_\Z$ are proper and lower semicontinuous functions, so do the functions $f$ and $g$;
            \item For any fixed $\bm V$, the partial gradient $\nabla_{\bm U} H(\bm U, \bm V)$ is globally Lipschitz continuous with modulus $L_1(\bm V) = \|\bm V^T \bm V\|_F$ defined by
            \begin{equation*}
                \|\nabla_{\bm U} H(\bm U_1, \bm V) - \nabla_{\bm U} H(\bm U_2, \bm V)\| \leq L_1(\bm V) \|\bm U_1 - \bm U_2\|, \quad \forall \bm U_1,\bm U_2 \in \R^{M\times R},
            \end{equation*}
            where $\|\cdot\|$ in this section denotes the $\ell_2$-norm of the vectorized input with the proper dimension (here, with the input in $\R^{MR\times 1}$).
            The similar Lipschitz continuity is evident for $\nabla_{\bm V} H(\bm U, \bm V)$ as well with modulus $L_2(\bm U) = \|\bm U \bm U^T\|_F$.
            \item The sequences $\bm U^k$ and $\bm V^k$ are bounded due to the indicator functions $\delta_{[a,b]}$ with bounded $a$ and $b$. Hence the moduli $L_1(\bm V^k)$ and $L_2(\bm U^k)$ are bounded from below and from above for all $k\in\N$.
            \item The function $H$ is twice differentiable, hence, its full gradient $\nabla H(\bm U,\bm V)$ is Lipschitz continuous on the bounded set $\bm U \in [a,b]^{M\times R}$, $\bm V \in [a,b]^{N\times R}$. Namely, with $M > 0$:
            \begin{align}
                \begin{split}
                    \|(\nabla_{\bm U} H(\bm U_1, \bm V_1) - \nabla_{\bm U} H(\bm U_2, \bm V_2), \nabla_{\bm V} H(\bm U_1, \bm V_1) - &\nabla_{\bm V} H(\bm U_2, \bm V_2))\| \\
                    &\leq M \|(\bm U_1 - \bm U_2, \bm V_1 - \bm V_2)\|,
                \end{split}
            \end{align}
            where $(\cdot,\cdot)$ denotes the concatination of the two arguments.
            \item The sets $[a,b]$ and integer numbers are semi-algebraic; so are their indicator functions. The function $H$ is also polynomial, hence it is semi-algebraic. The sum of these functions results in a semi-algebraic function $\Psi$ in \eqref{eq:IMF_surrogate}, hence $\Psi$ is a KL function.
        \end{enumerate}
    \end{prop}
    By Proposition \ref{prop:assumptions}, the optimization problem \eqref{eq:IMF_surrogate}---and equivalently the original problem \eqref{eq: imf problem}---can be solved by PALM, which is convergence to a locally optimal point, specifically a critical point $(\bm U^\star, \bm V^\star)$ where $0 \in \partial \Psi(\bm U^\star, \bm V^\star)$, with $\partial$ as the subdifferential of $\Psi$.

    In the following, we highlight that the updates in () can be implemented more simply and more efficiently by Algorithm \ref{alg:lrf:hals for nmf} for the problem of image compression. It is noted that the so-called \emph{forward} steps $\bm U^k - \frac{1}{c_k} \nabla_{\bm U} H(\bm U^k, \bm V^k)$ and $\bm V^k - \frac{1}{d_k} \nabla_{\bm V} H(\bm U^{k+1}, \bm V^k)$ in the $\prox$ operators can be replaced by the simple closed-form updates of Algorithm \ref{alg:lrf:hals for nmf} in steps () and (), respectively. This is thanks to the special form of the functions $H(\cdot, \bm V^k)$ and $H(\bm U^{k+1}, \cdot)$ being quadratic functions, each having a global optimal point which ensures a descent in each forward step. 
    Furthermore, the proximal operators $\prox^f_{c_k}$ and $\prox^g_{d_k}$ can efficiently be implemented by the steps () and () in Algorithm \ref{alg:lrf:hals for nmf}. The equivalence of these steps is proven in the following lemma.

    \begin{lem}[$\prox$ implementation]
        Define the following (elementwise) operators $\func{P_{[a,b]}}{\R}{\R}$, and $\func{T_\Z}{\R}{\R}$:
        \begin{align}
            P_{[a,b]}(x) &\coloneqq \min\{\max\{a, x\}, b\},\\
            T_\Z(x) &\coloneqq \max\{\lfloor x \rfloor, \lfloor x+0.5 \rfloor\}. \label{eq:Tz}
        \end{align}
        Then $\prox^f_{c_k}(\bm W) = T_\Z(P_{[a,b]}(\bm W))$ and $\prox^f_{d_k}(\bm Z) = T_\Z(P_{[a,b]}(\bm Z))$ for any $\bm W\in \R^{M\times R}$ and $\bm Z\in\R^{N\times R}$ and $T_\Z(P_{[a,b]}(\cdot))$ being an elementwise operator on the input matrices.
    \end{lem}
    \begin{proof}
        Define the following norms for a given matrix $\bm W \in \R^{M\times R}$:
        \begin{align*}
            \|\bm W\|_{[a,b]} \coloneqq \sum_{i,j \mid a \leq \bm W_{ij} \leq b} \bm W_{ij}^2, \quad 
            \|\bm W\|_a \coloneqq \sum_{i,j \mid \bm W_{ij} < a} \bm W_{ij}^2, \quad 
            \|\bm W\|_b \coloneqq \sum_{i,j \mid \bm W_{ij} > b} \bm W_{ij}^2.
        \end{align*}
        Moreover, note that the rounding operator in \eqref{eq:Tz} can be equivalently driven by the following proximal operator:
        \begin{equation}
            T_\Z(\bm W) = \argmin_{\bm U}\{\|\bm U - \bm W\|^2_F \mid \bm U \in \Z^{M\times R}\}.
            \label{eq:equivalence_prox_Tz}
        \end{equation}
        The proximal operator can $\prox^f_{c_k}(\bm W)$ can be rewritten as
        \begin{align*}
            \prox^f_{c_k}(\bm W) &= \argmin_{\bm U}\{\delta_{[a,b]}(\bm U) + \delta_{\Z}(\bm U) + \frac{c_k}{2} \|\bm U - \bm W\|^2_F\}\\
            &= \argmin_{\bm U}\{\|\bm U - \bm W\|^2_F \mid \bm U \in \Z_{[a,b]}^{M\times R}\}\\
            &= \argmin_{\bm U}\{\|\bm U - \bm W\|^2_{[a,b]} + \|\bm U - \bm A\|^2_a + \|\bm U - \bm B\|^2_b \mid \bm U \in \Z_{[a,b]}^{M\times R}\}\\
            &= \argmin_{\bm U}\{\|\bm U - \bm W\|^2_{[a,b]} + \|\bm U - \bm A\|^2_a + \|\bm U - \bm B\|^2_b \mid \bm U \in \Z^{M\times R}\}\\
            &= \argmin_{\bm U}\{\|\bm U - P_{[a,b]}(\bm W)\|^2_F \mid \bm U \in \Z^{M\times R}\}\\
            &= T_\Z(P_{[a,b]}(\bm W)).
        \end{align*}
        The first equality is due to the definition of $\prox$ which is equivalent to the second equality. 
        In the third equality the matrices $A\in\R^{M\times R}$ and $B\in\R^{M\times R}$ have elements all equal to $a$ and $b$, respectively.
        The third equality is due to the fact that replacing $\|\bm U - \bm W\|^2_a + \|\bm U - \bm W\|^2_b$ with $\|\bm U - \bm A\|^2_a + \|\bm U - \bm B\|^2_b$ has no effect on the solution of the minimization. The fourth equality is also trivial due to the involved norms in the third equality. The fifth equality can be easily confirmed by the definition of the projection $P_{[a,b]}$. Finally, in the last equality \eqref{eq:equivalence_prox_Tz} is invoked. A similar proof can be trivially followed for $\prox^f_{d_k}(\bm Z) = T_\Z(P_{[a,b]}(\bm Z))$ as well.
    \end{proof}
    Now that the equivalence of updates () with the simple and closed-form steps in Algorithm \ref{alg:lrf:hals for nmf} is fully established, and the assumptions required for the convergence are verified to be met by the problem \eqref{eq:IMF_surrogate} equivalent to the original problem \eqref{eq: imf problem}, the following proposition can be invoked to prove the convergence of Algorithm \ref{alg:lrf:hals for nmf}.
    \begin{prop}[Global convergence []]
        Let $\seq{(\bm U^k, \bm V^k)}$ be a sequence generated by Algorithm \ref{alg:lrf:hals for nmf}, then
        the sequence converges to a locally optimal point of problem \eqref{eq: imf problem} which is a critical point $(\bm U^\star, \bm V^\star)$ of the problem \eqref{eq:IMF_surrogate},
    \end{prop}
    




\end{proof}