num_classes: 200
image_size: 64
val_resize_size: 72
image_mean: [0.485, 0.456, 0.406]
image_std: [0.229, 0.224, 0.225]

batch_size: 256
val_batch_size: 512

train_transform:
    _target_: torchvision.transforms.v2.Compose
    transforms:
        - _target_: torchvision.transforms.v2.ToImage
        - _target_: torchvision.transforms.v2.RandomResizedCrop
          size: ${data.image_size}
          interpolation: bilinear
          antialias: true
        - _target_: torchvision.transforms.v2.RandomHorizontalFlip
          p: 0.5
        - _target_: torchvision.transforms.v2.TrivialAugmentWide
          interpolation: bilinear
        - _target_: torchvision.transforms.v2.ToDtype
          dtype:
              _target_: src.get_dtype
              name: float32
          scale: true
        - _target_: torchvision.transforms.v2.Normalize
          mean: ${data.image_mean}
          std: ${data.image_mean}
        - _target_: torchvision.transforms.v2.RandomErasing
          p: 0.1
        - _target_: torchvision.transforms.v2.ToPureTensor

train_batched_transform:
    _target_: torchvision.transforms.v2.RandomChoice
    transforms:
        - _target_: torchvision.transforms.v2.MixUp
          alpha: 0.2
          num_classes: ${data.num_classes}
        - _target_: torchvision.transforms.v2.CutMix
          alpha: 1.0
          num_classes: ${data.num_classes}

val_transform:
    _target_: torchvision.transforms.v2.Compose
    transforms:
        - _target_: torchvision.transforms.v2.ToImage
        - _target_: torchvision.transforms.v2.Resize
          size: ${data.val_resize_size}
          interpolation: bilinear
        - _target_: torchvision.transforms.v2.CenterCrop
          size: ${data.image_size}
        - _target_: torchvision.transforms.v2.ToDtype
          dtype:
              _target_: src.get_dtype
              name: float32
          scale: true
        - _target_: torchvision.transforms.v2.Normalize
          mean: ${data.image_mean}
          std: ${data.image_mean}
        - _target_: torchvision.transforms.v2.ToTensor

train_set:
    _target_: torchvision.datasets.ImageFolder
    root: ${path.data_dir}/tiny_imagenet_200/train
    transform: ${data.train_transform}

val_set:
    _target_: torchvision.datasets.ImageFolder
    root: ${path.data_dir}/tiny_imagenet_200/val
    transform: ${data.val_transform}

train_loader:
    _target_: ignite.distributed.auto_dataloader
    dataset: ${data.train_set}
    batch_size: ${data.batch_size}
    num_workers: 2
    collate_fn:
        _target_: src.collate_fn_wrapper
        transform: ${data.train_batched_transform}
    shuffle: true
    drop_last: true

val_loader:
    _target_: ignite.distributed.auto_dataloader
    dataset: ${data.val_set}
    batch_size: ${data.val_batch_size}
    num_workers: 2
    shuffle: false
