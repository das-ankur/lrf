compile: false
amp: false
max_epochs: 600
max_steps: .inf
warmup_steps: 100
non_blocking: true
val_every_epochs: 5
log_every_iters: 1

loss:
    _target_: torch.nn.CrossEntropyLoss

optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.001
    weight_decay: 0.0

lr_scheduler:
    _target_: src.scheduler.WarmupCosineAnnealingLR
    _partial_: true
    warmup_steps: ${trainer.warmup_steps}
    total_steps: ${trainer.max_steps}
    lr_min: 0
    last_epoch: -1
