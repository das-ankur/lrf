SLURM_JOB_ID: 55692391
SLURM_JOB_USER: vsc33647
SLURM_JOB_ACCOUNT: lp_inspiremed
SLURM_JOB_NAME: interpolate_cifar10_sgl
SLURM_CLUSTER_NAME: genius
SLURM_JOB_PARTITION: gpu_v100
SLURM_NNODES: 1
SLURM_NODELIST: r24g09
SLURM_JOB_CPUS_PER_NODE: 8
SLURM_JOB_GPUS: 4,5
Date: Wed Feb 21 21:00:03 CET 2024
Walltime: 00-00:15:00
========================================================================
This is before main
2024-02-21 21:00:08,009 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-21 21:00:08,009 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-21 21:00:08,009 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x14e7ac9fe200>' in 2 processes
This is before idist.Parallel
This is before parallel.run
2024-02-21 21:00:15,649 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-21 21:00:16,507 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 128, 'num_workers': 1, 'drop_last': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1495b61bcad0>, 'pin_memory': True}
2024-02-21 21:00:16,833 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 256, 'num_workers': 1, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1495b60b2a20>, 'pin_memory': True}
/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[2024-02-21 21:00:16,842][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-21 21:00:16,843][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-21 21:00:16,843][PyLogger][INFO]: GPU device: Tesla V100-SXM2-32GB
[2024-02-21 21:00:16,843][PyLogger][INFO]: CUDA version: 12.1
[2024-02-21 21:00:16,844][PyLogger][INFO]: CUDNN version: 8902
[2024-02-21 21:00:16,844][PyLogger][INFO]: Distributed setting:
[2024-02-21 21:00:16,844][PyLogger][INFO]: Backend: nccl
[2024-02-21 21:00:16,844][PyLogger][INFO]: World size: 2
[2024-02-21 21:00:16,844][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-21 21:00:16,844][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-21 21:00:16,844][PyLogger][INFO]: GPU device: Tesla V100-SXM2-32GB
[2024-02-21 21:00:16,844][PyLogger][INFO]: CUDA version: 12.1
[2024-02-21 21:00:16,845][PyLogger][INFO]: CUDNN version: 8902
[2024-02-21 21:00:16,845][PyLogger][INFO]: Distributed setting:
[2024-02-21 21:00:16,845][PyLogger][INFO]: Backend: nccl
[2024-02-21 21:00:16,845][PyLogger][INFO]: World size: 2
This is start of training
This is after idist.auto_model
Files already downloaded and verified
This is after hydra.utils.instantiate
This is before checkpoint
This is before pylogger
This is before tensorboard
This is start of training
This is after idist.auto_model
Files already downloaded and verified
This is after hydra.utils.instantiate
This is before checkpoint
This is before pylogger
This is before tensorboard
This is before trainer.run
Error executing job with overrides: ['dist.backend=nccl', 'dist.nproc_per_node=2', 'dist.nnodes=1', 'task_name=train_interpolate_cifar10_org', 'data=cifar10', 'metric=cifar10', 'model=interpolate_model_cifar', 'model.num_classes=10', 'model.new_size=32']
Traceback (most recent call last):
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/train.py", line 135, in main
    parallel.run(training, cfg)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/launcher.py", line 312, in run
    idist.spawn(self.backend, func, args=args, kwargs_dict=kwargs, **self._spawn_params)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/utils.py", line 322, in spawn
    comp_model_cls.spawn(
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 392, in spawn
    start_processes(
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
              ^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 158, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
    fn(i, *args)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 341, in _dist_worker_task_fn
    fn(local_rank, *args, **kw_dict)
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/train.py", line 115, in training
    handlers[key] = hydra.utils.instantiate(value)(objects=objects)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/utils/tensorboard.py", line 25, in tensorboard
    return tb_logger
           ^^^^^^^^^
UnboundLocalError: cannot access local variable 'tb_logger' where it is not associated with a value


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
cat: ../.temp/interpolate_cifar10_org.txt: No such file or directory
2024-02-21 21:00:26,405 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-21 21:00:26,405 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-21 21:00:26,405 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1470da0fa3e0>' in 2 processes
/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 14 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2024-02-21 21:00:33,496 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-21 21:00:33,892 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 256, 'num_workers': 1, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x151efd846600>, 'pin_memory': True}
Error executing job with overrides: ['dist.backend=nccl', 'dist.nproc_per_node=2', 'dist.nnodes=1', 'task_name=eval_interpolate_cifar10_org', 'data=cifar10', 'metric=cifar10', 'handler.checkpoint.load_from=', 'model=interpolate_model_cifar', 'model.num_classes=10', 'model.rescale=false', 'model.new_size=12']
Traceback (most recent call last):
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/eval.py", line 49, in main
    parallel.run(evaluation, cfg)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/launcher.py", line 312, in run
    idist.spawn(self.backend, func, args=args, kwargs_dict=kwargs, **self._spawn_params)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/utils.py", line 322, in spawn
    comp_model_cls.spawn(
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 392, in spawn
    start_processes(
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
              ^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 158, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
    fn(i, *args)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 341, in _dist_worker_task_fn
    fn(local_rank, *args, **kw_dict)
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/eval.py", line 37, in evaluation
    hydra.utils.instantiate(value)(objects=objects)
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/utils/checkpoint.py", line 54, in checkpoint
    ckpt = load_checkpoint(load_from)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/utils/checkpoint.py", line 11, in load_checkpoint
    ckpt = torch.load(ckpt_path.as_posix(), map_location="cpu")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/serialization.py", line 998, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/serialization.py", line 445, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/serialization.py", line 426, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
IsADirectoryError: [Errno 21] Is a directory: '.'


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
new_size=12 done.
2024-02-21 21:00:38,875 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-21 21:00:38,875 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-21 21:00:38,875 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x151c03bf63e0>' in 2 processes
2024-02-21 21:00:46,771 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-21 21:00:47,156 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 256, 'num_workers': 1, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14e8bfd958e0>, 'pin_memory': True}
Error executing job with overrides: ['dist.backend=nccl', 'dist.nproc_per_node=2', 'dist.nnodes=1', 'task_name=eval_interpolate_cifar10_org', 'data=cifar10', 'metric=cifar10', 'handler.checkpoint.load_from=', 'model=interpolate_model_cifar', 'model.num_classes=10', 'model.rescale=false', 'model.new_size=16']
Traceback (most recent call last):
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/eval.py", line 49, in main
    parallel.run(evaluation, cfg)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/launcher.py", line 312, in run
    idist.spawn(self.backend, func, args=args, kwargs_dict=kwargs, **self._spawn_params)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/utils.py", line 322, in spawn
    comp_model_cls.spawn(
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 392, in spawn
    start_processes(
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
              ^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 158, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
    fn(i, *args)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 341, in _dist_worker_task_fn
    fn(local_rank, *args, **kw_dict)
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/eval.py", line 37, in evaluation
    hydra.utils.instantiate(value)(objects=objects)
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/utils/checkpoint.py", line 54, in checkpoint
    ckpt = load_checkpoint(load_from)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/utils/checkpoint.py", line 11, in load_checkpoint
    ckpt = torch.load(ckpt_path.as_posix(), map_location="cpu")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/serialization.py", line 998, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/serialization.py", line 445, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/serialization.py", line 426, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
IsADirectoryError: [Errno 21] Is a directory: '.'


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
new_size=16 done.
