SLURM_JOB_ID: 55692378
SLURM_JOB_USER: vsc33647
SLURM_JOB_ACCOUNT: lp_inspiremed
SLURM_JOB_NAME: interpolate_cifar10_sgl
SLURM_CLUSTER_NAME: genius
SLURM_JOB_PARTITION: gpu_v100
SLURM_NNODES: 1
SLURM_NODELIST: r24g09
SLURM_JOB_CPUS_PER_NODE: 8
SLURM_JOB_GPUS: 0,1
Date: Wed Feb 21 20:58:29 CET 2024
Walltime: 00-00:15:00
========================================================================
This is before main
2024-02-21 20:58:33,988 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-21 20:58:33,988 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-21 20:58:33,988 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x1506b5986200>' in 2 processes
This is before idist.Parallel
This is before parallel.run
Error executing job with overrides: ['dist.backend=nccl', 'dist.nproc_per_node=2', 'dist.nnodes=1', 'task_name=train_interpolate_cifar10_org', 'data=cifar10', 'metric=cifar10', 'model=interpolate_model_cifar', 'model.num_classes=10', 'model.new_size=32']
Traceback (most recent call last):
  File "/vsc-hard-mounts/leuven-data/336/vsc33647/projects/lsvd/src/train.py", line 135, in main
    parallel.run(training, cfg)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/launcher.py", line 312, in run
    idist.spawn(self.backend, func, args=args, kwargs_dict=kwargs, **self._spawn_params)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/utils.py", line 322, in spawn
    comp_model_cls.spawn(
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 392, in spawn
    start_processes(
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
              ^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 158, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
    fn(i, *args)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 337, in _dist_worker_task_fn
    model = _NativeDistModel.create_from_backend(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 71, in create_from_backend
    return _NativeDistModel(
           ^^^^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 92, in __init__
    self._create_from_backend(
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 110, in _create_from_backend
    self.setup_env_vars(rank, world_size)
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 222, in setup_env_vars
    self._setup_env_in_slurm()
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 256, in _setup_env_in_slurm
    ddp_vars = _setup_ddp_vars_from_slurm_env(cast(Dict, os.environ))
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/ignite/distributed/comp_models/native.py", line 596, in _setup_ddp_vars_from_slurm_env
    raise RuntimeError(
RuntimeError: Environment variable defined for PyTorch Distributed context is inconsistent with equivalent SLURM env variable. We expect that WORLD_SIZE: 2 >= 8
SLURM vars: {'RANK': 1, 'LOCAL_RANK': 1, 'WORLD_SIZE': 8, 'MASTER_ADDR': None, 'MASTER_PORT': None}
PTH vars: {'RANK': '1', 'LOCAL_RANK': '1', 'WORLD_SIZE': '2', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '2222'}



Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
cat: ../.temp/interpolate_cifar10_org.txt: No such file or directory
