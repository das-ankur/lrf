SLURM_JOB_ID: 55692406
SLURM_JOB_USER: vsc33647
SLURM_JOB_ACCOUNT: lp_inspiremed
SLURM_JOB_NAME: interpolate_cifar10_sgl
SLURM_CLUSTER_NAME: genius
SLURM_JOB_PARTITION: gpu_v100
SLURM_NNODES: 1
SLURM_NODELIST: r24g09
SLURM_JOB_CPUS_PER_NODE: 8
SLURM_JOB_GPUS: 0,1
Date: Wed Feb 21 21:01:59 CET 2024
Walltime: 00-00:15:00
========================================================================
This is before main
2024-02-21 21:02:03,336 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-21 21:02:03,337 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-21 21:02:03,337 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x1540b3b72200>' in 2 processes
This is before idist.Parallel
This is before parallel.run
2024-02-21 21:02:10,472 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-21 21:02:11,317 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 128, 'num_workers': 1, 'drop_last': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1534fd93d9a0>, 'pin_memory': True}
2024-02-21 21:02:11,646 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 256, 'num_workers': 1, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x153437a9ec60>, 'pin_memory': True}
/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[2024-02-21 21:02:11,656][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-21 21:02:11,656][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-21 21:02:11,656][PyLogger][INFO]: GPU device: Tesla V100-SXM2-32GB
[2024-02-21 21:02:11,656][PyLogger][INFO]: CUDA version: 12.1
[2024-02-21 21:02:11,657][PyLogger][INFO]: CUDNN version: 8902
[2024-02-21 21:02:11,657][PyLogger][INFO]: Distributed setting:
[2024-02-21 21:02:11,657][PyLogger][INFO]: Backend: nccl
[2024-02-21 21:02:11,657][PyLogger][INFO]: World size: 2
/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[2024-02-21 21:02:11,666][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-21 21:02:11,666][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-21 21:02:11,666][PyLogger][INFO]: GPU device: Tesla V100-SXM2-32GB
[2024-02-21 21:02:11,666][PyLogger][INFO]: CUDA version: 12.1
[2024-02-21 21:02:11,667][PyLogger][INFO]: CUDNN version: 8902
[2024-02-21 21:02:11,667][PyLogger][INFO]: Distributed setting:
[2024-02-21 21:02:11,667][PyLogger][INFO]: Backend: nccl
[2024-02-21 21:02:11,667][PyLogger][INFO]: World size: 2
This is start of training
This is after idist.auto_model
Files already downloaded and verified
This is after hydra.utils.instantiate
This is before checkpoint
This is before pylogger
This is before tensorboard
This is before trainer.run
This is start of training
This is after idist.auto_model
Files already downloaded and verified
This is after hydra.utils.instantiate
This is before checkpoint
This is before pylogger
This is before tensorboard
This is before trainer.run
[2024-02-21 21:02:25,071][PyLogger][INFO]: Rank[0]: Epoch[1/600], iter[50] -  - loss: 2.63
[2024-02-21 21:02:25,071][PyLogger][INFO]: Rank[1]: Epoch[1/600], iter[50] -  - loss: 2.67
[2024-02-21 21:02:31,631][PyLogger][INFO]: Rank[1]: Epoch[1/600], iter[100] -  - loss: 2.27
[2024-02-21 21:02:31,631][PyLogger][INFO]: Rank[0]: Epoch[1/600], iter[100] -  - loss: 2.94
[2024-02-21 21:02:38,195][PyLogger][INFO]: Rank[1]: Epoch[1/600], iter[150] -  - loss: 2.20
[2024-02-21 21:02:38,195][PyLogger][INFO]: Rank[0]: Epoch[1/600], iter[150] -  - loss: 2.20
[2024-02-21 21:02:50,325][PyLogger][INFO]: Rank[1]: Epoch[2/600], iter[200] -  - loss: 2.12
[2024-02-21 21:02:50,325][PyLogger][INFO]: Rank[0]: Epoch[2/600], iter[200] -  - loss: 2.14
[2024-02-21 21:02:56,897][PyLogger][INFO]: Rank[1]: Epoch[2/600], iter[250] -  - loss: 2.02
[2024-02-21 21:02:56,897][PyLogger][INFO]: Rank[0]: Epoch[2/600], iter[250] -  - loss: 2.16
[2024-02-21 21:03:03,471][PyLogger][INFO]: Rank[1]: Epoch[2/600], iter[300] -  - loss: 2.11
[2024-02-21 21:03:03,471][PyLogger][INFO]: Rank[0]: Epoch[2/600], iter[300] -  - loss: 2.10
[2024-02-21 21:03:10,035][PyLogger][INFO]: Rank[0]: Epoch[2/600], iter[350] -  - loss: 1.99
[2024-02-21 21:03:10,035][PyLogger][INFO]: Rank[1]: Epoch[2/600], iter[350] -  - loss: 2.01
