SLURM_JOB_ID: 55710180
SLURM_JOB_USER: vsc33647
SLURM_JOB_ACCOUNT: lp_inspiremed
SLURM_JOB_NAME: dct_cifar10_dec
SLURM_CLUSTER_NAME: genius
SLURM_JOB_PARTITION: gpu_p100
SLURM_NNODES: 1
SLURM_NODELIST: r23g37
SLURM_JOB_CPUS_PER_NODE: 18
SLURM_JOB_GPUS: 0,1
Date: Tue Feb 27 21:55:20 CET 2024
Walltime: 00-06:00:00
========================================================================
2024-02-27 21:55:29,453 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 21:55:29,453 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 21:55:29,453 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x1516ec2953a0>' in 2 processes
2024-02-27 21:55:40,700 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 21:55:43,145 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 256, 'num_workers': 0, 'pin_memory': True, 'drop_last': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x147780196f00>}
2024-02-27 21:55:43,481 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1476fcd39bb0>}
/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[2024-02-27 21:55:45,005][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 21:55:45,005][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 21:55:45,005][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 21:55:45,005][PyLogger][INFO]: World size: 2
[2024-02-27 21:55:45,005][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 21:55:45,005][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 21:55:45,005][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 21:55:45,027][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 21:55:45,027][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 21:55:45,027][PyLogger][INFO]: World size: 2
[2024-02-27 21:56:01,218][PyLogger][INFO]: Rank[0]: Epoch[357/500], iter[34550] -  - loss: 1.42258
[2024-02-27 21:56:01,219][PyLogger][INFO]: Rank[1]: Epoch[357/500], iter[34550] -  - loss: 0.50152
[2024-02-27 21:56:27,744][PyLogger][INFO]: Rank[0]: Epoch[357/500], iter[34600] -  - loss: 0.50564
[2024-02-27 21:56:27,744][PyLogger][INFO]: Rank[1]: Epoch[357/500], iter[34600] -  - loss: 0.79890
[2024-02-27 21:56:55,003][PyLogger][INFO]: Rank[0]: Epoch[358/500], iter[34650] -  - loss: 0.51813
[2024-02-27 21:56:55,003][PyLogger][INFO]: Rank[1]: Epoch[358/500], iter[34650] -  - loss: 0.50422
[2024-02-27 21:57:21,513][PyLogger][INFO]: Rank[0]: Epoch[358/500], iter[34700] -  - loss: 0.51042
[2024-02-27 21:57:21,513][PyLogger][INFO]: Rank[1]: Epoch[358/500], iter[34700] -  - loss: 0.50268
[2024-02-27 21:57:49,008][PyLogger][INFO]: Rank[0]: Epoch[359/500], iter[34750] -  - loss: 0.50207
[2024-02-27 21:57:49,008][PyLogger][INFO]: Rank[1]: Epoch[359/500], iter[34750] -  - loss: 0.50661
[2024-02-27 21:58:15,686][PyLogger][INFO]: Rank[1]: Epoch[359/500], iter[34800] -  - loss: 0.50372
[2024-02-27 21:58:15,686][PyLogger][INFO]: Rank[0]: Epoch[359/500], iter[34800] -  - loss: 0.63919
[2024-02-27 21:58:43,056][PyLogger][INFO]: Rank[1]: Epoch[360/500], iter[34850] -  - loss: 0.50204
[2024-02-27 21:58:43,056][PyLogger][INFO]: Rank[0]: Epoch[360/500], iter[34850] -  - loss: 0.50368
[2024-02-27 21:59:09,626][PyLogger][INFO]: Rank[0]: Epoch[360/500], iter[34900] -  - loss: 0.63055
[2024-02-27 21:59:09,626][PyLogger][INFO]: Rank[1]: Epoch[360/500], iter[34900] -  - loss: 0.50140
[2024-02-27 22:00:04,854][PyLogger][INFO]: Rank[0]: Epoch[361/500], iter[34950] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50261
[2024-02-27 22:00:04,854][PyLogger][INFO]: Rank[1]: Epoch[361/500], iter[34950] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.79892
[2024-02-27 22:00:31,400][PyLogger][INFO]: Rank[0]: Epoch[361/500], iter[35000] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.83444
[2024-02-27 22:00:31,400][PyLogger][INFO]: Rank[1]: Epoch[361/500], iter[35000] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50206
[2024-02-27 22:00:58,659][PyLogger][INFO]: Rank[0]: Epoch[362/500], iter[35050] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 1.83310
[2024-02-27 22:00:58,659][PyLogger][INFO]: Rank[1]: Epoch[362/500], iter[35050] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 1.08727
[2024-02-27 22:01:25,130][PyLogger][INFO]: Rank[0]: Epoch[362/500], iter[35100] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.51694
[2024-02-27 22:01:25,130][PyLogger][INFO]: Rank[1]: Epoch[362/500], iter[35100] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.51078
[2024-02-27 22:01:52,486][PyLogger][INFO]: Rank[1]: Epoch[363/500], iter[35150] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50207
[2024-02-27 22:01:52,486][PyLogger][INFO]: Rank[0]: Epoch[363/500], iter[35150] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50487
[2024-02-27 22:02:19,072][PyLogger][INFO]: Rank[0]: Epoch[363/500], iter[35200] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.54217
[2024-02-27 22:02:19,072][PyLogger][INFO]: Rank[1]: Epoch[363/500], iter[35200] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50204
[2024-02-27 22:02:46,398][PyLogger][INFO]: Rank[0]: Epoch[364/500], iter[35250] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50163
[2024-02-27 22:02:46,398][PyLogger][INFO]: Rank[1]: Epoch[364/500], iter[35250] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50303
[2024-02-27 22:03:13,076][PyLogger][INFO]: Rank[0]: Epoch[364/500], iter[35300] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50398
[2024-02-27 22:03:13,076][PyLogger][INFO]: Rank[1]: Epoch[364/500], iter[35300] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50200
[2024-02-27 22:03:40,823][PyLogger][INFO]: Rank[1]: Epoch[365/500], iter[35350] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50417
[2024-02-27 22:03:40,823][PyLogger][INFO]: Rank[0]: Epoch[365/500], iter[35350] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50388
[2024-02-27 22:04:07,394][PyLogger][INFO]: Rank[0]: Epoch[365/500], iter[35400] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50412
[2024-02-27 22:04:07,394][PyLogger][INFO]: Rank[1]: Epoch[365/500], iter[35400] - train_accuracy: 0.91463 - val_accuracy: 0.65290 - train_loss: 0.72827 - val_loss: 1.43770 - loss: 0.50228
[2024-02-27 22:05:02,604][PyLogger][INFO]: Rank[0]: Epoch[366/500], iter[35450] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50615
[2024-02-27 22:05:02,604][PyLogger][INFO]: Rank[1]: Epoch[366/500], iter[35450] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50292
[2024-02-27 22:05:29,127][PyLogger][INFO]: Rank[0]: Epoch[366/500], iter[35500] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50460
[2024-02-27 22:05:29,127][PyLogger][INFO]: Rank[1]: Epoch[366/500], iter[35500] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50154
[2024-02-27 22:05:56,439][PyLogger][INFO]: Rank[0]: Epoch[367/500], iter[35550] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50198
[2024-02-27 22:05:56,439][PyLogger][INFO]: Rank[1]: Epoch[367/500], iter[35550] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.62378
[2024-02-27 22:06:23,767][PyLogger][INFO]: Rank[0]: Epoch[368/500], iter[35600] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.66291
[2024-02-27 22:06:23,767][PyLogger][INFO]: Rank[1]: Epoch[368/500], iter[35600] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50227
[2024-02-27 22:06:50,315][PyLogger][INFO]: Rank[0]: Epoch[368/500], iter[35650] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50223
[2024-02-27 22:06:50,315][PyLogger][INFO]: Rank[1]: Epoch[368/500], iter[35650] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50259
[2024-02-27 22:07:18,056][PyLogger][INFO]: Rank[1]: Epoch[369/500], iter[35700] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50411
[2024-02-27 22:07:18,056][PyLogger][INFO]: Rank[0]: Epoch[369/500], iter[35700] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50584
[2024-02-27 22:07:44,595][PyLogger][INFO]: Rank[0]: Epoch[369/500], iter[35750] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50914
[2024-02-27 22:07:44,595][PyLogger][INFO]: Rank[1]: Epoch[369/500], iter[35750] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50189
[2024-02-27 22:08:12,068][PyLogger][INFO]: Rank[1]: Epoch[370/500], iter[35800] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 1.15342
[2024-02-27 22:08:12,068][PyLogger][INFO]: Rank[0]: Epoch[370/500], iter[35800] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 1.41952
[2024-02-27 22:08:38,606][PyLogger][INFO]: Rank[0]: Epoch[370/500], iter[35850] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50410
[2024-02-27 22:08:38,606][PyLogger][INFO]: Rank[1]: Epoch[370/500], iter[35850] - train_accuracy: 0.91217 - val_accuracy: 0.76100 - train_loss: 0.75105 - val_loss: 1.10364 - loss: 0.50497
[2024-02-27 22:09:34,115][PyLogger][INFO]: Rank[0]: Epoch[371/500], iter[35900] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.60333
[2024-02-27 22:09:34,115][PyLogger][INFO]: Rank[1]: Epoch[371/500], iter[35900] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50371
[2024-02-27 22:10:00,676][PyLogger][INFO]: Rank[1]: Epoch[371/500], iter[35950] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50276
[2024-02-27 22:10:00,677][PyLogger][INFO]: Rank[0]: Epoch[371/500], iter[35950] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50535
[2024-02-27 22:10:28,490][PyLogger][INFO]: Rank[1]: Epoch[372/500], iter[36000] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50252
[2024-02-27 22:10:28,490][PyLogger][INFO]: Rank[0]: Epoch[372/500], iter[36000] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50268
[2024-02-27 22:10:55,019][PyLogger][INFO]: Rank[0]: Epoch[372/500], iter[36050] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50238
[2024-02-27 22:10:55,019][PyLogger][INFO]: Rank[1]: Epoch[372/500], iter[36050] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50940
[2024-02-27 22:11:22,256][PyLogger][INFO]: Rank[0]: Epoch[373/500], iter[36100] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50155
[2024-02-27 22:11:22,256][PyLogger][INFO]: Rank[1]: Epoch[373/500], iter[36100] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50386
[2024-02-27 22:11:48,577][PyLogger][INFO]: Rank[1]: Epoch[373/500], iter[36150] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50772
[2024-02-27 22:11:48,577][PyLogger][INFO]: Rank[0]: Epoch[373/500], iter[36150] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50424
[2024-02-27 22:12:15,907][PyLogger][INFO]: Rank[0]: Epoch[374/500], iter[36200] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.62509
[2024-02-27 22:12:15,907][PyLogger][INFO]: Rank[1]: Epoch[374/500], iter[36200] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50215
[2024-02-27 22:12:42,431][PyLogger][INFO]: Rank[1]: Epoch[374/500], iter[36250] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50412
[2024-02-27 22:12:42,431][PyLogger][INFO]: Rank[0]: Epoch[374/500], iter[36250] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50308
[2024-02-27 22:13:09,915][PyLogger][INFO]: Rank[0]: Epoch[375/500], iter[36300] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50402
[2024-02-27 22:13:09,915][PyLogger][INFO]: Rank[1]: Epoch[375/500], iter[36300] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 1.42458
[2024-02-27 22:13:36,424][PyLogger][INFO]: Rank[1]: Epoch[375/500], iter[36350] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50622
[2024-02-27 22:13:36,424][PyLogger][INFO]: Rank[0]: Epoch[375/500], iter[36350] - train_accuracy: 0.89117 - val_accuracy: 0.72730 - train_loss: 0.84625 - val_loss: 1.34824 - loss: 0.50738
[2024-02-27 22:14:31,924][PyLogger][INFO]: Rank[0]: Epoch[376/500], iter[36400] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50173
[2024-02-27 22:14:31,924][PyLogger][INFO]: Rank[1]: Epoch[376/500], iter[36400] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50470
[2024-02-27 22:14:58,448][PyLogger][INFO]: Rank[0]: Epoch[376/500], iter[36450] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50559
[2024-02-27 22:14:58,448][PyLogger][INFO]: Rank[1]: Epoch[376/500], iter[36450] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50540
[2024-02-27 22:15:25,719][PyLogger][INFO]: Rank[0]: Epoch[377/500], iter[36500] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50184
[2024-02-27 22:15:25,719][PyLogger][INFO]: Rank[1]: Epoch[377/500], iter[36500] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50351
[2024-02-27 22:15:52,191][PyLogger][INFO]: Rank[1]: Epoch[377/500], iter[36550] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.53616
[2024-02-27 22:15:52,192][PyLogger][INFO]: Rank[0]: Epoch[377/500], iter[36550] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 1.48158
[2024-02-27 22:16:20,055][PyLogger][INFO]: Rank[0]: Epoch[378/500], iter[36600] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50438
[2024-02-27 22:16:20,055][PyLogger][INFO]: Rank[1]: Epoch[378/500], iter[36600] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50250
[2024-02-27 22:16:46,610][PyLogger][INFO]: Rank[1]: Epoch[378/500], iter[36650] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50691
[2024-02-27 22:16:46,610][PyLogger][INFO]: Rank[0]: Epoch[378/500], iter[36650] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50281
[2024-02-27 22:17:13,798][PyLogger][INFO]: Rank[0]: Epoch[379/500], iter[36700] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50554
[2024-02-27 22:17:13,798][PyLogger][INFO]: Rank[1]: Epoch[379/500], iter[36700] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50216
[2024-02-27 22:17:40,251][PyLogger][INFO]: Rank[0]: Epoch[379/500], iter[36750] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50260
[2024-02-27 22:17:40,251][PyLogger][INFO]: Rank[1]: Epoch[379/500], iter[36750] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50183
[2024-02-27 22:18:07,571][PyLogger][INFO]: Rank[1]: Epoch[380/500], iter[36800] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50781
[2024-02-27 22:18:07,571][PyLogger][INFO]: Rank[0]: Epoch[380/500], iter[36800] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50800
[2024-02-27 22:18:34,111][PyLogger][INFO]: Rank[0]: Epoch[380/500], iter[36850] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50315
[2024-02-27 22:18:34,111][PyLogger][INFO]: Rank[1]: Epoch[380/500], iter[36850] - train_accuracy: 0.91094 - val_accuracy: 0.76420 - train_loss: 0.78259 - val_loss: 1.21669 - loss: 0.50168
[2024-02-27 22:19:29,314][PyLogger][INFO]: Rank[1]: Epoch[381/500], iter[36900] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.50695
[2024-02-27 22:19:29,314][PyLogger][INFO]: Rank[0]: Epoch[381/500], iter[36900] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.50341
[2024-02-27 22:19:55,748][PyLogger][INFO]: Rank[0]: Epoch[381/500], iter[36950] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.58412
[2024-02-27 22:19:55,749][PyLogger][INFO]: Rank[1]: Epoch[381/500], iter[36950] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.50218
[2024-02-27 22:20:22,928][PyLogger][INFO]: Rank[1]: Epoch[382/500], iter[37000] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.50196
[2024-02-27 22:20:22,928][PyLogger][INFO]: Rank[0]: Epoch[382/500], iter[37000] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.54642
[2024-02-27 22:20:49,415][PyLogger][INFO]: Rank[0]: Epoch[382/500], iter[37050] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.50478
[2024-02-27 22:20:49,415][PyLogger][INFO]: Rank[1]: Epoch[382/500], iter[37050] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.52984
[2024-02-27 22:21:16,570][PyLogger][INFO]: Rank[1]: Epoch[383/500], iter[37100] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.50430
[2024-02-27 22:21:16,570][PyLogger][INFO]: Rank[0]: Epoch[383/500], iter[37100] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.59265
[2024-02-27 22:21:43,118][PyLogger][INFO]: Rank[0]: Epoch[383/500], iter[37150] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.50322
[2024-02-27 22:21:43,118][PyLogger][INFO]: Rank[1]: Epoch[383/500], iter[37150] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.86192
[2024-02-27 22:22:10,271][PyLogger][INFO]: Rank[0]: Epoch[384/500], iter[37200] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.55481
[2024-02-27 22:22:10,271][PyLogger][INFO]: Rank[1]: Epoch[384/500], iter[37200] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.51020
[2024-02-27 22:22:37,545][PyLogger][INFO]: Rank[0]: Epoch[385/500], iter[37250] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.50234
[2024-02-27 22:22:37,545][PyLogger][INFO]: Rank[1]: Epoch[385/500], iter[37250] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.50342
[2024-02-27 22:23:04,036][PyLogger][INFO]: Rank[0]: Epoch[385/500], iter[37300] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.50163
[2024-02-27 22:23:04,036][PyLogger][INFO]: Rank[1]: Epoch[385/500], iter[37300] - train_accuracy: 0.91829 - val_accuracy: 0.74560 - train_loss: 0.74869 - val_loss: 1.21028 - loss: 0.50519
[2024-02-27 22:23:59,045][PyLogger][INFO]: Rank[0]: Epoch[386/500], iter[37350] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 1.01445
[2024-02-27 22:23:59,045][PyLogger][INFO]: Rank[1]: Epoch[386/500], iter[37350] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.78344
[2024-02-27 22:24:25,513][PyLogger][INFO]: Rank[0]: Epoch[386/500], iter[37400] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50403
[2024-02-27 22:24:25,513][PyLogger][INFO]: Rank[1]: Epoch[386/500], iter[37400] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50486
[2024-02-27 22:24:52,748][PyLogger][INFO]: Rank[0]: Epoch[387/500], iter[37450] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50283
[2024-02-27 22:24:52,748][PyLogger][INFO]: Rank[1]: Epoch[387/500], iter[37450] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50187
[2024-02-27 22:25:19,223][PyLogger][INFO]: Rank[0]: Epoch[387/500], iter[37500] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.85970
[2024-02-27 22:25:19,223][PyLogger][INFO]: Rank[1]: Epoch[387/500], iter[37500] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50449
[2024-02-27 22:25:46,485][PyLogger][INFO]: Rank[0]: Epoch[388/500], iter[37550] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50895
[2024-02-27 22:25:46,485][PyLogger][INFO]: Rank[1]: Epoch[388/500], iter[37550] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50388
[2024-02-27 22:26:12,960][PyLogger][INFO]: Rank[0]: Epoch[388/500], iter[37600] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50142
[2024-02-27 22:26:12,960][PyLogger][INFO]: Rank[1]: Epoch[388/500], iter[37600] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.79540
[2024-02-27 22:26:40,254][PyLogger][INFO]: Rank[0]: Epoch[389/500], iter[37650] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.53709
[2024-02-27 22:26:40,254][PyLogger][INFO]: Rank[1]: Epoch[389/500], iter[37650] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50799
[2024-02-27 22:27:06,640][PyLogger][INFO]: Rank[0]: Epoch[389/500], iter[37700] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.51825
[2024-02-27 22:27:06,640][PyLogger][INFO]: Rank[1]: Epoch[389/500], iter[37700] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50260
[2024-02-27 22:27:33,798][PyLogger][INFO]: Rank[0]: Epoch[390/500], iter[37750] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50313
[2024-02-27 22:27:33,798][PyLogger][INFO]: Rank[1]: Epoch[390/500], iter[37750] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50284
[2024-02-27 22:28:00,344][PyLogger][INFO]: Rank[0]: Epoch[390/500], iter[37800] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50118
[2024-02-27 22:28:00,344][PyLogger][INFO]: Rank[1]: Epoch[390/500], iter[37800] - train_accuracy: 0.94956 - val_accuracy: 0.79410 - train_loss: 0.64384 - val_loss: 1.05779 - loss: 0.50451
[2024-02-27 22:28:55,471][PyLogger][INFO]: Rank[0]: Epoch[391/500], iter[37850] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.51147
[2024-02-27 22:28:55,471][PyLogger][INFO]: Rank[1]: Epoch[391/500], iter[37850] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50176
[2024-02-27 22:29:22,104][PyLogger][INFO]: Rank[1]: Epoch[391/500], iter[37900] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.67232
[2024-02-27 22:29:22,104][PyLogger][INFO]: Rank[0]: Epoch[391/500], iter[37900] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50733
[2024-02-27 22:29:49,355][PyLogger][INFO]: Rank[0]: Epoch[392/500], iter[37950] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50245
[2024-02-27 22:29:49,355][PyLogger][INFO]: Rank[1]: Epoch[392/500], iter[37950] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50189
[2024-02-27 22:30:15,785][PyLogger][INFO]: Rank[0]: Epoch[392/500], iter[38000] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50970
[2024-02-27 22:30:15,785][PyLogger][INFO]: Rank[1]: Epoch[392/500], iter[38000] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 1.85793
[2024-02-27 22:30:42,997][PyLogger][INFO]: Rank[0]: Epoch[393/500], iter[38050] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50195
[2024-02-27 22:30:42,997][PyLogger][INFO]: Rank[1]: Epoch[393/500], iter[38050] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.52095
[2024-02-27 22:31:09,521][PyLogger][INFO]: Rank[0]: Epoch[393/500], iter[38100] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50446
[2024-02-27 22:31:09,521][PyLogger][INFO]: Rank[1]: Epoch[393/500], iter[38100] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50595
[2024-02-27 22:31:36,809][PyLogger][INFO]: Rank[0]: Epoch[394/500], iter[38150] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50192
[2024-02-27 22:31:36,809][PyLogger][INFO]: Rank[1]: Epoch[394/500], iter[38150] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50278
[2024-02-27 22:32:03,342][PyLogger][INFO]: Rank[0]: Epoch[394/500], iter[38200] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50263
[2024-02-27 22:32:03,342][PyLogger][INFO]: Rank[1]: Epoch[394/500], iter[38200] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50639
[2024-02-27 22:32:31,183][PyLogger][INFO]: Rank[0]: Epoch[395/500], iter[38250] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50732
[2024-02-27 22:32:31,183][PyLogger][INFO]: Rank[1]: Epoch[395/500], iter[38250] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50345
[2024-02-27 22:32:57,711][PyLogger][INFO]: Rank[1]: Epoch[395/500], iter[38300] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50197
[2024-02-27 22:32:57,711][PyLogger][INFO]: Rank[0]: Epoch[395/500], iter[38300] - train_accuracy: 0.93406 - val_accuracy: 0.81770 - train_loss: 0.70866 - val_loss: 1.01204 - loss: 0.50315
[2024-02-27 22:33:52,549][PyLogger][INFO]: Rank[0]: Epoch[396/500], iter[38350] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50217
[2024-02-27 22:33:52,549][PyLogger][INFO]: Rank[1]: Epoch[396/500], iter[38350] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50221
[2024-02-27 22:34:18,988][PyLogger][INFO]: Rank[0]: Epoch[396/500], iter[38400] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 1.82423
[2024-02-27 22:34:18,988][PyLogger][INFO]: Rank[1]: Epoch[396/500], iter[38400] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.57332
[2024-02-27 22:34:46,941][PyLogger][INFO]: Rank[0]: Epoch[397/500], iter[38450] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50291
[2024-02-27 22:34:46,941][PyLogger][INFO]: Rank[1]: Epoch[397/500], iter[38450] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50199
[2024-02-27 22:35:13,444][PyLogger][INFO]: Rank[1]: Epoch[397/500], iter[38500] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50405
[2024-02-27 22:35:13,444][PyLogger][INFO]: Rank[0]: Epoch[397/500], iter[38500] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.51136
[2024-02-27 22:35:40,648][PyLogger][INFO]: Rank[0]: Epoch[398/500], iter[38550] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50203
[2024-02-27 22:35:40,649][PyLogger][INFO]: Rank[1]: Epoch[398/500], iter[38550] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50813
[2024-02-27 22:36:07,117][PyLogger][INFO]: Rank[1]: Epoch[398/500], iter[38600] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50238
[2024-02-27 22:36:07,117][PyLogger][INFO]: Rank[0]: Epoch[398/500], iter[38600] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50217
[2024-02-27 22:36:34,458][PyLogger][INFO]: Rank[0]: Epoch[399/500], iter[38650] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50382
[2024-02-27 22:36:34,459][PyLogger][INFO]: Rank[1]: Epoch[399/500], iter[38650] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.68112
[2024-02-27 22:37:00,856][PyLogger][INFO]: Rank[0]: Epoch[399/500], iter[38700] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50440
[2024-02-27 22:37:00,856][PyLogger][INFO]: Rank[1]: Epoch[399/500], iter[38700] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50585
[2024-02-27 22:37:28,696][PyLogger][INFO]: Rank[0]: Epoch[400/500], iter[38750] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 1.43144
[2024-02-27 22:37:28,696][PyLogger][INFO]: Rank[1]: Epoch[400/500], iter[38750] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.52017
[2024-02-27 22:37:55,157][PyLogger][INFO]: Rank[0]: Epoch[400/500], iter[38800] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50304
[2024-02-27 22:37:55,157][PyLogger][INFO]: Rank[1]: Epoch[400/500], iter[38800] - train_accuracy: 0.91130 - val_accuracy: 0.79650 - train_loss: 0.77716 - val_loss: 1.08752 - loss: 0.50245
[2024-02-27 22:38:50,051][PyLogger][INFO]: Rank[0]: Epoch[401/500], iter[38850] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.50137
[2024-02-27 22:38:50,051][PyLogger][INFO]: Rank[1]: Epoch[401/500], iter[38850] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.52045
[2024-02-27 22:39:17,260][PyLogger][INFO]: Rank[1]: Epoch[402/500], iter[38900] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.50738
[2024-02-27 22:39:17,260][PyLogger][INFO]: Rank[0]: Epoch[402/500], iter[38900] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.50317
[2024-02-27 22:39:43,784][PyLogger][INFO]: Rank[0]: Epoch[402/500], iter[38950] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.51346
[2024-02-27 22:39:43,784][PyLogger][INFO]: Rank[1]: Epoch[402/500], iter[38950] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.50247
[2024-02-27 22:40:10,934][PyLogger][INFO]: Rank[0]: Epoch[403/500], iter[39000] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 1.08147
[2024-02-27 22:40:10,934][PyLogger][INFO]: Rank[1]: Epoch[403/500], iter[39000] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.50820
[2024-02-27 22:40:37,357][PyLogger][INFO]: Rank[0]: Epoch[403/500], iter[39050] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.53193
[2024-02-27 22:40:37,357][PyLogger][INFO]: Rank[1]: Epoch[403/500], iter[39050] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.50354
[2024-02-27 22:41:05,390][PyLogger][INFO]: Rank[0]: Epoch[404/500], iter[39100] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.56263
[2024-02-27 22:41:05,390][PyLogger][INFO]: Rank[1]: Epoch[404/500], iter[39100] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 1.05478
[2024-02-27 22:41:31,858][PyLogger][INFO]: Rank[1]: Epoch[404/500], iter[39150] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.50184
[2024-02-27 22:41:31,858][PyLogger][INFO]: Rank[0]: Epoch[404/500], iter[39150] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.58288
[2024-02-27 22:41:59,067][PyLogger][INFO]: Rank[0]: Epoch[405/500], iter[39200] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.50138
[2024-02-27 22:41:59,067][PyLogger][INFO]: Rank[1]: Epoch[405/500], iter[39200] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.50129
[2024-02-27 22:42:25,562][PyLogger][INFO]: Rank[1]: Epoch[405/500], iter[39250] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.50416
[2024-02-27 22:42:25,562][PyLogger][INFO]: Rank[0]: Epoch[405/500], iter[39250] - train_accuracy: 0.90152 - val_accuracy: 0.74570 - train_loss: 0.77533 - val_loss: 1.20307 - loss: 0.50380
[2024-02-27 22:43:20,615][PyLogger][INFO]: Rank[0]: Epoch[406/500], iter[39300] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 1.01722
[2024-02-27 22:43:20,615][PyLogger][INFO]: Rank[1]: Epoch[406/500], iter[39300] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.51262
[2024-02-27 22:43:47,070][PyLogger][INFO]: Rank[1]: Epoch[406/500], iter[39350] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 1.88010
[2024-02-27 22:43:47,070][PyLogger][INFO]: Rank[0]: Epoch[406/500], iter[39350] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.50868
[2024-02-27 22:44:15,122][PyLogger][INFO]: Rank[0]: Epoch[407/500], iter[39400] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.50256
[2024-02-27 22:44:15,122][PyLogger][INFO]: Rank[1]: Epoch[407/500], iter[39400] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.51039
[2024-02-27 22:44:41,577][PyLogger][INFO]: Rank[1]: Epoch[407/500], iter[39450] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.50385
[2024-02-27 22:44:41,577][PyLogger][INFO]: Rank[0]: Epoch[407/500], iter[39450] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 1.38252
[2024-02-27 22:45:08,976][PyLogger][INFO]: Rank[0]: Epoch[408/500], iter[39500] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.75750
[2024-02-27 22:45:08,976][PyLogger][INFO]: Rank[1]: Epoch[408/500], iter[39500] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.50226
[2024-02-27 22:45:35,372][PyLogger][INFO]: Rank[0]: Epoch[408/500], iter[39550] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.50259
[2024-02-27 22:45:35,372][PyLogger][INFO]: Rank[1]: Epoch[408/500], iter[39550] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.50643
[2024-02-27 22:46:02,536][PyLogger][INFO]: Rank[0]: Epoch[409/500], iter[39600] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.57524
[2024-02-27 22:46:02,536][PyLogger][INFO]: Rank[1]: Epoch[409/500], iter[39600] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.50236
[2024-02-27 22:46:28,983][PyLogger][INFO]: Rank[0]: Epoch[409/500], iter[39650] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 1.44112
[2024-02-27 22:46:28,983][PyLogger][INFO]: Rank[1]: Epoch[409/500], iter[39650] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.53253
[2024-02-27 22:46:56,289][PyLogger][INFO]: Rank[0]: Epoch[410/500], iter[39700] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.53269
[2024-02-27 22:46:56,289][PyLogger][INFO]: Rank[1]: Epoch[410/500], iter[39700] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.74199
[2024-02-27 22:47:22,762][PyLogger][INFO]: Rank[0]: Epoch[410/500], iter[39750] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.50400
[2024-02-27 22:47:22,762][PyLogger][INFO]: Rank[1]: Epoch[410/500], iter[39750] - train_accuracy: 0.91438 - val_accuracy: 0.73470 - train_loss: 0.77520 - val_loss: 1.33817 - loss: 0.51338
[2024-02-27 22:48:17,606][PyLogger][INFO]: Rank[0]: Epoch[411/500], iter[39800] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50554
[2024-02-27 22:48:17,606][PyLogger][INFO]: Rank[1]: Epoch[411/500], iter[39800] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50178
[2024-02-27 22:48:43,955][PyLogger][INFO]: Rank[0]: Epoch[411/500], iter[39850] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50246
[2024-02-27 22:48:43,955][PyLogger][INFO]: Rank[1]: Epoch[411/500], iter[39850] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50642
[2024-02-27 22:49:11,162][PyLogger][INFO]: Rank[1]: Epoch[412/500], iter[39900] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50902
[2024-02-27 22:49:11,163][PyLogger][INFO]: Rank[0]: Epoch[412/500], iter[39900] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50213
[2024-02-27 22:49:37,743][PyLogger][INFO]: Rank[0]: Epoch[412/500], iter[39950] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.52077
[2024-02-27 22:49:37,743][PyLogger][INFO]: Rank[1]: Epoch[412/500], iter[39950] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.52292
[2024-02-27 22:50:04,963][PyLogger][INFO]: Rank[1]: Epoch[413/500], iter[40000] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50260
[2024-02-27 22:50:04,963][PyLogger][INFO]: Rank[0]: Epoch[413/500], iter[40000] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.65925
[2024-02-27 22:50:31,307][PyLogger][INFO]: Rank[0]: Epoch[413/500], iter[40050] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50572
[2024-02-27 22:50:31,307][PyLogger][INFO]: Rank[1]: Epoch[413/500], iter[40050] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.51217
[2024-02-27 22:50:58,487][PyLogger][INFO]: Rank[1]: Epoch[414/500], iter[40100] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50192
[2024-02-27 22:50:58,487][PyLogger][INFO]: Rank[0]: Epoch[414/500], iter[40100] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50405
[2024-02-27 22:51:24,992][PyLogger][INFO]: Rank[0]: Epoch[414/500], iter[40150] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50347
[2024-02-27 22:51:24,992][PyLogger][INFO]: Rank[1]: Epoch[414/500], iter[40150] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.63690
[2024-02-27 22:51:52,153][PyLogger][INFO]: Rank[0]: Epoch[415/500], iter[40200] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50653
[2024-02-27 22:51:52,153][PyLogger][INFO]: Rank[1]: Epoch[415/500], iter[40200] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.54054
[2024-02-27 22:52:18,547][PyLogger][INFO]: Rank[1]: Epoch[415/500], iter[40250] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 1.06893
[2024-02-27 22:52:18,547][PyLogger][INFO]: Rank[0]: Epoch[415/500], iter[40250] - train_accuracy: 0.95061 - val_accuracy: 0.73410 - train_loss: 0.64331 - val_loss: 1.26743 - loss: 0.50375
[2024-02-27 22:53:13,285][PyLogger][INFO]: Rank[0]: Epoch[416/500], iter[40300] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50857
[2024-02-27 22:53:13,285][PyLogger][INFO]: Rank[1]: Epoch[416/500], iter[40300] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50462
[2024-02-27 22:53:39,693][PyLogger][INFO]: Rank[0]: Epoch[416/500], iter[40350] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50376
[2024-02-27 22:53:39,693][PyLogger][INFO]: Rank[1]: Epoch[416/500], iter[40350] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.51027
[2024-02-27 22:54:06,831][PyLogger][INFO]: Rank[0]: Epoch[417/500], iter[40400] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50340
[2024-02-27 22:54:06,831][PyLogger][INFO]: Rank[1]: Epoch[417/500], iter[40400] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50690
[2024-02-27 22:54:34,020][PyLogger][INFO]: Rank[0]: Epoch[418/500], iter[40450] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.53035
[2024-02-27 22:54:34,020][PyLogger][INFO]: Rank[1]: Epoch[418/500], iter[40450] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50283
[2024-02-27 22:55:00,329][PyLogger][INFO]: Rank[0]: Epoch[418/500], iter[40500] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50321
[2024-02-27 22:55:00,329][PyLogger][INFO]: Rank[1]: Epoch[418/500], iter[40500] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.51273
[2024-02-27 22:55:27,529][PyLogger][INFO]: Rank[1]: Epoch[419/500], iter[40550] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50312
[2024-02-27 22:55:27,529][PyLogger][INFO]: Rank[0]: Epoch[419/500], iter[40550] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 1.89915
[2024-02-27 22:55:53,916][PyLogger][INFO]: Rank[0]: Epoch[419/500], iter[40600] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50270
[2024-02-27 22:55:53,916][PyLogger][INFO]: Rank[1]: Epoch[419/500], iter[40600] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50185
[2024-02-27 22:56:21,077][PyLogger][INFO]: Rank[1]: Epoch[420/500], iter[40650] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50310
[2024-02-27 22:56:21,077][PyLogger][INFO]: Rank[0]: Epoch[420/500], iter[40650] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 1.45974
[2024-02-27 22:56:47,398][PyLogger][INFO]: Rank[0]: Epoch[420/500], iter[40700] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50364
[2024-02-27 22:56:47,398][PyLogger][INFO]: Rank[1]: Epoch[420/500], iter[40700] - train_accuracy: 0.93714 - val_accuracy: 0.79820 - train_loss: 0.69853 - val_loss: 1.07117 - loss: 0.50315
[2024-02-27 22:57:42,048][PyLogger][INFO]: Rank[0]: Epoch[421/500], iter[40750] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.50456
[2024-02-27 22:57:42,048][PyLogger][INFO]: Rank[1]: Epoch[421/500], iter[40750] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.51240
[2024-02-27 22:58:08,529][PyLogger][INFO]: Rank[0]: Epoch[421/500], iter[40800] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.53195
[2024-02-27 22:58:08,529][PyLogger][INFO]: Rank[1]: Epoch[421/500], iter[40800] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.63827
[2024-02-27 22:58:35,589][PyLogger][INFO]: Rank[1]: Epoch[422/500], iter[40850] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.50404
[2024-02-27 22:58:35,589][PyLogger][INFO]: Rank[0]: Epoch[422/500], iter[40850] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.50754
[2024-02-27 22:59:01,965][PyLogger][INFO]: Rank[0]: Epoch[422/500], iter[40900] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.50296
[2024-02-27 22:59:01,965][PyLogger][INFO]: Rank[1]: Epoch[422/500], iter[40900] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.50268
[2024-02-27 22:59:29,037][PyLogger][INFO]: Rank[0]: Epoch[423/500], iter[40950] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.50149
[2024-02-27 22:59:29,037][PyLogger][INFO]: Rank[1]: Epoch[423/500], iter[40950] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.50323
[2024-02-27 22:59:55,462][PyLogger][INFO]: Rank[0]: Epoch[423/500], iter[41000] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.50236
[2024-02-27 22:59:55,462][PyLogger][INFO]: Rank[1]: Epoch[423/500], iter[41000] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.50287
[2024-02-27 23:00:23,125][PyLogger][INFO]: Rank[0]: Epoch[424/500], iter[41050] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.51226
[2024-02-27 23:00:23,125][PyLogger][INFO]: Rank[1]: Epoch[424/500], iter[41050] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.50223
[2024-02-27 23:00:49,486][PyLogger][INFO]: Rank[0]: Epoch[424/500], iter[41100] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.52358
[2024-02-27 23:00:49,486][PyLogger][INFO]: Rank[1]: Epoch[424/500], iter[41100] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 1.94045
[2024-02-27 23:01:16,656][PyLogger][INFO]: Rank[0]: Epoch[425/500], iter[41150] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.50253
[2024-02-27 23:01:16,656][PyLogger][INFO]: Rank[1]: Epoch[425/500], iter[41150] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 1.00958
[2024-02-27 23:01:43,055][PyLogger][INFO]: Rank[1]: Epoch[425/500], iter[41200] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.51778
[2024-02-27 23:01:43,055][PyLogger][INFO]: Rank[0]: Epoch[425/500], iter[41200] - train_accuracy: 0.89483 - val_accuracy: 0.82680 - train_loss: 0.83363 - val_loss: 0.98279 - loss: 0.52199
[2024-02-27 23:02:37,703][PyLogger][INFO]: Rank[0]: Epoch[426/500], iter[41250] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50198
[2024-02-27 23:02:37,703][PyLogger][INFO]: Rank[1]: Epoch[426/500], iter[41250] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50502
[2024-02-27 23:03:04,103][PyLogger][INFO]: Rank[0]: Epoch[426/500], iter[41300] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 1.89647
[2024-02-27 23:03:04,103][PyLogger][INFO]: Rank[1]: Epoch[426/500], iter[41300] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50316
[2024-02-27 23:03:31,298][PyLogger][INFO]: Rank[0]: Epoch[427/500], iter[41350] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50290
[2024-02-27 23:03:31,298][PyLogger][INFO]: Rank[1]: Epoch[427/500], iter[41350] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50204
[2024-02-27 23:03:57,676][PyLogger][INFO]: Rank[0]: Epoch[427/500], iter[41400] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50530
[2024-02-27 23:03:57,676][PyLogger][INFO]: Rank[1]: Epoch[427/500], iter[41400] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50334
[2024-02-27 23:04:24,856][PyLogger][INFO]: Rank[0]: Epoch[428/500], iter[41450] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50301
[2024-02-27 23:04:24,856][PyLogger][INFO]: Rank[1]: Epoch[428/500], iter[41450] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.51227
[2024-02-27 23:04:51,205][PyLogger][INFO]: Rank[0]: Epoch[428/500], iter[41500] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50669
[2024-02-27 23:04:51,205][PyLogger][INFO]: Rank[1]: Epoch[428/500], iter[41500] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50634
[2024-02-27 23:05:18,702][PyLogger][INFO]: Rank[1]: Epoch[429/500], iter[41550] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50972
[2024-02-27 23:05:18,702][PyLogger][INFO]: Rank[0]: Epoch[429/500], iter[41550] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50346
[2024-02-27 23:05:45,114][PyLogger][INFO]: Rank[0]: Epoch[429/500], iter[41600] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50300
[2024-02-27 23:05:45,114][PyLogger][INFO]: Rank[1]: Epoch[429/500], iter[41600] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.51672
[2024-02-27 23:06:12,370][PyLogger][INFO]: Rank[0]: Epoch[430/500], iter[41650] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50818
[2024-02-27 23:06:12,370][PyLogger][INFO]: Rank[1]: Epoch[430/500], iter[41650] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50205
[2024-02-27 23:06:38,744][PyLogger][INFO]: Rank[1]: Epoch[430/500], iter[41700] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.50523
[2024-02-27 23:06:38,744][PyLogger][INFO]: Rank[0]: Epoch[430/500], iter[41700] - train_accuracy: 0.93508 - val_accuracy: 0.78850 - train_loss: 0.70237 - val_loss: 1.13945 - loss: 0.87636
[2024-02-27 23:07:33,859][PyLogger][INFO]: Rank[0]: Epoch[431/500], iter[41750] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50597
[2024-02-27 23:07:33,859][PyLogger][INFO]: Rank[1]: Epoch[431/500], iter[41750] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50286
[2024-02-27 23:08:00,255][PyLogger][INFO]: Rank[0]: Epoch[431/500], iter[41800] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50278
[2024-02-27 23:08:00,255][PyLogger][INFO]: Rank[1]: Epoch[431/500], iter[41800] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50293
[2024-02-27 23:08:27,546][PyLogger][INFO]: Rank[0]: Epoch[432/500], iter[41850] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.53373
[2024-02-27 23:08:27,546][PyLogger][INFO]: Rank[1]: Epoch[432/500], iter[41850] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50117
[2024-02-27 23:08:54,049][PyLogger][INFO]: Rank[1]: Epoch[432/500], iter[41900] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50340
[2024-02-27 23:08:54,049][PyLogger][INFO]: Rank[0]: Epoch[432/500], iter[41900] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50734
[2024-02-27 23:09:21,239][PyLogger][INFO]: Rank[0]: Epoch[433/500], iter[41950] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50285
[2024-02-27 23:09:21,239][PyLogger][INFO]: Rank[1]: Epoch[433/500], iter[41950] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 1.80681
[2024-02-27 23:09:47,577][PyLogger][INFO]: Rank[1]: Epoch[433/500], iter[42000] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50189
[2024-02-27 23:09:47,577][PyLogger][INFO]: Rank[0]: Epoch[433/500], iter[42000] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50818
[2024-02-27 23:10:14,794][PyLogger][INFO]: Rank[0]: Epoch[434/500], iter[42050] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50449
[2024-02-27 23:10:14,794][PyLogger][INFO]: Rank[1]: Epoch[434/500], iter[42050] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.51904
[2024-02-27 23:10:42,329][PyLogger][INFO]: Rank[0]: Epoch[435/500], iter[42100] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50599
[2024-02-27 23:10:42,329][PyLogger][INFO]: Rank[1]: Epoch[435/500], iter[42100] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50155
[2024-02-27 23:11:08,769][PyLogger][INFO]: Rank[1]: Epoch[435/500], iter[42150] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.51793
[2024-02-27 23:11:08,769][PyLogger][INFO]: Rank[0]: Epoch[435/500], iter[42150] - train_accuracy: 0.90915 - val_accuracy: 0.76440 - train_loss: 0.77751 - val_loss: 1.17797 - loss: 0.50454
[2024-02-27 23:12:03,597][PyLogger][INFO]: Rank[0]: Epoch[436/500], iter[42200] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50446
[2024-02-27 23:12:03,597][PyLogger][INFO]: Rank[1]: Epoch[436/500], iter[42200] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50233
[2024-02-27 23:12:29,948][PyLogger][INFO]: Rank[0]: Epoch[436/500], iter[42250] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.84093
[2024-02-27 23:12:29,948][PyLogger][INFO]: Rank[1]: Epoch[436/500], iter[42250] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.59150
[2024-02-27 23:12:57,097][PyLogger][INFO]: Rank[0]: Epoch[437/500], iter[42300] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50770
[2024-02-27 23:12:57,097][PyLogger][INFO]: Rank[1]: Epoch[437/500], iter[42300] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50579
[2024-02-27 23:13:23,426][PyLogger][INFO]: Rank[0]: Epoch[437/500], iter[42350] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50707
[2024-02-27 23:13:23,426][PyLogger][INFO]: Rank[1]: Epoch[437/500], iter[42350] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50290
[2024-02-27 23:13:50,646][PyLogger][INFO]: Rank[0]: Epoch[438/500], iter[42400] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50424
[2024-02-27 23:13:50,646][PyLogger][INFO]: Rank[1]: Epoch[438/500], iter[42400] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50392
[2024-02-27 23:14:17,046][PyLogger][INFO]: Rank[1]: Epoch[438/500], iter[42450] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50261
[2024-02-27 23:14:17,046][PyLogger][INFO]: Rank[0]: Epoch[438/500], iter[42450] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50198
[2024-02-27 23:14:44,128][PyLogger][INFO]: Rank[0]: Epoch[439/500], iter[42500] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50159
[2024-02-27 23:14:44,128][PyLogger][INFO]: Rank[1]: Epoch[439/500], iter[42500] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 1.82605
[2024-02-27 23:15:10,447][PyLogger][INFO]: Rank[1]: Epoch[439/500], iter[42550] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50533
[2024-02-27 23:15:10,447][PyLogger][INFO]: Rank[0]: Epoch[439/500], iter[42550] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50575
[2024-02-27 23:15:37,750][PyLogger][INFO]: Rank[1]: Epoch[440/500], iter[42600] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50205
[2024-02-27 23:15:37,750][PyLogger][INFO]: Rank[0]: Epoch[440/500], iter[42600] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50158
[2024-02-27 23:16:04,091][PyLogger][INFO]: Rank[1]: Epoch[440/500], iter[42650] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50178
[2024-02-27 23:16:04,091][PyLogger][INFO]: Rank[0]: Epoch[440/500], iter[42650] - train_accuracy: 0.93680 - val_accuracy: 0.78770 - train_loss: 0.69282 - val_loss: 1.11938 - loss: 0.50277
[2024-02-27 23:16:59,472][PyLogger][INFO]: Rank[0]: Epoch[441/500], iter[42700] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.53093
[2024-02-27 23:16:59,472][PyLogger][INFO]: Rank[1]: Epoch[441/500], iter[42700] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.52466
[2024-02-27 23:17:25,737][PyLogger][INFO]: Rank[0]: Epoch[441/500], iter[42750] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.51236
[2024-02-27 23:17:25,737][PyLogger][INFO]: Rank[1]: Epoch[441/500], iter[42750] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50290
[2024-02-27 23:17:52,958][PyLogger][INFO]: Rank[0]: Epoch[442/500], iter[42800] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50573
[2024-02-27 23:17:52,958][PyLogger][INFO]: Rank[1]: Epoch[442/500], iter[42800] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50997
[2024-02-27 23:18:19,328][PyLogger][INFO]: Rank[0]: Epoch[442/500], iter[42850] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50293
[2024-02-27 23:18:19,328][PyLogger][INFO]: Rank[1]: Epoch[442/500], iter[42850] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50184
[2024-02-27 23:18:47,441][PyLogger][INFO]: Rank[0]: Epoch[443/500], iter[42900] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 1.36563
[2024-02-27 23:18:47,441][PyLogger][INFO]: Rank[1]: Epoch[443/500], iter[42900] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.62518
[2024-02-27 23:19:13,885][PyLogger][INFO]: Rank[0]: Epoch[443/500], iter[42950] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50396
[2024-02-27 23:19:13,885][PyLogger][INFO]: Rank[1]: Epoch[443/500], iter[42950] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50307
[2024-02-27 23:19:41,726][PyLogger][INFO]: Rank[0]: Epoch[444/500], iter[43000] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50279
[2024-02-27 23:19:41,726][PyLogger][INFO]: Rank[1]: Epoch[444/500], iter[43000] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50509
[2024-02-27 23:20:08,192][PyLogger][INFO]: Rank[1]: Epoch[444/500], iter[43050] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50961
[2024-02-27 23:20:08,192][PyLogger][INFO]: Rank[0]: Epoch[444/500], iter[43050] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50269
[2024-02-27 23:20:35,301][PyLogger][INFO]: Rank[0]: Epoch[445/500], iter[43100] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50561
[2024-02-27 23:20:35,301][PyLogger][INFO]: Rank[1]: Epoch[445/500], iter[43100] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50380
[2024-02-27 23:21:01,788][PyLogger][INFO]: Rank[1]: Epoch[445/500], iter[43150] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50288
[2024-02-27 23:21:01,788][PyLogger][INFO]: Rank[0]: Epoch[445/500], iter[43150] - train_accuracy: 0.91265 - val_accuracy: 0.70450 - train_loss: 0.73299 - val_loss: 1.29751 - loss: 0.50300
[2024-02-27 23:21:56,385][PyLogger][INFO]: Rank[0]: Epoch[446/500], iter[43200] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.51185
[2024-02-27 23:21:56,385][PyLogger][INFO]: Rank[1]: Epoch[446/500], iter[43200] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 1.43635
[2024-02-27 23:22:22,697][PyLogger][INFO]: Rank[0]: Epoch[446/500], iter[43250] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50543
[2024-02-27 23:22:22,697][PyLogger][INFO]: Rank[1]: Epoch[446/500], iter[43250] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.56229
[2024-02-27 23:22:49,752][PyLogger][INFO]: Rank[0]: Epoch[447/500], iter[43300] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.98618
[2024-02-27 23:22:49,752][PyLogger][INFO]: Rank[1]: Epoch[447/500], iter[43300] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50643
[2024-02-27 23:23:16,140][PyLogger][INFO]: Rank[1]: Epoch[447/500], iter[43350] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50464
[2024-02-27 23:23:16,140][PyLogger][INFO]: Rank[0]: Epoch[447/500], iter[43350] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.51970
[2024-02-27 23:23:43,318][PyLogger][INFO]: Rank[0]: Epoch[448/500], iter[43400] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50196
[2024-02-27 23:23:43,318][PyLogger][INFO]: Rank[1]: Epoch[448/500], iter[43400] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50328
[2024-02-27 23:24:09,710][PyLogger][INFO]: Rank[0]: Epoch[448/500], iter[43450] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50556
[2024-02-27 23:24:09,710][PyLogger][INFO]: Rank[1]: Epoch[448/500], iter[43450] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50520
[2024-02-27 23:24:36,875][PyLogger][INFO]: Rank[0]: Epoch[449/500], iter[43500] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50382
[2024-02-27 23:24:36,875][PyLogger][INFO]: Rank[1]: Epoch[449/500], iter[43500] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50270
[2024-02-27 23:25:03,237][PyLogger][INFO]: Rank[0]: Epoch[449/500], iter[43550] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50412
[2024-02-27 23:25:03,237][PyLogger][INFO]: Rank[1]: Epoch[449/500], iter[43550] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50237
[2024-02-27 23:25:30,684][PyLogger][INFO]: Rank[0]: Epoch[450/500], iter[43600] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50422
[2024-02-27 23:25:30,685][PyLogger][INFO]: Rank[1]: Epoch[450/500], iter[43600] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50333
[2024-02-27 23:25:57,112][PyLogger][INFO]: Rank[0]: Epoch[450/500], iter[43650] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50626
[2024-02-27 23:25:57,112][PyLogger][INFO]: Rank[1]: Epoch[450/500], iter[43650] - train_accuracy: 0.91620 - val_accuracy: 0.79250 - train_loss: 0.76644 - val_loss: 1.10243 - loss: 0.50836
[2024-02-27 23:26:51,647][PyLogger][INFO]: Rank[0]: Epoch[451/500], iter[43700] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50775
[2024-02-27 23:26:51,647][PyLogger][INFO]: Rank[1]: Epoch[451/500], iter[43700] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50362
[2024-02-27 23:27:18,827][PyLogger][INFO]: Rank[0]: Epoch[452/500], iter[43750] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50456
[2024-02-27 23:27:18,827][PyLogger][INFO]: Rank[1]: Epoch[452/500], iter[43750] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50358
[2024-02-27 23:27:45,140][PyLogger][INFO]: Rank[0]: Epoch[452/500], iter[43800] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.99715
[2024-02-27 23:27:45,141][PyLogger][INFO]: Rank[1]: Epoch[452/500], iter[43800] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 1.38952
[2024-02-27 23:28:12,194][PyLogger][INFO]: Rank[0]: Epoch[453/500], iter[43850] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.57480
[2024-02-27 23:28:12,194][PyLogger][INFO]: Rank[1]: Epoch[453/500], iter[43850] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.51836
[2024-02-27 23:28:38,528][PyLogger][INFO]: Rank[0]: Epoch[453/500], iter[43900] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.64709
[2024-02-27 23:28:38,528][PyLogger][INFO]: Rank[1]: Epoch[453/500], iter[43900] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50442
[2024-02-27 23:29:05,649][PyLogger][INFO]: Rank[0]: Epoch[454/500], iter[43950] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.52892
[2024-02-27 23:29:05,649][PyLogger][INFO]: Rank[1]: Epoch[454/500], iter[43950] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50284
[2024-02-27 23:29:31,927][PyLogger][INFO]: Rank[0]: Epoch[454/500], iter[44000] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50210
[2024-02-27 23:29:31,927][PyLogger][INFO]: Rank[1]: Epoch[454/500], iter[44000] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50184
[2024-02-27 23:29:59,483][PyLogger][INFO]: Rank[0]: Epoch[455/500], iter[44050] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50151
[2024-02-27 23:29:59,483][PyLogger][INFO]: Rank[1]: Epoch[455/500], iter[44050] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50340
[2024-02-27 23:30:25,859][PyLogger][INFO]: Rank[0]: Epoch[455/500], iter[44100] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50277
[2024-02-27 23:30:25,860][PyLogger][INFO]: Rank[1]: Epoch[455/500], iter[44100] - train_accuracy: 0.90840 - val_accuracy: 0.75730 - train_loss: 0.78122 - val_loss: 1.22985 - loss: 0.50273
[2024-02-27 23:31:20,779][PyLogger][INFO]: Rank[1]: Epoch[456/500], iter[44150] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.55592
[2024-02-27 23:31:20,779][PyLogger][INFO]: Rank[0]: Epoch[456/500], iter[44150] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50361
[2024-02-27 23:31:47,085][PyLogger][INFO]: Rank[0]: Epoch[456/500], iter[44200] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50730
[2024-02-27 23:31:47,085][PyLogger][INFO]: Rank[1]: Epoch[456/500], iter[44200] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50617
[2024-02-27 23:32:14,305][PyLogger][INFO]: Rank[1]: Epoch[457/500], iter[44250] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 1.00546
[2024-02-27 23:32:14,305][PyLogger][INFO]: Rank[0]: Epoch[457/500], iter[44250] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.74189
[2024-02-27 23:32:40,572][PyLogger][INFO]: Rank[1]: Epoch[457/500], iter[44300] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50341
[2024-02-27 23:32:40,573][PyLogger][INFO]: Rank[0]: Epoch[457/500], iter[44300] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50497
[2024-02-27 23:33:07,835][PyLogger][INFO]: Rank[0]: Epoch[458/500], iter[44350] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50832
[2024-02-27 23:33:07,835][PyLogger][INFO]: Rank[1]: Epoch[458/500], iter[44350] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.53639
[2024-02-27 23:33:34,084][PyLogger][INFO]: Rank[1]: Epoch[458/500], iter[44400] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50165
[2024-02-27 23:33:34,084][PyLogger][INFO]: Rank[0]: Epoch[458/500], iter[44400] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50256
[2024-02-27 23:34:01,180][PyLogger][INFO]: Rank[0]: Epoch[459/500], iter[44450] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50214
[2024-02-27 23:34:01,180][PyLogger][INFO]: Rank[1]: Epoch[459/500], iter[44450] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 1.40149
[2024-02-27 23:34:27,622][PyLogger][INFO]: Rank[1]: Epoch[459/500], iter[44500] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50350
[2024-02-27 23:34:27,622][PyLogger][INFO]: Rank[0]: Epoch[459/500], iter[44500] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.75755
[2024-02-27 23:34:54,693][PyLogger][INFO]: Rank[0]: Epoch[460/500], iter[44550] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50407
[2024-02-27 23:34:54,693][PyLogger][INFO]: Rank[1]: Epoch[460/500], iter[44550] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50178
[2024-02-27 23:35:21,019][PyLogger][INFO]: Rank[1]: Epoch[460/500], iter[44600] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.51362
[2024-02-27 23:35:21,019][PyLogger][INFO]: Rank[0]: Epoch[460/500], iter[44600] - train_accuracy: 0.95510 - val_accuracy: 0.77640 - train_loss: 0.62162 - val_loss: 1.11018 - loss: 0.50370
[2024-02-27 23:36:15,610][PyLogger][INFO]: Rank[0]: Epoch[461/500], iter[44650] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.50290
[2024-02-27 23:36:15,610][PyLogger][INFO]: Rank[1]: Epoch[461/500], iter[44650] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.51714
[2024-02-27 23:36:42,009][PyLogger][INFO]: Rank[1]: Epoch[461/500], iter[44700] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.98666
[2024-02-27 23:36:42,009][PyLogger][INFO]: Rank[0]: Epoch[461/500], iter[44700] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.50545
[2024-02-27 23:37:09,216][PyLogger][INFO]: Rank[0]: Epoch[462/500], iter[44750] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.50853
[2024-02-27 23:37:09,216][PyLogger][INFO]: Rank[1]: Epoch[462/500], iter[44750] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.50746
[2024-02-27 23:37:35,489][PyLogger][INFO]: Rank[1]: Epoch[462/500], iter[44800] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.50373
[2024-02-27 23:37:35,489][PyLogger][INFO]: Rank[0]: Epoch[462/500], iter[44800] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.57817
[2024-02-27 23:38:02,626][PyLogger][INFO]: Rank[0]: Epoch[463/500], iter[44850] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.50249
[2024-02-27 23:38:02,626][PyLogger][INFO]: Rank[1]: Epoch[463/500], iter[44850] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.55097
[2024-02-27 23:38:29,005][PyLogger][INFO]: Rank[1]: Epoch[463/500], iter[44900] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.63504
[2024-02-27 23:38:29,005][PyLogger][INFO]: Rank[0]: Epoch[463/500], iter[44900] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.52393
[2024-02-27 23:38:56,009][PyLogger][INFO]: Rank[1]: Epoch[464/500], iter[44950] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.51417
[2024-02-27 23:38:56,009][PyLogger][INFO]: Rank[0]: Epoch[464/500], iter[44950] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.50887
[2024-02-27 23:39:22,541][PyLogger][INFO]: Rank[0]: Epoch[464/500], iter[45000] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.50180
[2024-02-27 23:39:22,541][PyLogger][INFO]: Rank[1]: Epoch[464/500], iter[45000] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.50594
[2024-02-27 23:39:49,593][PyLogger][INFO]: Rank[0]: Epoch[465/500], iter[45050] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.50438
[2024-02-27 23:39:49,593][PyLogger][INFO]: Rank[1]: Epoch[465/500], iter[45050] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 0.55389
[2024-02-27 23:40:15,926][PyLogger][INFO]: Rank[0]: Epoch[465/500], iter[45100] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 1.81449
[2024-02-27 23:40:15,927][PyLogger][INFO]: Rank[1]: Epoch[465/500], iter[45100] - train_accuracy: 0.90031 - val_accuracy: 0.80690 - train_loss: 0.81011 - val_loss: 1.05831 - loss: 1.47289
[2024-02-27 23:41:10,608][PyLogger][INFO]: Rank[0]: Epoch[466/500], iter[45150] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.50395
[2024-02-27 23:41:10,608][PyLogger][INFO]: Rank[1]: Epoch[466/500], iter[45150] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.50379
[2024-02-27 23:41:37,117][PyLogger][INFO]: Rank[0]: Epoch[466/500], iter[45200] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.51159
[2024-02-27 23:41:37,117][PyLogger][INFO]: Rank[1]: Epoch[466/500], iter[45200] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.51903
[2024-02-27 23:42:04,128][PyLogger][INFO]: Rank[0]: Epoch[467/500], iter[45250] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.50333
[2024-02-27 23:42:04,128][PyLogger][INFO]: Rank[1]: Epoch[467/500], iter[45250] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.50421
[2024-02-27 23:42:31,437][PyLogger][INFO]: Rank[0]: Epoch[468/500], iter[45300] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.50496
[2024-02-27 23:42:31,437][PyLogger][INFO]: Rank[1]: Epoch[468/500], iter[45300] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.53907
[2024-02-27 23:42:57,801][PyLogger][INFO]: Rank[1]: Epoch[468/500], iter[45350] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.50539
[2024-02-27 23:42:57,801][PyLogger][INFO]: Rank[0]: Epoch[468/500], iter[45350] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.50361
[2024-02-27 23:43:24,949][PyLogger][INFO]: Rank[0]: Epoch[469/500], iter[45400] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.51280
[2024-02-27 23:43:24,949][PyLogger][INFO]: Rank[1]: Epoch[469/500], iter[45400] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.51567
[2024-02-27 23:43:51,408][PyLogger][INFO]: Rank[1]: Epoch[469/500], iter[45450] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.51824
[2024-02-27 23:43:51,408][PyLogger][INFO]: Rank[0]: Epoch[469/500], iter[45450] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.50188
[2024-02-27 23:44:18,530][PyLogger][INFO]: Rank[0]: Epoch[470/500], iter[45500] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.50753
[2024-02-27 23:44:18,530][PyLogger][INFO]: Rank[1]: Epoch[470/500], iter[45500] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.50604
[2024-02-27 23:44:45,085][PyLogger][INFO]: Rank[1]: Epoch[470/500], iter[45550] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.51036
[2024-02-27 23:44:45,085][PyLogger][INFO]: Rank[0]: Epoch[470/500], iter[45550] - train_accuracy: 0.89691 - val_accuracy: 0.76360 - train_loss: 0.81245 - val_loss: 1.14984 - loss: 0.51128
[2024-02-27 23:45:39,603][PyLogger][INFO]: Rank[0]: Epoch[471/500], iter[45600] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50415
[2024-02-27 23:45:39,603][PyLogger][INFO]: Rank[1]: Epoch[471/500], iter[45600] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50455
[2024-02-27 23:46:05,995][PyLogger][INFO]: Rank[1]: Epoch[471/500], iter[45650] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 1.43342
[2024-02-27 23:46:05,995][PyLogger][INFO]: Rank[0]: Epoch[471/500], iter[45650] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50436
[2024-02-27 23:46:33,165][PyLogger][INFO]: Rank[0]: Epoch[472/500], iter[45700] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50633
[2024-02-27 23:46:33,165][PyLogger][INFO]: Rank[1]: Epoch[472/500], iter[45700] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 1.03898
[2024-02-27 23:46:59,501][PyLogger][INFO]: Rank[0]: Epoch[472/500], iter[45750] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50515
[2024-02-27 23:46:59,501][PyLogger][INFO]: Rank[1]: Epoch[472/500], iter[45750] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50875
[2024-02-27 23:47:26,738][PyLogger][INFO]: Rank[0]: Epoch[473/500], iter[45800] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50519
[2024-02-27 23:47:26,738][PyLogger][INFO]: Rank[1]: Epoch[473/500], iter[45800] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.98976
[2024-02-27 23:47:53,133][PyLogger][INFO]: Rank[0]: Epoch[473/500], iter[45850] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50220
[2024-02-27 23:47:53,133][PyLogger][INFO]: Rank[1]: Epoch[473/500], iter[45850] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50952
[2024-02-27 23:48:20,159][PyLogger][INFO]: Rank[0]: Epoch[474/500], iter[45900] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.51898
[2024-02-27 23:48:20,159][PyLogger][INFO]: Rank[1]: Epoch[474/500], iter[45900] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50631
[2024-02-27 23:48:46,591][PyLogger][INFO]: Rank[1]: Epoch[474/500], iter[45950] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.52054
[2024-02-27 23:48:46,591][PyLogger][INFO]: Rank[0]: Epoch[474/500], iter[45950] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50314
[2024-02-27 23:49:14,157][PyLogger][INFO]: Rank[0]: Epoch[475/500], iter[46000] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50966
[2024-02-27 23:49:14,157][PyLogger][INFO]: Rank[1]: Epoch[475/500], iter[46000] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.65253
[2024-02-27 23:49:40,612][PyLogger][INFO]: Rank[0]: Epoch[475/500], iter[46050] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50542
[2024-02-27 23:49:40,612][PyLogger][INFO]: Rank[1]: Epoch[475/500], iter[46050] - train_accuracy: 0.91738 - val_accuracy: 0.72010 - train_loss: 0.74688 - val_loss: 1.28491 - loss: 0.50697
[2024-02-27 23:50:35,276][PyLogger][INFO]: Rank[0]: Epoch[476/500], iter[46100] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.50363
[2024-02-27 23:50:35,276][PyLogger][INFO]: Rank[1]: Epoch[476/500], iter[46100] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.50379
[2024-02-27 23:51:01,542][PyLogger][INFO]: Rank[1]: Epoch[476/500], iter[46150] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.50921
[2024-02-27 23:51:01,542][PyLogger][INFO]: Rank[0]: Epoch[476/500], iter[46150] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.50242
[2024-02-27 23:51:28,800][PyLogger][INFO]: Rank[0]: Epoch[477/500], iter[46200] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 1.07092
[2024-02-27 23:51:28,800][PyLogger][INFO]: Rank[1]: Epoch[477/500], iter[46200] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.50764
[2024-02-27 23:51:55,203][PyLogger][INFO]: Rank[1]: Epoch[477/500], iter[46250] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.50676
[2024-02-27 23:51:55,203][PyLogger][INFO]: Rank[0]: Epoch[477/500], iter[46250] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.53101
[2024-02-27 23:52:22,395][PyLogger][INFO]: Rank[0]: Epoch[478/500], iter[46300] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.50559
[2024-02-27 23:52:22,395][PyLogger][INFO]: Rank[1]: Epoch[478/500], iter[46300] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.50394
[2024-02-27 23:52:48,771][PyLogger][INFO]: Rank[0]: Epoch[478/500], iter[46350] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.53468
[2024-02-27 23:52:48,771][PyLogger][INFO]: Rank[1]: Epoch[478/500], iter[46350] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.50202
[2024-02-27 23:53:15,885][PyLogger][INFO]: Rank[0]: Epoch[479/500], iter[46400] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.57407
[2024-02-27 23:53:15,885][PyLogger][INFO]: Rank[1]: Epoch[479/500], iter[46400] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 1.89039
[2024-02-27 23:53:42,254][PyLogger][INFO]: Rank[0]: Epoch[479/500], iter[46450] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.52818
[2024-02-27 23:53:42,255][PyLogger][INFO]: Rank[1]: Epoch[479/500], iter[46450] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.58180
[2024-02-27 23:54:09,428][PyLogger][INFO]: Rank[1]: Epoch[480/500], iter[46500] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 1.09391
[2024-02-27 23:54:09,429][PyLogger][INFO]: Rank[0]: Epoch[480/500], iter[46500] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.50263
[2024-02-27 23:54:35,984][PyLogger][INFO]: Rank[0]: Epoch[480/500], iter[46550] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.52264
[2024-02-27 23:54:35,984][PyLogger][INFO]: Rank[1]: Epoch[480/500], iter[46550] - train_accuracy: 0.91308 - val_accuracy: 0.69640 - train_loss: 0.76143 - val_loss: 1.36352 - loss: 0.50345
[2024-02-27 23:55:30,545][PyLogger][INFO]: Rank[0]: Epoch[481/500], iter[46600] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50766
[2024-02-27 23:55:30,545][PyLogger][INFO]: Rank[1]: Epoch[481/500], iter[46600] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50581
[2024-02-27 23:55:56,788][PyLogger][INFO]: Rank[0]: Epoch[481/500], iter[46650] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 1.02729
[2024-02-27 23:55:56,788][PyLogger][INFO]: Rank[1]: Epoch[481/500], iter[46650] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50925
[2024-02-27 23:56:24,024][PyLogger][INFO]: Rank[1]: Epoch[482/500], iter[46700] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.51558
[2024-02-27 23:56:24,024][PyLogger][INFO]: Rank[0]: Epoch[482/500], iter[46700] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.99005
[2024-02-27 23:56:50,529][PyLogger][INFO]: Rank[0]: Epoch[482/500], iter[46750] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50704
[2024-02-27 23:56:50,529][PyLogger][INFO]: Rank[1]: Epoch[482/500], iter[46750] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.51456
[2024-02-27 23:57:17,895][PyLogger][INFO]: Rank[1]: Epoch[483/500], iter[46800] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.53440
[2024-02-27 23:57:17,895][PyLogger][INFO]: Rank[0]: Epoch[483/500], iter[46800] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.51108
[2024-02-27 23:57:44,282][PyLogger][INFO]: Rank[0]: Epoch[483/500], iter[46850] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50266
[2024-02-27 23:57:44,283][PyLogger][INFO]: Rank[1]: Epoch[483/500], iter[46850] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50684
[2024-02-27 23:58:11,474][PyLogger][INFO]: Rank[1]: Epoch[484/500], iter[46900] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50399
[2024-02-27 23:58:11,474][PyLogger][INFO]: Rank[0]: Epoch[484/500], iter[46900] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50210
[2024-02-27 23:58:38,621][PyLogger][INFO]: Rank[0]: Epoch[485/500], iter[46950] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50448
[2024-02-27 23:58:38,621][PyLogger][INFO]: Rank[1]: Epoch[485/500], iter[46950] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50795
[2024-02-27 23:59:04,951][PyLogger][INFO]: Rank[0]: Epoch[485/500], iter[47000] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50833
[2024-02-27 23:59:04,951][PyLogger][INFO]: Rank[1]: Epoch[485/500], iter[47000] - train_accuracy: 0.92159 - val_accuracy: 0.82210 - train_loss: 0.75586 - val_loss: 1.01059 - loss: 0.50368
[2024-02-27 23:59:59,586][PyLogger][INFO]: Rank[0]: Epoch[486/500], iter[47050] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50214
[2024-02-27 23:59:59,586][PyLogger][INFO]: Rank[1]: Epoch[486/500], iter[47050] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.51574
[2024-02-28 00:00:25,887][PyLogger][INFO]: Rank[1]: Epoch[486/500], iter[47100] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50267
[2024-02-28 00:00:25,887][PyLogger][INFO]: Rank[0]: Epoch[486/500], iter[47100] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.51081
[2024-02-28 00:00:52,963][PyLogger][INFO]: Rank[0]: Epoch[487/500], iter[47150] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50645
[2024-02-28 00:00:52,963][PyLogger][INFO]: Rank[1]: Epoch[487/500], iter[47150] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50798
[2024-02-28 00:01:19,292][PyLogger][INFO]: Rank[1]: Epoch[487/500], iter[47200] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50696
[2024-02-28 00:01:19,292][PyLogger][INFO]: Rank[0]: Epoch[487/500], iter[47200] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50349
[2024-02-28 00:01:46,548][PyLogger][INFO]: Rank[0]: Epoch[488/500], iter[47250] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.53700
[2024-02-28 00:01:46,548][PyLogger][INFO]: Rank[1]: Epoch[488/500], iter[47250] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50505
[2024-02-28 00:02:12,924][PyLogger][INFO]: Rank[0]: Epoch[488/500], iter[47300] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50313
[2024-02-28 00:02:12,924][PyLogger][INFO]: Rank[1]: Epoch[488/500], iter[47300] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50480
[2024-02-28 00:02:40,432][PyLogger][INFO]: Rank[1]: Epoch[489/500], iter[47350] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50233
[2024-02-28 00:02:40,432][PyLogger][INFO]: Rank[0]: Epoch[489/500], iter[47350] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50849
[2024-02-28 00:03:06,744][PyLogger][INFO]: Rank[0]: Epoch[489/500], iter[47400] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.51153
[2024-02-28 00:03:06,744][PyLogger][INFO]: Rank[1]: Epoch[489/500], iter[47400] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50760
[2024-02-28 00:03:34,001][PyLogger][INFO]: Rank[1]: Epoch[490/500], iter[47450] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.51010
[2024-02-28 00:03:34,001][PyLogger][INFO]: Rank[0]: Epoch[490/500], iter[47450] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.55331
[2024-02-28 00:04:00,328][PyLogger][INFO]: Rank[0]: Epoch[490/500], iter[47500] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50675
[2024-02-28 00:04:00,328][PyLogger][INFO]: Rank[1]: Epoch[490/500], iter[47500] - train_accuracy: 0.92228 - val_accuracy: 0.75390 - train_loss: 0.72205 - val_loss: 1.23250 - loss: 0.50473
[2024-02-28 00:04:54,934][PyLogger][INFO]: Rank[0]: Epoch[491/500], iter[47550] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50457
[2024-02-28 00:04:54,934][PyLogger][INFO]: Rank[1]: Epoch[491/500], iter[47550] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.55055
[2024-02-28 00:05:21,287][PyLogger][INFO]: Rank[0]: Epoch[491/500], iter[47600] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50416
[2024-02-28 00:05:21,287][PyLogger][INFO]: Rank[1]: Epoch[491/500], iter[47600] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50506
[2024-02-28 00:05:48,461][PyLogger][INFO]: Rank[1]: Epoch[492/500], iter[47650] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 1.33756
[2024-02-28 00:05:48,461][PyLogger][INFO]: Rank[0]: Epoch[492/500], iter[47650] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50915
[2024-02-28 00:06:14,888][PyLogger][INFO]: Rank[0]: Epoch[492/500], iter[47700] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50239
[2024-02-28 00:06:14,888][PyLogger][INFO]: Rank[1]: Epoch[492/500], iter[47700] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.51348
[2024-02-28 00:06:42,148][PyLogger][INFO]: Rank[1]: Epoch[493/500], iter[47750] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50686
[2024-02-28 00:06:42,148][PyLogger][INFO]: Rank[0]: Epoch[493/500], iter[47750] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50408
[2024-02-28 00:07:08,480][PyLogger][INFO]: Rank[0]: Epoch[493/500], iter[47800] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 1.10234
[2024-02-28 00:07:08,480][PyLogger][INFO]: Rank[1]: Epoch[493/500], iter[47800] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50804
[2024-02-28 00:07:35,993][PyLogger][INFO]: Rank[0]: Epoch[494/500], iter[47850] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50847
[2024-02-28 00:07:35,993][PyLogger][INFO]: Rank[1]: Epoch[494/500], iter[47850] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.53546
[2024-02-28 00:08:02,408][PyLogger][INFO]: Rank[0]: Epoch[494/500], iter[47900] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50278
[2024-02-28 00:08:02,408][PyLogger][INFO]: Rank[1]: Epoch[494/500], iter[47900] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50260
[2024-02-28 00:08:29,573][PyLogger][INFO]: Rank[0]: Epoch[495/500], iter[47950] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50484
[2024-02-28 00:08:29,574][PyLogger][INFO]: Rank[1]: Epoch[495/500], iter[47950] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50771
[2024-02-28 00:08:55,864][PyLogger][INFO]: Rank[1]: Epoch[495/500], iter[48000] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50962
[2024-02-28 00:08:55,864][PyLogger][INFO]: Rank[0]: Epoch[495/500], iter[48000] - train_accuracy: 0.92222 - val_accuracy: 0.77260 - train_loss: 0.72038 - val_loss: 1.07988 - loss: 0.50662
[2024-02-28 00:09:50,919][PyLogger][INFO]: Rank[0]: Epoch[496/500], iter[48050] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.54256
[2024-02-28 00:09:50,919][PyLogger][INFO]: Rank[1]: Epoch[496/500], iter[48050] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.51634
[2024-02-28 00:10:17,239][PyLogger][INFO]: Rank[0]: Epoch[496/500], iter[48100] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 1.81134
[2024-02-28 00:10:17,239][PyLogger][INFO]: Rank[1]: Epoch[496/500], iter[48100] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.52925
[2024-02-28 00:10:44,468][PyLogger][INFO]: Rank[0]: Epoch[497/500], iter[48150] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.51127
[2024-02-28 00:10:44,468][PyLogger][INFO]: Rank[1]: Epoch[497/500], iter[48150] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.50365
[2024-02-28 00:11:10,764][PyLogger][INFO]: Rank[0]: Epoch[497/500], iter[48200] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 1.39486
[2024-02-28 00:11:10,764][PyLogger][INFO]: Rank[1]: Epoch[497/500], iter[48200] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.50949
[2024-02-28 00:11:38,059][PyLogger][INFO]: Rank[0]: Epoch[498/500], iter[48250] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.62474
[2024-02-28 00:11:38,059][PyLogger][INFO]: Rank[1]: Epoch[498/500], iter[48250] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.50669
[2024-02-28 00:12:04,315][PyLogger][INFO]: Rank[1]: Epoch[498/500], iter[48300] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.50498
[2024-02-28 00:12:04,315][PyLogger][INFO]: Rank[0]: Epoch[498/500], iter[48300] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.51534
[2024-02-28 00:12:32,082][PyLogger][INFO]: Rank[0]: Epoch[499/500], iter[48350] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.57484
[2024-02-28 00:12:32,082][PyLogger][INFO]: Rank[1]: Epoch[499/500], iter[48350] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.51988
[2024-02-28 00:12:58,437][PyLogger][INFO]: Rank[0]: Epoch[499/500], iter[48400] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.50867
[2024-02-28 00:12:58,437][PyLogger][INFO]: Rank[1]: Epoch[499/500], iter[48400] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.57746
[2024-02-28 00:13:25,721][PyLogger][INFO]: Rank[0]: Epoch[500/500], iter[48450] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.50249
[2024-02-28 00:13:25,721][PyLogger][INFO]: Rank[1]: Epoch[500/500], iter[48450] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.50558
[2024-02-28 00:13:52,108][PyLogger][INFO]: Rank[0]: Epoch[500/500], iter[48500] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.98035
[2024-02-28 00:13:52,108][PyLogger][INFO]: Rank[1]: Epoch[500/500], iter[48500] - train_accuracy: 0.90989 - val_accuracy: 0.74990 - train_loss: 0.75448 - val_loss: 1.17333 - loss: 0.50701
Files already downloaded and verified
Files already downloaded and verified
2024-02-28 00:14:22,805 ignite.distributed.launcher.Parallel INFO: End of run
2024-02-28 00:14:27,249 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:14:27,249 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:14:27,249 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14aef2afe3e0>' in 2 processes
2024-02-28 00:14:35,208 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:14:35,624 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x150abe65c440>}
[2024-02-28 00:14:37,928][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:14:37,928][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:14:37,928][PyLogger][INFO]: World size: 2
[2024-02-28 00:14:37,929][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:14:37,929][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:14:37,929][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:14:37,929][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:14:37,930][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:14:37,930][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:14:37,930][PyLogger][INFO]: World size: 2
[2024-02-28 00:14:43,266][PyLogger][INFO]: Rank[0]: val_accuracy: 0.20900
[2024-02-28 00:14:43,266][PyLogger][INFO]: Rank[1]: val_accuracy: 0.20900
2024-02-28 00:14:44,787 ignite.distributed.launcher.Parallel INFO: End of run
new_size=2 done.
2024-02-28 00:14:48,901 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:14:48,901 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:14:48,901 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x15511fdb23e0>' in 2 processes
2024-02-28 00:14:57,026 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:14:57,426 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14be7021c740>}
[2024-02-28 00:14:57,640][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:14:57,640][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:14:57,640][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:14:57,640][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:14:57,641][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:14:57,641][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:14:57,641][PyLogger][INFO]: World size: 2
[2024-02-28 00:14:57,642][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:14:57,642][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:14:57,642][PyLogger][INFO]: World size: 2
[2024-02-28 00:15:02,872][PyLogger][INFO]: Rank[1]: val_accuracy: 0.33630
[2024-02-28 00:15:02,872][PyLogger][INFO]: Rank[0]: val_accuracy: 0.33630
2024-02-28 00:15:03,962 ignite.distributed.launcher.Parallel INFO: End of run
new_size=3 done.
2024-02-28 00:15:07,683 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:15:07,683 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:15:07,683 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14d97e6a23e0>' in 2 processes
2024-02-28 00:15:15,858 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:15:16,262 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14a882cd4f80>}
[2024-02-28 00:15:16,471][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:15:16,471][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:15:16,471][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:15:16,471][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:15:16,472][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:15:16,472][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:15:16,472][PyLogger][INFO]: World size: 2
[2024-02-28 00:15:16,476][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:15:16,476][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:15:16,476][PyLogger][INFO]: World size: 2
[2024-02-28 00:15:21,693][PyLogger][INFO]: Rank[1]: val_accuracy: 0.47110
[2024-02-28 00:15:21,693][PyLogger][INFO]: Rank[0]: val_accuracy: 0.47110
2024-02-28 00:15:23,187 ignite.distributed.launcher.Parallel INFO: End of run
new_size=4 done.
2024-02-28 00:15:28,321 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:15:28,321 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:15:28,321 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1466d61323e0>' in 2 processes
2024-02-28 00:15:35,808 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:15:36,210 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14ef30697800>}
[2024-02-28 00:15:36,421][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:15:36,421][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:15:36,421][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:15:36,421][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:15:36,422][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:15:36,423][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:15:36,423][PyLogger][INFO]: World size: 2
[2024-02-28 00:15:36,423][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:15:36,423][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:15:36,423][PyLogger][INFO]: World size: 2
[2024-02-28 00:15:41,739][PyLogger][INFO]: Rank[1]: val_accuracy: 0.61410
[2024-02-28 00:15:41,739][PyLogger][INFO]: Rank[0]: val_accuracy: 0.61410
2024-02-28 00:15:42,811 ignite.distributed.launcher.Parallel INFO: End of run
new_size=5 done.
2024-02-28 00:15:46,506 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:15:46,506 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:15:46,506 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14d53a5fa3e0>' in 2 processes
2024-02-28 00:15:54,744 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:15:55,265 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14847f2e6480>}
[2024-02-28 00:15:55,473][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:15:55,473][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:15:55,474][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:15:55,474][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:15:55,480][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:15:55,480][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:15:55,480][PyLogger][INFO]: World size: 2
[2024-02-28 00:15:55,481][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:15:55,481][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:15:55,481][PyLogger][INFO]: World size: 2
[2024-02-28 00:16:00,679][PyLogger][INFO]: Rank[0]: val_accuracy: 0.68600
[2024-02-28 00:16:00,679][PyLogger][INFO]: Rank[1]: val_accuracy: 0.68600
2024-02-28 00:16:02,173 ignite.distributed.launcher.Parallel INFO: End of run
new_size=6 done.
2024-02-28 00:16:05,946 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:16:05,946 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:16:05,946 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14747e3423e0>' in 2 processes
2024-02-28 00:16:14,032 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:16:14,438 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x15109aa2cb90>}
[2024-02-28 00:16:14,649][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:16:14,650][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:16:14,650][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:16:14,650][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:16:14,651][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:16:14,651][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:16:14,651][PyLogger][INFO]: World size: 2
[2024-02-28 00:16:14,655][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:16:14,655][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:16:14,655][PyLogger][INFO]: World size: 2
[2024-02-28 00:16:19,910][PyLogger][INFO]: Rank[0]: val_accuracy: 0.72970
[2024-02-28 00:16:19,910][PyLogger][INFO]: Rank[1]: val_accuracy: 0.72970
2024-02-28 00:16:21,421 ignite.distributed.launcher.Parallel INFO: End of run
new_size=7 done.
2024-02-28 00:16:25,414 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:16:25,414 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:16:25,414 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x15263fda23e0>' in 2 processes
2024-02-28 00:16:32,597 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:16:32,994 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x153871bdda00>}
[2024-02-28 00:16:33,197][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:16:33,197][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:16:33,197][PyLogger][INFO]: World size: 2
[2024-02-28 00:16:33,204][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:16:33,204][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:16:33,204][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:16:33,204][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:16:33,205][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:16:33,205][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:16:33,205][PyLogger][INFO]: World size: 2
[2024-02-28 00:16:38,411][PyLogger][INFO]: Rank[0]: val_accuracy: 0.76020
[2024-02-28 00:16:38,411][PyLogger][INFO]: Rank[1]: val_accuracy: 0.76020
2024-02-28 00:16:39,881 ignite.distributed.launcher.Parallel INFO: End of run
new_size=8 done.
2024-02-28 00:16:43,602 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:16:43,602 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:16:43,602 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14798c72e3e0>' in 2 processes
2024-02-28 00:16:50,929 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:16:51,320 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1517ed3ccb30>}
[2024-02-28 00:16:51,535][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:16:51,535][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:16:51,535][PyLogger][INFO]: World size: 2
[2024-02-28 00:16:51,536][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:16:51,536][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:16:51,537][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:16:51,537][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:16:51,537][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:16:51,538][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:16:51,538][PyLogger][INFO]: World size: 2
[2024-02-28 00:16:56,835][PyLogger][INFO]: Rank[0]: val_accuracy: 0.77690
[2024-02-28 00:16:56,835][PyLogger][INFO]: Rank[1]: val_accuracy: 0.77690
2024-02-28 00:16:58,345 ignite.distributed.launcher.Parallel INFO: End of run
new_size=9 done.
2024-02-28 00:17:02,099 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:17:02,099 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:17:02,099 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14cadba0a3e0>' in 2 processes
2024-02-28 00:17:09,329 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:17:09,724 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14d714d29dc0>}
[2024-02-28 00:17:09,928][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:17:09,928][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:17:09,928][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:17:09,928][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:17:09,929][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:17:09,929][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:17:09,929][PyLogger][INFO]: World size: 2
[2024-02-28 00:17:09,933][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:17:09,934][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:17:09,934][PyLogger][INFO]: World size: 2
[2024-02-28 00:17:15,204][PyLogger][INFO]: Rank[0]: val_accuracy: 0.79180
[2024-02-28 00:17:15,204][PyLogger][INFO]: Rank[1]: val_accuracy: 0.79180
2024-02-28 00:17:16,684 ignite.distributed.launcher.Parallel INFO: End of run
new_size=10 done.
2024-02-28 00:17:20,460 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:17:20,460 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:17:20,460 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14e06edde3e0>' in 2 processes
2024-02-28 00:17:28,325 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:17:28,729 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1544dbeacb00>}
[2024-02-28 00:17:28,936][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:17:28,936][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:17:28,936][PyLogger][INFO]: World size: 2
[2024-02-28 00:17:28,941][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:17:28,941][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:17:28,941][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:17:28,941][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:17:28,942][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:17:28,942][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:17:28,942][PyLogger][INFO]: World size: 2
[2024-02-28 00:17:34,219][PyLogger][INFO]: Rank[1]: val_accuracy: 0.79930
[2024-02-28 00:17:34,219][PyLogger][INFO]: Rank[0]: val_accuracy: 0.79930
2024-02-28 00:17:35,825 ignite.distributed.launcher.Parallel INFO: End of run
new_size=11 done.
2024-02-28 00:17:39,689 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:17:39,689 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:17:39,689 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x15494647a3e0>' in 2 processes
2024-02-28 00:17:47,132 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:17:47,539 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14fafb82cd70>}
[2024-02-28 00:17:47,749][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:17:47,750][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:17:47,750][PyLogger][INFO]: World size: 2
[2024-02-28 00:17:47,751][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:17:47,751][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:17:47,751][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:17:47,751][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:17:47,752][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:17:47,752][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:17:47,752][PyLogger][INFO]: World size: 2
[2024-02-28 00:17:53,043][PyLogger][INFO]: Rank[0]: val_accuracy: 0.80610
[2024-02-28 00:17:53,044][PyLogger][INFO]: Rank[1]: val_accuracy: 0.80610
2024-02-28 00:17:54,553 ignite.distributed.launcher.Parallel INFO: End of run
new_size=12 done.
2024-02-28 00:17:58,256 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:17:58,256 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:17:58,256 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14e4450923e0>' in 2 processes
2024-02-28 00:18:06,229 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:18:06,622 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x153bf48efa10>}
[2024-02-28 00:18:06,832][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:18:06,832][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:18:06,832][PyLogger][INFO]: World size: 2
[2024-02-28 00:18:06,833][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:18:06,833][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:18:06,833][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:18:06,833][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:18:06,834][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:18:06,834][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:18:06,834][PyLogger][INFO]: World size: 2
[2024-02-28 00:18:12,084][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81030
[2024-02-28 00:18:12,084][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81030
2024-02-28 00:18:13,158 ignite.distributed.launcher.Parallel INFO: End of run
new_size=13 done.
2024-02-28 00:18:16,880 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:18:16,880 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:18:16,880 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1466b6b7a3e0>' in 2 processes
2024-02-28 00:18:24,945 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:18:25,347 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1475bf658da0>}
[2024-02-28 00:18:25,556][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:18:25,556][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:18:25,556][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:18:25,556][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:18:25,557][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:18:25,557][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:18:25,557][PyLogger][INFO]: World size: 2
[2024-02-28 00:18:25,559][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:18:25,559][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:18:25,559][PyLogger][INFO]: World size: 2
[2024-02-28 00:18:30,889][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81180
[2024-02-28 00:18:30,889][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81180
2024-02-28 00:18:32,398 ignite.distributed.launcher.Parallel INFO: End of run
new_size=14 done.
2024-02-28 00:18:36,284 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:18:36,284 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:18:36,284 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14b94a7b63e0>' in 2 processes
2024-02-28 00:18:44,389 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:18:44,808 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14810e2303e0>}
[2024-02-28 00:18:45,014][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:18:45,015][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:18:45,015][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:18:45,015][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:18:45,016][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:18:45,016][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:18:45,016][PyLogger][INFO]: World size: 2
[2024-02-28 00:18:45,022][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:18:45,022][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:18:45,022][PyLogger][INFO]: World size: 2
[2024-02-28 00:18:50,276][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81420
[2024-02-28 00:18:50,276][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81420
2024-02-28 00:18:51,778 ignite.distributed.launcher.Parallel INFO: End of run
new_size=15 done.
2024-02-28 00:18:55,465 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:18:55,465 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:18:55,465 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14d65047e3e0>' in 2 processes
2024-02-28 00:19:02,584 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:19:02,986 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x149acaa408f0>}
[2024-02-28 00:19:03,192][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:19:03,193][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:19:03,193][PyLogger][INFO]: World size: 2
[2024-02-28 00:19:03,194][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:19:03,194][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:19:03,194][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:19:03,194][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:19:03,195][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:19:03,195][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:19:03,195][PyLogger][INFO]: World size: 2
[2024-02-28 00:19:08,465][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81620
[2024-02-28 00:19:08,465][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81620
2024-02-28 00:19:09,953 ignite.distributed.launcher.Parallel INFO: End of run
new_size=16 done.
2024-02-28 00:19:13,645 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:19:13,645 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:19:13,645 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x146c8ce263e0>' in 2 processes
2024-02-28 00:19:21,886 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:19:22,302 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14f63e188500>}
[2024-02-28 00:19:22,512][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:19:22,512][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:19:22,513][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:19:22,513][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:19:22,513][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:19:22,513][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:19:22,513][PyLogger][INFO]: World size: 2
[2024-02-28 00:19:22,514][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:19:22,514][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:19:22,514][PyLogger][INFO]: World size: 2
[2024-02-28 00:19:27,839][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81650
[2024-02-28 00:19:27,839][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81650
2024-02-28 00:19:28,931 ignite.distributed.launcher.Parallel INFO: End of run
new_size=17 done.
2024-02-28 00:19:32,585 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:19:32,585 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:19:32,585 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14e62d35e3e0>' in 2 processes
2024-02-28 00:19:40,821 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:19:41,228 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x15074578ec30>}
[2024-02-28 00:19:41,439][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:19:41,440][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:19:41,440][PyLogger][INFO]: World size: 2
[2024-02-28 00:19:41,441][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:19:41,441][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:19:41,441][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:19:41,442][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:19:41,442][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:19:41,442][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:19:41,442][PyLogger][INFO]: World size: 2
[2024-02-28 00:19:46,704][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81730
[2024-02-28 00:19:46,704][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81730
2024-02-28 00:19:48,203 ignite.distributed.launcher.Parallel INFO: End of run
new_size=18 done.
2024-02-28 00:19:52,186 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:19:52,186 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:19:52,186 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x153e428fa3e0>' in 2 processes
2024-02-28 00:20:00,775 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:20:01,168 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1515f8f06660>}
[2024-02-28 00:20:01,378][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:20:01,378][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:20:01,378][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:20:01,378][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:20:01,380][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:20:01,380][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:20:01,380][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:20:01,380][PyLogger][INFO]: World size: 2
[2024-02-28 00:20:01,380][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:20:01,380][PyLogger][INFO]: World size: 2
[2024-02-28 00:20:06,632][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81840
[2024-02-28 00:20:06,632][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81840
2024-02-28 00:20:08,146 ignite.distributed.launcher.Parallel INFO: End of run
new_size=19 done.
2024-02-28 00:20:11,903 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:20:11,903 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:20:11,904 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x152f1164a3e0>' in 2 processes
2024-02-28 00:20:20,182 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:20:20,585 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x149165fa4440>}
[2024-02-28 00:20:20,795][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:20:20,795][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:20:20,795][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:20:20,795][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:20:20,796][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:20:20,796][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:20:20,796][PyLogger][INFO]: World size: 2
[2024-02-28 00:20:20,796][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:20:20,796][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:20:20,796][PyLogger][INFO]: World size: 2
[2024-02-28 00:20:26,069][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81850
[2024-02-28 00:20:26,069][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81850
2024-02-28 00:20:27,576 ignite.distributed.launcher.Parallel INFO: End of run
new_size=20 done.
2024-02-28 00:20:31,363 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:20:31,363 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:20:31,363 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x147769d223e0>' in 2 processes
2024-02-28 00:20:38,802 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:20:39,207 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14e9a2f89a00>}
[2024-02-28 00:20:39,416][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:20:39,416][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:20:39,416][PyLogger][INFO]: World size: 2
[2024-02-28 00:20:39,418][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:20:39,418][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:20:39,418][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:20:39,418][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:20:39,419][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:20:39,419][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:20:39,419][PyLogger][INFO]: World size: 2
[2024-02-28 00:20:44,719][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81890
[2024-02-28 00:20:44,719][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81890
2024-02-28 00:20:46,210 ignite.distributed.launcher.Parallel INFO: End of run
new_size=21 done.
2024-02-28 00:20:50,143 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:20:50,143 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:20:50,143 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1473b14223e0>' in 2 processes
2024-02-28 00:20:57,459 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:20:57,857 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x151843f68b90>}
[2024-02-28 00:20:58,062][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:20:58,062][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:20:58,062][PyLogger][INFO]: World size: 2
[2024-02-28 00:20:58,066][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:20:58,066][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:20:58,066][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:20:58,066][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:20:58,067][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:20:58,067][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:20:58,067][PyLogger][INFO]: World size: 2
[2024-02-28 00:21:03,267][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81850
[2024-02-28 00:21:03,267][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81850
2024-02-28 00:21:04,858 ignite.distributed.launcher.Parallel INFO: End of run
new_size=22 done.
2024-02-28 00:21:08,614 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:21:08,614 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:21:08,614 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14874f5d63e0>' in 2 processes
2024-02-28 00:21:15,682 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:21:16,087 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14bde40e4410>}
[2024-02-28 00:21:16,296][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:21:16,296][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:21:16,296][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:21:16,296][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:21:16,297][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:21:16,297][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:21:16,297][PyLogger][INFO]: World size: 2
[2024-02-28 00:21:16,300][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:21:16,300][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:21:16,300][PyLogger][INFO]: World size: 2
[2024-02-28 00:21:21,576][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81890
[2024-02-28 00:21:21,576][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81890
2024-02-28 00:21:23,062 ignite.distributed.launcher.Parallel INFO: End of run
new_size=23 done.
2024-02-28 00:21:26,812 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:21:26,812 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:21:26,812 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14cb84cca3e0>' in 2 processes
2024-02-28 00:21:34,126 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:21:34,523 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x145c22b2d9a0>}
[2024-02-28 00:21:34,735][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:21:34,735][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:21:34,735][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:21:34,735][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:21:34,736][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:21:34,736][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:21:34,736][PyLogger][INFO]: World size: 2
[2024-02-28 00:21:34,738][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:21:34,738][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:21:34,738][PyLogger][INFO]: World size: 2
[2024-02-28 00:21:40,061][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81960
[2024-02-28 00:21:40,061][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81960
2024-02-28 00:21:41,661 ignite.distributed.launcher.Parallel INFO: End of run
new_size=24 done.
2024-02-28 00:21:45,552 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:21:45,552 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:21:45,552 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x153648ba23e0>' in 2 processes
2024-02-28 00:21:52,740 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:21:53,143 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x151ec49acb90>}
[2024-02-28 00:21:53,356][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:21:53,356][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:21:53,356][PyLogger][INFO]: World size: 2
[2024-02-28 00:21:53,357][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:21:53,357][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:21:53,358][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:21:53,358][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:21:53,358][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:21:53,358][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:21:53,359][PyLogger][INFO]: World size: 2
[2024-02-28 00:21:58,713][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81960
[2024-02-28 00:21:58,713][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81960
2024-02-28 00:22:00,312 ignite.distributed.launcher.Parallel INFO: End of run
new_size=25 done.
2024-02-28 00:22:04,297 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:22:04,297 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:22:04,297 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x151d1da263e0>' in 2 processes
2024-02-28 00:22:13,041 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:22:13,465 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x15014f7a8170>}
[2024-02-28 00:22:13,678][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:22:13,678][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:22:13,678][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:22:13,678][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:22:13,679][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:22:13,679][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:22:13,679][PyLogger][INFO]: World size: 2
[2024-02-28 00:22:13,684][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:22:13,684][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:22:13,684][PyLogger][INFO]: World size: 2
[2024-02-28 00:22:20,277][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81940
[2024-02-28 00:22:20,277][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81940
2024-02-28 00:22:21,827 ignite.distributed.launcher.Parallel INFO: End of run
new_size=26 done.
2024-02-28 00:22:26,208 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:22:26,208 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:22:26,208 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14c9f7a7a3e0>' in 2 processes
2024-02-28 00:22:38,893 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:22:39,386 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1473493f0800>}
[2024-02-28 00:22:39,600][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:22:39,600][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:22:39,600][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:22:39,600][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:22:39,601][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:22:39,601][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:22:39,601][PyLogger][INFO]: World size: 2
[2024-02-28 00:22:39,603][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:22:39,603][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:22:39,603][PyLogger][INFO]: World size: 2
[2024-02-28 00:22:50,600][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81960
[2024-02-28 00:22:50,600][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81960
2024-02-28 00:22:52,099 ignite.distributed.launcher.Parallel INFO: End of run
new_size=27 done.
2024-02-28 00:22:57,433 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:22:57,433 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:22:57,433 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x148fb182e3e0>' in 2 processes
2024-02-28 00:23:07,064 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:23:07,459 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14ba8f174f80>}
[2024-02-28 00:23:07,671][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:23:07,671][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:23:07,671][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:23:07,671][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:23:07,672][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:23:07,673][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:23:07,673][PyLogger][INFO]: World size: 2
[2024-02-28 00:23:07,673][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:23:07,673][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:23:07,673][PyLogger][INFO]: World size: 2
[2024-02-28 00:23:14,139][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81970
[2024-02-28 00:23:14,139][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81970
2024-02-28 00:23:15,644 ignite.distributed.launcher.Parallel INFO: End of run
new_size=28 done.
2024-02-28 00:23:22,469 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:23:22,469 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:23:22,469 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x145e32cc63e0>' in 2 processes
2024-02-28 00:23:35,844 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:23:36,429 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14983cb34290>}
[2024-02-28 00:23:36,616][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:23:36,616][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:23:36,616][PyLogger][INFO]: World size: 2
[2024-02-28 00:23:36,644][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:23:36,644][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:23:36,644][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:23:36,644][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:23:36,645][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:23:36,645][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:23:36,646][PyLogger][INFO]: World size: 2
[2024-02-28 00:23:41,907][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81950
[2024-02-28 00:23:41,907][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81950
2024-02-28 00:23:43,405 ignite.distributed.launcher.Parallel INFO: End of run
new_size=29 done.
2024-02-28 00:23:47,493 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:23:47,493 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:23:47,493 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14cd1fc0e3e0>' in 2 processes
2024-02-28 00:23:55,299 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:23:55,699 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14a92a23b440>}
[2024-02-28 00:23:55,907][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:23:55,907][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:23:55,907][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:23:55,907][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:23:55,908][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:23:55,909][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:23:55,909][PyLogger][INFO]: World size: 2
[2024-02-28 00:23:55,909][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:23:55,909][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:23:55,909][PyLogger][INFO]: World size: 2
[2024-02-28 00:24:01,244][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81910
[2024-02-28 00:24:01,244][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81910
2024-02-28 00:24:02,722 ignite.distributed.launcher.Parallel INFO: End of run
new_size=30 done.
2024-02-28 00:24:06,553 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:24:06,553 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:24:06,553 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14e10e1523e0>' in 2 processes
2024-02-28 00:24:14,875 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:24:15,277 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14bb36e88cb0>}
[2024-02-28 00:24:15,482][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:24:15,482][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:24:15,483][PyLogger][INFO]: World size: 2
[2024-02-28 00:24:15,485][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:24:15,485][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:24:15,485][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:24:15,485][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:24:15,486][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:24:15,486][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:24:15,486][PyLogger][INFO]: World size: 2
[2024-02-28 00:24:20,696][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81940
[2024-02-28 00:24:20,696][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81940
2024-02-28 00:24:22,215 ignite.distributed.launcher.Parallel INFO: End of run
new_size=31 done.
2024-02-28 00:24:26,315 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:24:26,315 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:24:26,315 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x146e48c763e0>' in 2 processes
2024-02-28 00:24:33,418 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:24:33,810 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x146a05ee83b0>}
[2024-02-28 00:24:34,021][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:24:34,021][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:24:34,021][PyLogger][INFO]: World size: 2
[2024-02-28 00:24:34,023][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:24:34,023][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:24:34,023][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:24:34,023][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:24:34,024][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:24:34,024][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:24:34,024][PyLogger][INFO]: World size: 2
[2024-02-28 00:24:39,279][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81940
[2024-02-28 00:24:39,280][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81940
2024-02-28 00:24:40,781 ignite.distributed.launcher.Parallel INFO: End of run
new_size=32 done.
2024-02-28 00:24:44,594 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:24:44,595 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:24:44,595 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x154b999a63e0>' in 2 processes
2024-02-28 00:24:51,835 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:24:52,262 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1483811ee870>}
[2024-02-28 00:24:52,468][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:24:52,468][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:24:52,468][PyLogger][INFO]: World size: 2
[2024-02-28 00:24:52,475][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:24:52,475][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:24:52,475][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:24:52,475][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:24:52,476][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:24:52,476][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:24:52,476][PyLogger][INFO]: World size: 2
[2024-02-28 00:24:57,750][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81940
[2024-02-28 00:24:57,750][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81940
2024-02-28 00:24:59,238 ignite.distributed.launcher.Parallel INFO: End of run
new_size=33 done.
2024-02-28 00:25:02,916 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:25:02,916 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:25:02,916 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x154a186223e0>' in 2 processes
2024-02-28 00:25:10,117 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:25:10,532 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x153ac702c5c0>}
[2024-02-28 00:25:10,734][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:25:10,734][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:25:10,734][PyLogger][INFO]: World size: 2
[2024-02-28 00:25:10,741][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:25:10,741][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:25:10,742][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:25:10,742][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:25:10,742][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:25:10,743][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:25:10,743][PyLogger][INFO]: World size: 2
[2024-02-28 00:25:16,098][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81940
[2024-02-28 00:25:16,098][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81940
2024-02-28 00:25:17,579 ignite.distributed.launcher.Parallel INFO: End of run
new_size=34 done.
2024-02-28 00:25:21,375 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:25:21,375 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:25:21,375 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1483646e23e0>' in 2 processes
2024-02-28 00:25:28,811 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:25:29,202 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14ad7cf84620>}
[2024-02-28 00:25:29,410][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:25:29,410][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:25:29,410][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:25:29,410][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:25:29,411][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:25:29,411][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:25:29,411][PyLogger][INFO]: World size: 2
[2024-02-28 00:25:29,411][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:25:29,411][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:25:29,411][PyLogger][INFO]: World size: 2
[2024-02-28 00:25:34,732][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81940
[2024-02-28 00:25:34,732][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81940
2024-02-28 00:25:36,217 ignite.distributed.launcher.Parallel INFO: End of run
new_size=35 done.
2024-02-28 00:25:40,326 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-28 00:25:40,326 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-28 00:25:40,326 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x149f69f9a3e0>' in 2 processes
2024-02-28 00:25:47,400 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-28 00:25:47,798 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14c8ae9acc20>}
[2024-02-28 00:25:48,009][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-28 00:25:48,009][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-28 00:25:48,009][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-28 00:25:48,009][PyLogger][INFO]: CUDA version: 12.1
[2024-02-28 00:25:48,010][PyLogger][INFO]: CUDNN version: 8902
[2024-02-28 00:25:48,010][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:25:48,010][PyLogger][INFO]: World size: 2
[2024-02-28 00:25:48,012][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-28 00:25:48,012][PyLogger][INFO]: Distributed backend: nccl
[2024-02-28 00:25:48,012][PyLogger][INFO]: World size: 2
[2024-02-28 00:25:53,271][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81940
[2024-02-28 00:25:53,271][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81940
2024-02-28 00:25:54,753 ignite.distributed.launcher.Parallel INFO: End of run
new_size=36 done.
