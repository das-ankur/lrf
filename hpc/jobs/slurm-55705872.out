SLURM_JOB_ID: 55705872
SLURM_JOB_USER: vsc33647
SLURM_JOB_ACCOUNT: lp_inspiremed
SLURM_JOB_NAME: interpolate_cifar10_sgl
SLURM_CLUSTER_NAME: genius
SLURM_JOB_PARTITION: gpu_p100
SLURM_NNODES: 1
SLURM_NODELIST: r23g39
SLURM_JOB_CPUS_PER_NODE: 18
SLURM_JOB_GPUS: 0,1
Date: Tue Feb 27 04:34:47 CET 2024
Walltime: 00-12:00:00
========================================================================
2024-02-27 04:34:57,858 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 04:34:57,858 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 04:34:57,858 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x14d2e78893a0>' in 2 processes
2024-02-27 04:35:09,779 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 04:35:11,858 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 256, 'num_workers': 0, 'pin_memory': True, 'drop_last': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x15474d1f4200>}
2024-02-27 04:35:12,196 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x15474cfa9eb0>}
/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[2024-02-27 04:35:12,206][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 04:35:12,207][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 04:35:12,207][PyLogger][INFO]: World size: 2
[2024-02-27 04:35:12,207][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 04:35:12,207][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 04:35:12,208][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 04:35:12,208][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 04:35:12,231][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 04:35:12,231][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 04:35:12,231][PyLogger][INFO]: World size: 2
[2024-02-27 04:35:44,992][PyLogger][INFO]: Rank[1]: Epoch[1/500], iter[50] -  - loss: 3.30472
[2024-02-27 04:35:44,992][PyLogger][INFO]: Rank[0]: Epoch[1/500], iter[50] -  - loss: 2.34603
[2024-02-27 04:36:11,865][PyLogger][INFO]: Rank[0]: Epoch[2/500], iter[100] -  - loss: 2.27021
[2024-02-27 04:36:11,865][PyLogger][INFO]: Rank[1]: Epoch[2/500], iter[100] -  - loss: 2.22425
[2024-02-27 04:36:38,092][PyLogger][INFO]: Rank[1]: Epoch[2/500], iter[150] -  - loss: 2.14603
[2024-02-27 04:36:38,092][PyLogger][INFO]: Rank[0]: Epoch[2/500], iter[150] -  - loss: 2.14907
[2024-02-27 04:37:04,847][PyLogger][INFO]: Rank[0]: Epoch[3/500], iter[200] -  - loss: 2.01942
[2024-02-27 04:37:04,847][PyLogger][INFO]: Rank[1]: Epoch[3/500], iter[200] -  - loss: 2.07492
[2024-02-27 04:37:31,286][PyLogger][INFO]: Rank[0]: Epoch[3/500], iter[250] -  - loss: 1.98544
[2024-02-27 04:37:31,286][PyLogger][INFO]: Rank[1]: Epoch[3/500], iter[250] -  - loss: 2.08959
[2024-02-27 04:37:58,234][PyLogger][INFO]: Rank[1]: Epoch[4/500], iter[300] -  - loss: 1.95824
[2024-02-27 04:37:58,235][PyLogger][INFO]: Rank[0]: Epoch[4/500], iter[300] -  - loss: 1.94255
[2024-02-27 04:38:24,641][PyLogger][INFO]: Rank[0]: Epoch[4/500], iter[350] -  - loss: 1.87947
[2024-02-27 04:38:24,641][PyLogger][INFO]: Rank[1]: Epoch[4/500], iter[350] -  - loss: 2.00640
/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/data/leuven/336/vsc33647/miniconda3/envs/deepenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[2024-02-27 04:38:51,435][PyLogger][INFO]: Rank[1]: Epoch[5/500], iter[400] -  - loss: 1.84385
[2024-02-27 04:38:51,435][PyLogger][INFO]: Rank[0]: Epoch[5/500], iter[400] -  - loss: 1.84816
[2024-02-27 04:39:17,779][PyLogger][INFO]: Rank[1]: Epoch[5/500], iter[450] -  - loss: 1.82661
[2024-02-27 04:39:17,779][PyLogger][INFO]: Rank[0]: Epoch[5/500], iter[450] -  - loss: 1.77451
[2024-02-27 04:40:12,215][PyLogger][INFO]: Rank[0]: Epoch[6/500], iter[500] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.76491
[2024-02-27 04:40:12,215][PyLogger][INFO]: Rank[1]: Epoch[6/500], iter[500] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.80561
[2024-02-27 04:40:38,520][PyLogger][INFO]: Rank[1]: Epoch[6/500], iter[550] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.76267
[2024-02-27 04:40:38,520][PyLogger][INFO]: Rank[0]: Epoch[6/500], iter[550] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.74280
[2024-02-27 04:41:05,377][PyLogger][INFO]: Rank[0]: Epoch[7/500], iter[600] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.74146
[2024-02-27 04:41:05,377][PyLogger][INFO]: Rank[1]: Epoch[7/500], iter[600] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.69750
[2024-02-27 04:41:31,734][PyLogger][INFO]: Rank[1]: Epoch[7/500], iter[650] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.57979
[2024-02-27 04:41:31,734][PyLogger][INFO]: Rank[0]: Epoch[7/500], iter[650] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.59740
[2024-02-27 04:41:58,641][PyLogger][INFO]: Rank[0]: Epoch[8/500], iter[700] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.56723
[2024-02-27 04:41:58,641][PyLogger][INFO]: Rank[1]: Epoch[8/500], iter[700] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.59811
[2024-02-27 04:42:24,855][PyLogger][INFO]: Rank[0]: Epoch[8/500], iter[750] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.44686
[2024-02-27 04:42:24,855][PyLogger][INFO]: Rank[1]: Epoch[8/500], iter[750] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.53376
[2024-02-27 04:42:51,883][PyLogger][INFO]: Rank[1]: Epoch[9/500], iter[800] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.47513
[2024-02-27 04:42:51,883][PyLogger][INFO]: Rank[0]: Epoch[9/500], iter[800] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.54091
[2024-02-27 04:43:18,333][PyLogger][INFO]: Rank[1]: Epoch[9/500], iter[850] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.47099
[2024-02-27 04:43:18,333][PyLogger][INFO]: Rank[0]: Epoch[9/500], iter[850] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.50399
[2024-02-27 04:43:45,408][PyLogger][INFO]: Rank[1]: Epoch[10/500], iter[900] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.43028
[2024-02-27 04:43:45,408][PyLogger][INFO]: Rank[0]: Epoch[10/500], iter[900] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.36479
[2024-02-27 04:44:11,816][PyLogger][INFO]: Rank[0]: Epoch[10/500], iter[950] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.32451
[2024-02-27 04:44:11,816][PyLogger][INFO]: Rank[1]: Epoch[10/500], iter[950] - train_accuracy: 0.34518 - val_accuracy: 0.36030 - train_loss: 1.86516 - val_loss: 1.84187 - loss: 1.32447
[2024-02-27 04:45:06,295][PyLogger][INFO]: Rank[0]: Epoch[11/500], iter[1000] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.30147
[2024-02-27 04:45:06,295][PyLogger][INFO]: Rank[1]: Epoch[11/500], iter[1000] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.38619
[2024-02-27 04:45:32,665][PyLogger][INFO]: Rank[0]: Epoch[11/500], iter[1050] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.31005
[2024-02-27 04:45:32,665][PyLogger][INFO]: Rank[1]: Epoch[11/500], iter[1050] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.22265
[2024-02-27 04:45:59,490][PyLogger][INFO]: Rank[0]: Epoch[12/500], iter[1100] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.21003
[2024-02-27 04:45:59,490][PyLogger][INFO]: Rank[1]: Epoch[12/500], iter[1100] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.16537
[2024-02-27 04:46:25,866][PyLogger][INFO]: Rank[1]: Epoch[12/500], iter[1150] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.24204
[2024-02-27 04:46:25,866][PyLogger][INFO]: Rank[0]: Epoch[12/500], iter[1150] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.24386
[2024-02-27 04:46:52,805][PyLogger][INFO]: Rank[0]: Epoch[13/500], iter[1200] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.26089
[2024-02-27 04:46:52,805][PyLogger][INFO]: Rank[1]: Epoch[13/500], iter[1200] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.22400
[2024-02-27 04:47:19,337][PyLogger][INFO]: Rank[0]: Epoch[13/500], iter[1250] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.25934
[2024-02-27 04:47:19,337][PyLogger][INFO]: Rank[1]: Epoch[13/500], iter[1250] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.27891
[2024-02-27 04:47:46,297][PyLogger][INFO]: Rank[1]: Epoch[14/500], iter[1300] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.23241
[2024-02-27 04:47:46,297][PyLogger][INFO]: Rank[0]: Epoch[14/500], iter[1300] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.17743
[2024-02-27 04:48:12,653][PyLogger][INFO]: Rank[0]: Epoch[14/500], iter[1350] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.09146
[2024-02-27 04:48:12,653][PyLogger][INFO]: Rank[1]: Epoch[14/500], iter[1350] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.09208
[2024-02-27 04:48:39,462][PyLogger][INFO]: Rank[0]: Epoch[15/500], iter[1400] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.07889
[2024-02-27 04:48:39,462][PyLogger][INFO]: Rank[1]: Epoch[15/500], iter[1400] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.16902
[2024-02-27 04:49:05,779][PyLogger][INFO]: Rank[1]: Epoch[15/500], iter[1450] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.20442
[2024-02-27 04:49:05,779][PyLogger][INFO]: Rank[0]: Epoch[15/500], iter[1450] - train_accuracy: 0.59186 - val_accuracy: 0.58900 - train_loss: 1.39627 - val_loss: 1.39271 - loss: 1.11171
[2024-02-27 04:50:00,233][PyLogger][INFO]: Rank[0]: Epoch[16/500], iter[1500] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.07926
[2024-02-27 04:50:00,233][PyLogger][INFO]: Rank[1]: Epoch[16/500], iter[1500] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.05092
[2024-02-27 04:50:26,735][PyLogger][INFO]: Rank[1]: Epoch[16/500], iter[1550] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.07589
[2024-02-27 04:50:26,735][PyLogger][INFO]: Rank[0]: Epoch[16/500], iter[1550] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.10247
[2024-02-27 04:50:53,588][PyLogger][INFO]: Rank[1]: Epoch[17/500], iter[1600] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 0.99589
[2024-02-27 04:50:53,588][PyLogger][INFO]: Rank[0]: Epoch[17/500], iter[1600] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.08884
[2024-02-27 04:51:20,399][PyLogger][INFO]: Rank[0]: Epoch[18/500], iter[1650] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.00450
[2024-02-27 04:51:20,399][PyLogger][INFO]: Rank[1]: Epoch[18/500], iter[1650] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.09516
[2024-02-27 04:51:46,818][PyLogger][INFO]: Rank[0]: Epoch[18/500], iter[1700] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.06137
[2024-02-27 04:51:46,818][PyLogger][INFO]: Rank[1]: Epoch[18/500], iter[1700] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.06101
[2024-02-27 04:52:13,656][PyLogger][INFO]: Rank[1]: Epoch[19/500], iter[1750] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 0.96712
[2024-02-27 04:52:13,656][PyLogger][INFO]: Rank[0]: Epoch[19/500], iter[1750] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.04583
[2024-02-27 04:52:40,019][PyLogger][INFO]: Rank[0]: Epoch[19/500], iter[1800] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 0.97863
[2024-02-27 04:52:40,019][PyLogger][INFO]: Rank[1]: Epoch[19/500], iter[1800] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.03376
[2024-02-27 04:53:06,831][PyLogger][INFO]: Rank[0]: Epoch[20/500], iter[1850] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 0.97102
[2024-02-27 04:53:06,831][PyLogger][INFO]: Rank[1]: Epoch[20/500], iter[1850] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 0.94967
[2024-02-27 04:53:33,182][PyLogger][INFO]: Rank[1]: Epoch[20/500], iter[1900] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 1.05472
[2024-02-27 04:53:33,182][PyLogger][INFO]: Rank[0]: Epoch[20/500], iter[1900] - train_accuracy: 0.72527 - val_accuracy: 0.70690 - train_loss: 1.10785 - val_loss: 1.15045 - loss: 0.99244
[2024-02-27 04:54:27,843][PyLogger][INFO]: Rank[0]: Epoch[21/500], iter[1950] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.87012
[2024-02-27 04:54:27,843][PyLogger][INFO]: Rank[1]: Epoch[21/500], iter[1950] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.94935
[2024-02-27 04:54:54,232][PyLogger][INFO]: Rank[1]: Epoch[21/500], iter[2000] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.96775
[2024-02-27 04:54:54,233][PyLogger][INFO]: Rank[0]: Epoch[21/500], iter[2000] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.88708
[2024-02-27 04:55:20,978][PyLogger][INFO]: Rank[0]: Epoch[22/500], iter[2050] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.87559
[2024-02-27 04:55:20,978][PyLogger][INFO]: Rank[1]: Epoch[22/500], iter[2050] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.80266
[2024-02-27 04:55:47,304][PyLogger][INFO]: Rank[0]: Epoch[22/500], iter[2100] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.88944
[2024-02-27 04:55:47,304][PyLogger][INFO]: Rank[1]: Epoch[22/500], iter[2100] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.97487
[2024-02-27 04:56:14,281][PyLogger][INFO]: Rank[1]: Epoch[23/500], iter[2150] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.89556
[2024-02-27 04:56:14,281][PyLogger][INFO]: Rank[0]: Epoch[23/500], iter[2150] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.94595
[2024-02-27 04:56:40,722][PyLogger][INFO]: Rank[0]: Epoch[23/500], iter[2200] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.92768
[2024-02-27 04:56:40,722][PyLogger][INFO]: Rank[1]: Epoch[23/500], iter[2200] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.99073
[2024-02-27 04:57:07,762][PyLogger][INFO]: Rank[0]: Epoch[24/500], iter[2250] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.89686
[2024-02-27 04:57:07,762][PyLogger][INFO]: Rank[1]: Epoch[24/500], iter[2250] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.82864
[2024-02-27 04:57:34,246][PyLogger][INFO]: Rank[1]: Epoch[24/500], iter[2300] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.84494
[2024-02-27 04:57:34,246][PyLogger][INFO]: Rank[0]: Epoch[24/500], iter[2300] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.90161
[2024-02-27 04:58:01,019][PyLogger][INFO]: Rank[0]: Epoch[25/500], iter[2350] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.89806
[2024-02-27 04:58:01,019][PyLogger][INFO]: Rank[1]: Epoch[25/500], iter[2350] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.81636
[2024-02-27 04:58:27,445][PyLogger][INFO]: Rank[1]: Epoch[25/500], iter[2400] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.86101
[2024-02-27 04:58:27,445][PyLogger][INFO]: Rank[0]: Epoch[25/500], iter[2400] - train_accuracy: 0.79470 - val_accuracy: 0.78000 - train_loss: 0.96366 - val_loss: 1.00142 - loss: 0.86790
[2024-02-27 04:59:22,333][PyLogger][INFO]: Rank[0]: Epoch[26/500], iter[2450] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.84611
[2024-02-27 04:59:22,333][PyLogger][INFO]: Rank[1]: Epoch[26/500], iter[2450] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.81704
[2024-02-27 04:59:48,725][PyLogger][INFO]: Rank[1]: Epoch[26/500], iter[2500] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.84764
[2024-02-27 04:59:48,725][PyLogger][INFO]: Rank[0]: Epoch[26/500], iter[2500] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.83011
[2024-02-27 05:00:15,521][PyLogger][INFO]: Rank[0]: Epoch[27/500], iter[2550] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.78429
[2024-02-27 05:00:15,521][PyLogger][INFO]: Rank[1]: Epoch[27/500], iter[2550] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.88555
[2024-02-27 05:00:41,859][PyLogger][INFO]: Rank[1]: Epoch[27/500], iter[2600] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.83201
[2024-02-27 05:00:41,859][PyLogger][INFO]: Rank[0]: Epoch[27/500], iter[2600] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.79930
[2024-02-27 05:01:08,676][PyLogger][INFO]: Rank[1]: Epoch[28/500], iter[2650] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.81807
[2024-02-27 05:01:08,676][PyLogger][INFO]: Rank[0]: Epoch[28/500], iter[2650] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.87596
[2024-02-27 05:01:35,129][PyLogger][INFO]: Rank[0]: Epoch[28/500], iter[2700] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.79557
[2024-02-27 05:01:35,129][PyLogger][INFO]: Rank[1]: Epoch[28/500], iter[2700] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.77445
[2024-02-27 05:02:02,047][PyLogger][INFO]: Rank[1]: Epoch[29/500], iter[2750] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.72464
[2024-02-27 05:02:02,047][PyLogger][INFO]: Rank[0]: Epoch[29/500], iter[2750] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.76244
[2024-02-27 05:02:28,371][PyLogger][INFO]: Rank[1]: Epoch[29/500], iter[2800] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.83255
[2024-02-27 05:02:28,371][PyLogger][INFO]: Rank[0]: Epoch[29/500], iter[2800] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.79847
[2024-02-27 05:02:55,261][PyLogger][INFO]: Rank[0]: Epoch[30/500], iter[2850] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.81563
[2024-02-27 05:02:55,261][PyLogger][INFO]: Rank[1]: Epoch[30/500], iter[2850] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.80407
[2024-02-27 05:03:21,658][PyLogger][INFO]: Rank[1]: Epoch[30/500], iter[2900] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.82009
[2024-02-27 05:03:21,658][PyLogger][INFO]: Rank[0]: Epoch[30/500], iter[2900] - train_accuracy: 0.84045 - val_accuracy: 0.81970 - train_loss: 0.86619 - val_loss: 0.91826 - loss: 0.83072
[2024-02-27 05:04:16,070][PyLogger][INFO]: Rank[1]: Epoch[31/500], iter[2950] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.82469
[2024-02-27 05:04:16,070][PyLogger][INFO]: Rank[0]: Epoch[31/500], iter[2950] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.76823
[2024-02-27 05:04:42,497][PyLogger][INFO]: Rank[1]: Epoch[31/500], iter[3000] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.73368
[2024-02-27 05:04:42,497][PyLogger][INFO]: Rank[0]: Epoch[31/500], iter[3000] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.75223
[2024-02-27 05:05:09,439][PyLogger][INFO]: Rank[0]: Epoch[32/500], iter[3050] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.83995
[2024-02-27 05:05:09,439][PyLogger][INFO]: Rank[1]: Epoch[32/500], iter[3050] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.74667
[2024-02-27 05:05:35,775][PyLogger][INFO]: Rank[1]: Epoch[32/500], iter[3100] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.75305
[2024-02-27 05:05:35,775][PyLogger][INFO]: Rank[0]: Epoch[32/500], iter[3100] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.77953
[2024-02-27 05:06:02,596][PyLogger][INFO]: Rank[0]: Epoch[33/500], iter[3150] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.78720
[2024-02-27 05:06:02,596][PyLogger][INFO]: Rank[1]: Epoch[33/500], iter[3150] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.74436
[2024-02-27 05:06:28,979][PyLogger][INFO]: Rank[0]: Epoch[33/500], iter[3200] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.76676
[2024-02-27 05:06:28,979][PyLogger][INFO]: Rank[1]: Epoch[33/500], iter[3200] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.73219
[2024-02-27 05:06:55,710][PyLogger][INFO]: Rank[1]: Epoch[34/500], iter[3250] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.72117
[2024-02-27 05:06:55,710][PyLogger][INFO]: Rank[0]: Epoch[34/500], iter[3250] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.69971
[2024-02-27 05:07:22,580][PyLogger][INFO]: Rank[0]: Epoch[35/500], iter[3300] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.71697
[2024-02-27 05:07:22,580][PyLogger][INFO]: Rank[1]: Epoch[35/500], iter[3300] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.68206
[2024-02-27 05:07:48,938][PyLogger][INFO]: Rank[1]: Epoch[35/500], iter[3350] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.64635
[2024-02-27 05:07:48,938][PyLogger][INFO]: Rank[0]: Epoch[35/500], iter[3350] - train_accuracy: 0.88102 - val_accuracy: 0.84910 - train_loss: 0.77499 - val_loss: 0.86142 - loss: 0.76124
[2024-02-27 05:08:43,499][PyLogger][INFO]: Rank[1]: Epoch[36/500], iter[3400] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.69168
[2024-02-27 05:08:43,499][PyLogger][INFO]: Rank[0]: Epoch[36/500], iter[3400] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.74657
[2024-02-27 05:09:10,022][PyLogger][INFO]: Rank[0]: Epoch[36/500], iter[3450] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.73736
[2024-02-27 05:09:10,022][PyLogger][INFO]: Rank[1]: Epoch[36/500], iter[3450] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.68142
[2024-02-27 05:09:36,939][PyLogger][INFO]: Rank[0]: Epoch[37/500], iter[3500] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.72030
[2024-02-27 05:09:36,939][PyLogger][INFO]: Rank[1]: Epoch[37/500], iter[3500] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.72736
[2024-02-27 05:10:03,292][PyLogger][INFO]: Rank[1]: Epoch[37/500], iter[3550] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.66668
[2024-02-27 05:10:03,292][PyLogger][INFO]: Rank[0]: Epoch[37/500], iter[3550] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.71123
[2024-02-27 05:10:30,146][PyLogger][INFO]: Rank[1]: Epoch[38/500], iter[3600] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.66102
[2024-02-27 05:10:30,147][PyLogger][INFO]: Rank[0]: Epoch[38/500], iter[3600] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.71711
[2024-02-27 05:10:56,528][PyLogger][INFO]: Rank[0]: Epoch[38/500], iter[3650] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.69987
[2024-02-27 05:10:56,528][PyLogger][INFO]: Rank[1]: Epoch[38/500], iter[3650] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.73337
[2024-02-27 05:11:23,508][PyLogger][INFO]: Rank[0]: Epoch[39/500], iter[3700] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.72458
[2024-02-27 05:11:23,508][PyLogger][INFO]: Rank[1]: Epoch[39/500], iter[3700] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.67873
[2024-02-27 05:11:49,777][PyLogger][INFO]: Rank[0]: Epoch[39/500], iter[3750] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.69292
[2024-02-27 05:11:49,777][PyLogger][INFO]: Rank[1]: Epoch[39/500], iter[3750] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.66420
[2024-02-27 05:12:16,615][PyLogger][INFO]: Rank[0]: Epoch[40/500], iter[3800] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.72660
[2024-02-27 05:12:16,615][PyLogger][INFO]: Rank[1]: Epoch[40/500], iter[3800] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.71594
[2024-02-27 05:12:42,965][PyLogger][INFO]: Rank[0]: Epoch[40/500], iter[3850] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.68445
[2024-02-27 05:12:42,965][PyLogger][INFO]: Rank[1]: Epoch[40/500], iter[3850] - train_accuracy: 0.89993 - val_accuracy: 0.85760 - train_loss: 0.73108 - val_loss: 0.83316 - loss: 0.67844
[2024-02-27 05:13:37,398][PyLogger][INFO]: Rank[0]: Epoch[41/500], iter[3900] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.68417
[2024-02-27 05:13:37,398][PyLogger][INFO]: Rank[1]: Epoch[41/500], iter[3900] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.64250
[2024-02-27 05:14:03,948][PyLogger][INFO]: Rank[0]: Epoch[41/500], iter[3950] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.65427
[2024-02-27 05:14:03,948][PyLogger][INFO]: Rank[1]: Epoch[41/500], iter[3950] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.70902
[2024-02-27 05:14:30,692][PyLogger][INFO]: Rank[1]: Epoch[42/500], iter[4000] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.67615
[2024-02-27 05:14:30,692][PyLogger][INFO]: Rank[0]: Epoch[42/500], iter[4000] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.65267
[2024-02-27 05:14:57,107][PyLogger][INFO]: Rank[0]: Epoch[42/500], iter[4050] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.65478
[2024-02-27 05:14:57,107][PyLogger][INFO]: Rank[1]: Epoch[42/500], iter[4050] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.70463
[2024-02-27 05:15:23,993][PyLogger][INFO]: Rank[1]: Epoch[43/500], iter[4100] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.71522
[2024-02-27 05:15:23,994][PyLogger][INFO]: Rank[0]: Epoch[43/500], iter[4100] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.68665
[2024-02-27 05:15:50,320][PyLogger][INFO]: Rank[0]: Epoch[43/500], iter[4150] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.66273
[2024-02-27 05:15:50,320][PyLogger][INFO]: Rank[1]: Epoch[43/500], iter[4150] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.68574
[2024-02-27 05:16:17,150][PyLogger][INFO]: Rank[0]: Epoch[44/500], iter[4200] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.66485
[2024-02-27 05:16:17,150][PyLogger][INFO]: Rank[1]: Epoch[44/500], iter[4200] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.68051
[2024-02-27 05:16:43,546][PyLogger][INFO]: Rank[1]: Epoch[44/500], iter[4250] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.64276
[2024-02-27 05:16:43,546][PyLogger][INFO]: Rank[0]: Epoch[44/500], iter[4250] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.66483
[2024-02-27 05:17:10,401][PyLogger][INFO]: Rank[1]: Epoch[45/500], iter[4300] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.65658
[2024-02-27 05:17:10,401][PyLogger][INFO]: Rank[0]: Epoch[45/500], iter[4300] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.63246
[2024-02-27 05:17:36,781][PyLogger][INFO]: Rank[1]: Epoch[45/500], iter[4350] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.62584
[2024-02-27 05:17:36,781][PyLogger][INFO]: Rank[0]: Epoch[45/500], iter[4350] - train_accuracy: 0.91608 - val_accuracy: 0.87150 - train_loss: 0.69852 - val_loss: 0.81938 - loss: 0.65997
[2024-02-27 05:18:31,273][PyLogger][INFO]: Rank[0]: Epoch[46/500], iter[4400] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.61395
[2024-02-27 05:18:31,273][PyLogger][INFO]: Rank[1]: Epoch[46/500], iter[4400] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.62669
[2024-02-27 05:18:57,520][PyLogger][INFO]: Rank[1]: Epoch[46/500], iter[4450] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.62184
[2024-02-27 05:18:57,520][PyLogger][INFO]: Rank[0]: Epoch[46/500], iter[4450] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.65014
[2024-02-27 05:19:24,452][PyLogger][INFO]: Rank[1]: Epoch[47/500], iter[4500] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.61655
[2024-02-27 05:19:24,452][PyLogger][INFO]: Rank[0]: Epoch[47/500], iter[4500] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.62835
[2024-02-27 05:19:50,917][PyLogger][INFO]: Rank[0]: Epoch[47/500], iter[4550] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.64482
[2024-02-27 05:19:50,917][PyLogger][INFO]: Rank[1]: Epoch[47/500], iter[4550] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.64525
[2024-02-27 05:20:17,724][PyLogger][INFO]: Rank[1]: Epoch[48/500], iter[4600] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.65440
[2024-02-27 05:20:17,724][PyLogger][INFO]: Rank[0]: Epoch[48/500], iter[4600] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.61257
[2024-02-27 05:20:44,027][PyLogger][INFO]: Rank[0]: Epoch[48/500], iter[4650] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.62110
[2024-02-27 05:20:44,027][PyLogger][INFO]: Rank[1]: Epoch[48/500], iter[4650] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.66310
[2024-02-27 05:21:11,043][PyLogger][INFO]: Rank[0]: Epoch[49/500], iter[4700] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.61522
[2024-02-27 05:21:11,043][PyLogger][INFO]: Rank[1]: Epoch[49/500], iter[4700] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.67110
[2024-02-27 05:21:37,480][PyLogger][INFO]: Rank[1]: Epoch[49/500], iter[4750] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.66607
[2024-02-27 05:21:37,480][PyLogger][INFO]: Rank[0]: Epoch[49/500], iter[4750] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.62752
[2024-02-27 05:22:04,518][PyLogger][INFO]: Rank[0]: Epoch[50/500], iter[4800] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.61220
[2024-02-27 05:22:04,518][PyLogger][INFO]: Rank[1]: Epoch[50/500], iter[4800] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.64221
[2024-02-27 05:22:30,915][PyLogger][INFO]: Rank[1]: Epoch[50/500], iter[4850] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.70431
[2024-02-27 05:22:30,915][PyLogger][INFO]: Rank[0]: Epoch[50/500], iter[4850] - train_accuracy: 0.89016 - val_accuracy: 0.84130 - train_loss: 0.76010 - val_loss: 0.90485 - loss: 0.62439
[2024-02-27 05:23:25,409][PyLogger][INFO]: Rank[0]: Epoch[51/500], iter[4900] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.63968
[2024-02-27 05:23:25,409][PyLogger][INFO]: Rank[1]: Epoch[51/500], iter[4900] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.61181
[2024-02-27 05:23:52,161][PyLogger][INFO]: Rank[1]: Epoch[52/500], iter[4950] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.61872
[2024-02-27 05:23:52,161][PyLogger][INFO]: Rank[0]: Epoch[52/500], iter[4950] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.55966
[2024-02-27 05:24:18,788][PyLogger][INFO]: Rank[1]: Epoch[52/500], iter[5000] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.66027
[2024-02-27 05:24:18,788][PyLogger][INFO]: Rank[0]: Epoch[52/500], iter[5000] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.61486
[2024-02-27 05:24:45,625][PyLogger][INFO]: Rank[0]: Epoch[53/500], iter[5050] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.61790
[2024-02-27 05:24:45,625][PyLogger][INFO]: Rank[1]: Epoch[53/500], iter[5050] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.61250
[2024-02-27 05:25:11,891][PyLogger][INFO]: Rank[1]: Epoch[53/500], iter[5100] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.62814
[2024-02-27 05:25:11,891][PyLogger][INFO]: Rank[0]: Epoch[53/500], iter[5100] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.67515
[2024-02-27 05:25:38,613][PyLogger][INFO]: Rank[0]: Epoch[54/500], iter[5150] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.61120
[2024-02-27 05:25:38,613][PyLogger][INFO]: Rank[1]: Epoch[54/500], iter[5150] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.57294
[2024-02-27 05:26:04,950][PyLogger][INFO]: Rank[1]: Epoch[54/500], iter[5200] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.65441
[2024-02-27 05:26:04,950][PyLogger][INFO]: Rank[0]: Epoch[54/500], iter[5200] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.58719
[2024-02-27 05:26:31,845][PyLogger][INFO]: Rank[1]: Epoch[55/500], iter[5250] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.61406
[2024-02-27 05:26:31,845][PyLogger][INFO]: Rank[0]: Epoch[55/500], iter[5250] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.59233
[2024-02-27 05:26:58,350][PyLogger][INFO]: Rank[1]: Epoch[55/500], iter[5300] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.58047
[2024-02-27 05:26:58,350][PyLogger][INFO]: Rank[0]: Epoch[55/500], iter[5300] - train_accuracy: 0.92737 - val_accuracy: 0.87260 - train_loss: 0.66741 - val_loss: 0.82039 - loss: 0.65136
[2024-02-27 05:27:52,743][PyLogger][INFO]: Rank[1]: Epoch[56/500], iter[5350] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.64155
[2024-02-27 05:27:52,743][PyLogger][INFO]: Rank[0]: Epoch[56/500], iter[5350] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.56441
[2024-02-27 05:28:19,116][PyLogger][INFO]: Rank[1]: Epoch[56/500], iter[5400] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.61919
[2024-02-27 05:28:19,116][PyLogger][INFO]: Rank[0]: Epoch[56/500], iter[5400] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.59601
[2024-02-27 05:28:45,863][PyLogger][INFO]: Rank[1]: Epoch[57/500], iter[5450] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.59074
[2024-02-27 05:28:45,863][PyLogger][INFO]: Rank[0]: Epoch[57/500], iter[5450] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.61460
[2024-02-27 05:29:12,270][PyLogger][INFO]: Rank[0]: Epoch[57/500], iter[5500] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.65615
[2024-02-27 05:29:12,270][PyLogger][INFO]: Rank[1]: Epoch[57/500], iter[5500] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.66154
[2024-02-27 05:29:39,108][PyLogger][INFO]: Rank[0]: Epoch[58/500], iter[5550] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.61741
[2024-02-27 05:29:39,108][PyLogger][INFO]: Rank[1]: Epoch[58/500], iter[5550] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.58118
[2024-02-27 05:30:05,492][PyLogger][INFO]: Rank[1]: Epoch[58/500], iter[5600] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.59453
[2024-02-27 05:30:05,492][PyLogger][INFO]: Rank[0]: Epoch[58/500], iter[5600] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.56139
[2024-02-27 05:30:32,463][PyLogger][INFO]: Rank[0]: Epoch[59/500], iter[5650] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.60920
[2024-02-27 05:30:32,463][PyLogger][INFO]: Rank[1]: Epoch[59/500], iter[5650] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.60451
[2024-02-27 05:30:58,894][PyLogger][INFO]: Rank[1]: Epoch[59/500], iter[5700] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.59477
[2024-02-27 05:30:58,894][PyLogger][INFO]: Rank[0]: Epoch[59/500], iter[5700] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.60842
[2024-02-27 05:31:25,766][PyLogger][INFO]: Rank[0]: Epoch[60/500], iter[5750] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.60766
[2024-02-27 05:31:25,766][PyLogger][INFO]: Rank[1]: Epoch[60/500], iter[5750] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.61802
[2024-02-27 05:31:52,127][PyLogger][INFO]: Rank[0]: Epoch[60/500], iter[5800] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.63002
[2024-02-27 05:31:52,127][PyLogger][INFO]: Rank[1]: Epoch[60/500], iter[5800] - train_accuracy: 0.94163 - val_accuracy: 0.87690 - train_loss: 0.63337 - val_loss: 0.80458 - loss: 0.56368
[2024-02-27 05:32:46,617][PyLogger][INFO]: Rank[1]: Epoch[61/500], iter[5850] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.54393
[2024-02-27 05:32:46,617][PyLogger][INFO]: Rank[0]: Epoch[61/500], iter[5850] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.57945
[2024-02-27 05:33:12,997][PyLogger][INFO]: Rank[1]: Epoch[61/500], iter[5900] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.54172
[2024-02-27 05:33:12,997][PyLogger][INFO]: Rank[0]: Epoch[61/500], iter[5900] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.64436
[2024-02-27 05:33:39,871][PyLogger][INFO]: Rank[1]: Epoch[62/500], iter[5950] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.59887
[2024-02-27 05:33:39,871][PyLogger][INFO]: Rank[0]: Epoch[62/500], iter[5950] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.57010
[2024-02-27 05:34:06,201][PyLogger][INFO]: Rank[1]: Epoch[62/500], iter[6000] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.58993
[2024-02-27 05:34:06,201][PyLogger][INFO]: Rank[0]: Epoch[62/500], iter[6000] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.58786
[2024-02-27 05:34:33,455][PyLogger][INFO]: Rank[1]: Epoch[63/500], iter[6050] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.60164
[2024-02-27 05:34:33,455][PyLogger][INFO]: Rank[0]: Epoch[63/500], iter[6050] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.56607
[2024-02-27 05:34:59,868][PyLogger][INFO]: Rank[0]: Epoch[63/500], iter[6100] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.57645
[2024-02-27 05:34:59,868][PyLogger][INFO]: Rank[1]: Epoch[63/500], iter[6100] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.60652
[2024-02-27 05:35:26,648][PyLogger][INFO]: Rank[1]: Epoch[64/500], iter[6150] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.62978
[2024-02-27 05:35:26,648][PyLogger][INFO]: Rank[0]: Epoch[64/500], iter[6150] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.55047
[2024-02-27 05:35:52,888][PyLogger][INFO]: Rank[1]: Epoch[64/500], iter[6200] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.57776
[2024-02-27 05:35:52,888][PyLogger][INFO]: Rank[0]: Epoch[64/500], iter[6200] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.59047
[2024-02-27 05:36:19,744][PyLogger][INFO]: Rank[0]: Epoch[65/500], iter[6250] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.59116
[2024-02-27 05:36:19,744][PyLogger][INFO]: Rank[1]: Epoch[65/500], iter[6250] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.58945
[2024-02-27 05:36:46,155][PyLogger][INFO]: Rank[0]: Epoch[65/500], iter[6300] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.59501
[2024-02-27 05:36:46,155][PyLogger][INFO]: Rank[1]: Epoch[65/500], iter[6300] - train_accuracy: 0.94350 - val_accuracy: 0.87890 - train_loss: 0.62906 - val_loss: 0.80995 - loss: 0.59596
[2024-02-27 05:37:40,416][PyLogger][INFO]: Rank[1]: Epoch[66/500], iter[6350] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.55151
[2024-02-27 05:37:40,416][PyLogger][INFO]: Rank[0]: Epoch[66/500], iter[6350] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.54292
[2024-02-27 05:38:06,747][PyLogger][INFO]: Rank[0]: Epoch[66/500], iter[6400] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.55866
[2024-02-27 05:38:06,747][PyLogger][INFO]: Rank[1]: Epoch[66/500], iter[6400] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.64215
[2024-02-27 05:38:33,660][PyLogger][INFO]: Rank[1]: Epoch[67/500], iter[6450] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.56720
[2024-02-27 05:38:33,660][PyLogger][INFO]: Rank[0]: Epoch[67/500], iter[6450] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.57925
[2024-02-27 05:39:00,512][PyLogger][INFO]: Rank[0]: Epoch[68/500], iter[6500] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.56289
[2024-02-27 05:39:00,512][PyLogger][INFO]: Rank[1]: Epoch[68/500], iter[6500] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.54920
[2024-02-27 05:39:26,977][PyLogger][INFO]: Rank[1]: Epoch[68/500], iter[6550] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.61742
[2024-02-27 05:39:26,977][PyLogger][INFO]: Rank[0]: Epoch[68/500], iter[6550] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.57959
[2024-02-27 05:39:53,788][PyLogger][INFO]: Rank[0]: Epoch[69/500], iter[6600] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.59639
[2024-02-27 05:39:53,788][PyLogger][INFO]: Rank[1]: Epoch[69/500], iter[6600] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.58652
[2024-02-27 05:40:20,181][PyLogger][INFO]: Rank[1]: Epoch[69/500], iter[6650] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.55637
[2024-02-27 05:40:20,181][PyLogger][INFO]: Rank[0]: Epoch[69/500], iter[6650] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.56209
[2024-02-27 05:40:46,978][PyLogger][INFO]: Rank[1]: Epoch[70/500], iter[6700] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.59581
[2024-02-27 05:40:46,978][PyLogger][INFO]: Rank[0]: Epoch[70/500], iter[6700] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.60545
[2024-02-27 05:41:13,400][PyLogger][INFO]: Rank[0]: Epoch[70/500], iter[6750] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.56439
[2024-02-27 05:41:13,400][PyLogger][INFO]: Rank[1]: Epoch[70/500], iter[6750] - train_accuracy: 0.95355 - val_accuracy: 0.88290 - train_loss: 0.60568 - val_loss: 0.79869 - loss: 0.57765
[2024-02-27 05:42:07,802][PyLogger][INFO]: Rank[1]: Epoch[71/500], iter[6800] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.57206
[2024-02-27 05:42:07,802][PyLogger][INFO]: Rank[0]: Epoch[71/500], iter[6800] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.60707
[2024-02-27 05:42:34,192][PyLogger][INFO]: Rank[0]: Epoch[71/500], iter[6850] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.57688
[2024-02-27 05:42:34,192][PyLogger][INFO]: Rank[1]: Epoch[71/500], iter[6850] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.57530
[2024-02-27 05:43:00,918][PyLogger][INFO]: Rank[1]: Epoch[72/500], iter[6900] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.56757
[2024-02-27 05:43:00,918][PyLogger][INFO]: Rank[0]: Epoch[72/500], iter[6900] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.58297
[2024-02-27 05:43:27,342][PyLogger][INFO]: Rank[0]: Epoch[72/500], iter[6950] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.55680
[2024-02-27 05:43:27,342][PyLogger][INFO]: Rank[1]: Epoch[72/500], iter[6950] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.59785
[2024-02-27 05:43:54,215][PyLogger][INFO]: Rank[0]: Epoch[73/500], iter[7000] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.56456
[2024-02-27 05:43:54,215][PyLogger][INFO]: Rank[1]: Epoch[73/500], iter[7000] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.56496
[2024-02-27 05:44:20,441][PyLogger][INFO]: Rank[1]: Epoch[73/500], iter[7050] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.61734
[2024-02-27 05:44:20,441][PyLogger][INFO]: Rank[0]: Epoch[73/500], iter[7050] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.56120
[2024-02-27 05:44:47,370][PyLogger][INFO]: Rank[1]: Epoch[74/500], iter[7100] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.59219
[2024-02-27 05:44:47,370][PyLogger][INFO]: Rank[0]: Epoch[74/500], iter[7100] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.59188
[2024-02-27 05:45:13,743][PyLogger][INFO]: Rank[0]: Epoch[74/500], iter[7150] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.56750
[2024-02-27 05:45:13,744][PyLogger][INFO]: Rank[1]: Epoch[74/500], iter[7150] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.59494
[2024-02-27 05:45:40,469][PyLogger][INFO]: Rank[1]: Epoch[75/500], iter[7200] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.56184
[2024-02-27 05:45:40,469][PyLogger][INFO]: Rank[0]: Epoch[75/500], iter[7200] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.57944
[2024-02-27 05:46:06,884][PyLogger][INFO]: Rank[1]: Epoch[75/500], iter[7250] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.55227
[2024-02-27 05:46:06,884][PyLogger][INFO]: Rank[0]: Epoch[75/500], iter[7250] - train_accuracy: 0.95959 - val_accuracy: 0.88000 - train_loss: 0.59314 - val_loss: 0.79131 - loss: 0.55365
[2024-02-27 05:47:01,442][PyLogger][INFO]: Rank[0]: Epoch[76/500], iter[7300] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.58508
[2024-02-27 05:47:01,442][PyLogger][INFO]: Rank[1]: Epoch[76/500], iter[7300] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.56512
[2024-02-27 05:47:27,835][PyLogger][INFO]: Rank[0]: Epoch[76/500], iter[7350] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.56393
[2024-02-27 05:47:27,835][PyLogger][INFO]: Rank[1]: Epoch[76/500], iter[7350] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.52994
[2024-02-27 05:47:54,554][PyLogger][INFO]: Rank[1]: Epoch[77/500], iter[7400] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.56720
[2024-02-27 05:47:54,554][PyLogger][INFO]: Rank[0]: Epoch[77/500], iter[7400] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.56876
[2024-02-27 05:48:20,978][PyLogger][INFO]: Rank[0]: Epoch[77/500], iter[7450] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.56433
[2024-02-27 05:48:20,978][PyLogger][INFO]: Rank[1]: Epoch[77/500], iter[7450] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.53238
[2024-02-27 05:48:47,834][PyLogger][INFO]: Rank[1]: Epoch[78/500], iter[7500] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.53531
[2024-02-27 05:48:47,834][PyLogger][INFO]: Rank[0]: Epoch[78/500], iter[7500] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.56021
[2024-02-27 05:49:14,196][PyLogger][INFO]: Rank[1]: Epoch[78/500], iter[7550] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.55108
[2024-02-27 05:49:14,196][PyLogger][INFO]: Rank[0]: Epoch[78/500], iter[7550] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.54752
[2024-02-27 05:49:41,211][PyLogger][INFO]: Rank[0]: Epoch[79/500], iter[7600] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.57607
[2024-02-27 05:49:41,211][PyLogger][INFO]: Rank[1]: Epoch[79/500], iter[7600] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.54563
[2024-02-27 05:50:07,488][PyLogger][INFO]: Rank[1]: Epoch[79/500], iter[7650] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.55147
[2024-02-27 05:50:07,488][PyLogger][INFO]: Rank[0]: Epoch[79/500], iter[7650] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.54631
[2024-02-27 05:50:34,190][PyLogger][INFO]: Rank[1]: Epoch[80/500], iter[7700] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.55339
[2024-02-27 05:50:34,190][PyLogger][INFO]: Rank[0]: Epoch[80/500], iter[7700] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.55595
[2024-02-27 05:51:00,639][PyLogger][INFO]: Rank[0]: Epoch[80/500], iter[7750] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.56389
[2024-02-27 05:51:00,639][PyLogger][INFO]: Rank[1]: Epoch[80/500], iter[7750] - train_accuracy: 0.94725 - val_accuracy: 0.87510 - train_loss: 0.62021 - val_loss: 0.82616 - loss: 0.58266
[2024-02-27 05:51:55,244][PyLogger][INFO]: Rank[1]: Epoch[81/500], iter[7800] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.55708
[2024-02-27 05:51:55,244][PyLogger][INFO]: Rank[0]: Epoch[81/500], iter[7800] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.57264
[2024-02-27 05:52:21,565][PyLogger][INFO]: Rank[0]: Epoch[81/500], iter[7850] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.57275
[2024-02-27 05:52:21,565][PyLogger][INFO]: Rank[1]: Epoch[81/500], iter[7850] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.55850
[2024-02-27 05:52:48,352][PyLogger][INFO]: Rank[1]: Epoch[82/500], iter[7900] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.56272
[2024-02-27 05:52:48,352][PyLogger][INFO]: Rank[0]: Epoch[82/500], iter[7900] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.54140
[2024-02-27 05:53:14,749][PyLogger][INFO]: Rank[1]: Epoch[82/500], iter[7950] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.56223
[2024-02-27 05:53:14,749][PyLogger][INFO]: Rank[0]: Epoch[82/500], iter[7950] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.53113
[2024-02-27 05:53:41,627][PyLogger][INFO]: Rank[1]: Epoch[83/500], iter[8000] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.56427
[2024-02-27 05:53:41,627][PyLogger][INFO]: Rank[0]: Epoch[83/500], iter[8000] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.54677
[2024-02-27 05:54:07,848][PyLogger][INFO]: Rank[1]: Epoch[83/500], iter[8050] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.56919
[2024-02-27 05:54:07,848][PyLogger][INFO]: Rank[0]: Epoch[83/500], iter[8050] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.55867
[2024-02-27 05:54:34,605][PyLogger][INFO]: Rank[1]: Epoch[84/500], iter[8100] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.57357
[2024-02-27 05:54:34,605][PyLogger][INFO]: Rank[0]: Epoch[84/500], iter[8100] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.54348
[2024-02-27 05:55:01,419][PyLogger][INFO]: Rank[1]: Epoch[85/500], iter[8150] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.56262
[2024-02-27 05:55:01,419][PyLogger][INFO]: Rank[0]: Epoch[85/500], iter[8150] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.54868
[2024-02-27 05:55:27,607][PyLogger][INFO]: Rank[0]: Epoch[85/500], iter[8200] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.53333
[2024-02-27 05:55:27,607][PyLogger][INFO]: Rank[1]: Epoch[85/500], iter[8200] - train_accuracy: 0.96287 - val_accuracy: 0.88690 - train_loss: 0.58644 - val_loss: 0.80378 - loss: 0.56798
[2024-02-27 05:56:21,966][PyLogger][INFO]: Rank[1]: Epoch[86/500], iter[8250] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.55880
[2024-02-27 05:56:21,966][PyLogger][INFO]: Rank[0]: Epoch[86/500], iter[8250] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.55221
[2024-02-27 05:56:48,349][PyLogger][INFO]: Rank[0]: Epoch[86/500], iter[8300] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.57742
[2024-02-27 05:56:48,349][PyLogger][INFO]: Rank[1]: Epoch[86/500], iter[8300] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.55803
[2024-02-27 05:57:15,090][PyLogger][INFO]: Rank[1]: Epoch[87/500], iter[8350] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.56837
[2024-02-27 05:57:15,090][PyLogger][INFO]: Rank[0]: Epoch[87/500], iter[8350] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.54740
[2024-02-27 05:57:41,452][PyLogger][INFO]: Rank[1]: Epoch[87/500], iter[8400] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.61067
[2024-02-27 05:57:41,452][PyLogger][INFO]: Rank[0]: Epoch[87/500], iter[8400] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.52936
[2024-02-27 05:58:08,173][PyLogger][INFO]: Rank[1]: Epoch[88/500], iter[8450] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.57548
[2024-02-27 05:58:08,173][PyLogger][INFO]: Rank[0]: Epoch[88/500], iter[8450] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.54674
[2024-02-27 05:58:34,420][PyLogger][INFO]: Rank[0]: Epoch[88/500], iter[8500] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.54801
[2024-02-27 05:58:34,420][PyLogger][INFO]: Rank[1]: Epoch[88/500], iter[8500] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.56048
[2024-02-27 05:59:01,132][PyLogger][INFO]: Rank[1]: Epoch[89/500], iter[8550] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.58654
[2024-02-27 05:59:01,132][PyLogger][INFO]: Rank[0]: Epoch[89/500], iter[8550] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.53899
[2024-02-27 05:59:27,554][PyLogger][INFO]: Rank[1]: Epoch[89/500], iter[8600] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.54748
[2024-02-27 05:59:27,554][PyLogger][INFO]: Rank[0]: Epoch[89/500], iter[8600] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.55697
[2024-02-27 05:59:54,341][PyLogger][INFO]: Rank[0]: Epoch[90/500], iter[8650] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.53469
[2024-02-27 05:59:54,341][PyLogger][INFO]: Rank[1]: Epoch[90/500], iter[8650] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.53862
[2024-02-27 06:00:20,731][PyLogger][INFO]: Rank[1]: Epoch[90/500], iter[8700] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.54729
[2024-02-27 06:00:20,731][PyLogger][INFO]: Rank[0]: Epoch[90/500], iter[8700] - train_accuracy: 0.97346 - val_accuracy: 0.89620 - train_loss: 0.56198 - val_loss: 0.76954 - loss: 0.56188
[2024-02-27 06:01:15,154][PyLogger][INFO]: Rank[1]: Epoch[91/500], iter[8750] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.54653
[2024-02-27 06:01:15,154][PyLogger][INFO]: Rank[0]: Epoch[91/500], iter[8750] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.52973
[2024-02-27 06:01:41,522][PyLogger][INFO]: Rank[1]: Epoch[91/500], iter[8800] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.57881
[2024-02-27 06:01:41,522][PyLogger][INFO]: Rank[0]: Epoch[91/500], iter[8800] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.53701
[2024-02-27 06:02:08,347][PyLogger][INFO]: Rank[1]: Epoch[92/500], iter[8850] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.56224
[2024-02-27 06:02:08,347][PyLogger][INFO]: Rank[0]: Epoch[92/500], iter[8850] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.55443
[2024-02-27 06:02:34,667][PyLogger][INFO]: Rank[0]: Epoch[92/500], iter[8900] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.53875
[2024-02-27 06:02:34,667][PyLogger][INFO]: Rank[1]: Epoch[92/500], iter[8900] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.53680
[2024-02-27 06:03:01,343][PyLogger][INFO]: Rank[1]: Epoch[93/500], iter[8950] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.55408
[2024-02-27 06:03:01,343][PyLogger][INFO]: Rank[0]: Epoch[93/500], iter[8950] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.53376
[2024-02-27 06:03:27,571][PyLogger][INFO]: Rank[1]: Epoch[93/500], iter[9000] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.52981
[2024-02-27 06:03:27,571][PyLogger][INFO]: Rank[0]: Epoch[93/500], iter[9000] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.55312
[2024-02-27 06:03:54,397][PyLogger][INFO]: Rank[1]: Epoch[94/500], iter[9050] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.52527
[2024-02-27 06:03:54,397][PyLogger][INFO]: Rank[0]: Epoch[94/500], iter[9050] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.54786
[2024-02-27 06:04:20,623][PyLogger][INFO]: Rank[0]: Epoch[94/500], iter[9100] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.54839
[2024-02-27 06:04:20,623][PyLogger][INFO]: Rank[1]: Epoch[94/500], iter[9100] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.54449
[2024-02-27 06:04:47,368][PyLogger][INFO]: Rank[1]: Epoch[95/500], iter[9150] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.56682
[2024-02-27 06:04:47,368][PyLogger][INFO]: Rank[0]: Epoch[95/500], iter[9150] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.51336
[2024-02-27 06:05:13,689][PyLogger][INFO]: Rank[1]: Epoch[95/500], iter[9200] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.58039
[2024-02-27 06:05:13,689][PyLogger][INFO]: Rank[0]: Epoch[95/500], iter[9200] - train_accuracy: 0.97090 - val_accuracy: 0.89230 - train_loss: 0.56765 - val_loss: 0.77634 - loss: 0.55502
[2024-02-27 06:06:07,676][PyLogger][INFO]: Rank[0]: Epoch[96/500], iter[9250] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.53912
[2024-02-27 06:06:07,677][PyLogger][INFO]: Rank[1]: Epoch[96/500], iter[9250] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.53234
[2024-02-27 06:06:33,742][PyLogger][INFO]: Rank[1]: Epoch[96/500], iter[9300] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.56274
[2024-02-27 06:06:33,742][PyLogger][INFO]: Rank[0]: Epoch[96/500], iter[9300] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.61262
[2024-02-27 06:07:00,349][PyLogger][INFO]: Rank[0]: Epoch[97/500], iter[9350] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.55672
[2024-02-27 06:07:00,349][PyLogger][INFO]: Rank[1]: Epoch[97/500], iter[9350] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.52995
[2024-02-27 06:07:26,614][PyLogger][INFO]: Rank[1]: Epoch[97/500], iter[9400] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.56995
[2024-02-27 06:07:26,614][PyLogger][INFO]: Rank[0]: Epoch[97/500], iter[9400] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.57031
[2024-02-27 06:07:53,371][PyLogger][INFO]: Rank[1]: Epoch[98/500], iter[9450] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.54501
[2024-02-27 06:07:53,371][PyLogger][INFO]: Rank[0]: Epoch[98/500], iter[9450] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.56448
[2024-02-27 06:08:19,643][PyLogger][INFO]: Rank[0]: Epoch[98/500], iter[9500] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.53783
[2024-02-27 06:08:19,643][PyLogger][INFO]: Rank[1]: Epoch[98/500], iter[9500] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.55002
[2024-02-27 06:08:46,348][PyLogger][INFO]: Rank[1]: Epoch[99/500], iter[9550] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.55375
[2024-02-27 06:08:46,348][PyLogger][INFO]: Rank[0]: Epoch[99/500], iter[9550] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.55473
[2024-02-27 06:09:12,570][PyLogger][INFO]: Rank[1]: Epoch[99/500], iter[9600] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.55103
[2024-02-27 06:09:12,570][PyLogger][INFO]: Rank[0]: Epoch[99/500], iter[9600] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.55754
[2024-02-27 06:09:39,461][PyLogger][INFO]: Rank[1]: Epoch[100/500], iter[9650] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.55431
[2024-02-27 06:09:39,461][PyLogger][INFO]: Rank[0]: Epoch[100/500], iter[9650] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.53823
[2024-02-27 06:10:05,795][PyLogger][INFO]: Rank[1]: Epoch[100/500], iter[9700] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.57073
[2024-02-27 06:10:05,795][PyLogger][INFO]: Rank[0]: Epoch[100/500], iter[9700] - train_accuracy: 0.97453 - val_accuracy: 0.89600 - train_loss: 0.56012 - val_loss: 0.77470 - loss: 0.53639
[2024-02-27 06:11:00,024][PyLogger][INFO]: Rank[1]: Epoch[101/500], iter[9750] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.54677
[2024-02-27 06:11:00,024][PyLogger][INFO]: Rank[0]: Epoch[101/500], iter[9750] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.54409
[2024-02-27 06:11:26,922][PyLogger][INFO]: Rank[1]: Epoch[102/500], iter[9800] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.53989
[2024-02-27 06:11:26,922][PyLogger][INFO]: Rank[0]: Epoch[102/500], iter[9800] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.56061
[2024-02-27 06:11:53,235][PyLogger][INFO]: Rank[0]: Epoch[102/500], iter[9850] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.54629
[2024-02-27 06:11:53,235][PyLogger][INFO]: Rank[1]: Epoch[102/500], iter[9850] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.55230
[2024-02-27 06:12:19,970][PyLogger][INFO]: Rank[1]: Epoch[103/500], iter[9900] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.56903
[2024-02-27 06:12:19,970][PyLogger][INFO]: Rank[0]: Epoch[103/500], iter[9900] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.52725
[2024-02-27 06:12:46,239][PyLogger][INFO]: Rank[1]: Epoch[103/500], iter[9950] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.52964
[2024-02-27 06:12:46,239][PyLogger][INFO]: Rank[0]: Epoch[103/500], iter[9950] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.53542
[2024-02-27 06:13:12,981][PyLogger][INFO]: Rank[0]: Epoch[104/500], iter[10000] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.53103
[2024-02-27 06:13:12,981][PyLogger][INFO]: Rank[1]: Epoch[104/500], iter[10000] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.52025
[2024-02-27 06:13:39,227][PyLogger][INFO]: Rank[1]: Epoch[104/500], iter[10050] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.57563
[2024-02-27 06:13:39,227][PyLogger][INFO]: Rank[0]: Epoch[104/500], iter[10050] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.53652
[2024-02-27 06:14:06,115][PyLogger][INFO]: Rank[1]: Epoch[105/500], iter[10100] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.54039
[2024-02-27 06:14:06,115][PyLogger][INFO]: Rank[0]: Epoch[105/500], iter[10100] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.52162
[2024-02-27 06:14:32,313][PyLogger][INFO]: Rank[1]: Epoch[105/500], iter[10150] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.59262
[2024-02-27 06:14:32,313][PyLogger][INFO]: Rank[0]: Epoch[105/500], iter[10150] - train_accuracy: 0.97848 - val_accuracy: 0.90140 - train_loss: 0.55136 - val_loss: 0.76044 - loss: 0.54249
[2024-02-27 06:15:26,384][PyLogger][INFO]: Rank[1]: Epoch[106/500], iter[10200] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.56217
[2024-02-27 06:15:26,384][PyLogger][INFO]: Rank[0]: Epoch[106/500], iter[10200] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.51350
[2024-02-27 06:15:52,689][PyLogger][INFO]: Rank[0]: Epoch[106/500], iter[10250] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.53679
[2024-02-27 06:15:52,689][PyLogger][INFO]: Rank[1]: Epoch[106/500], iter[10250] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.53639
[2024-02-27 06:16:19,318][PyLogger][INFO]: Rank[1]: Epoch[107/500], iter[10300] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.52027
[2024-02-27 06:16:19,318][PyLogger][INFO]: Rank[0]: Epoch[107/500], iter[10300] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.54723
[2024-02-27 06:16:45,601][PyLogger][INFO]: Rank[1]: Epoch[107/500], iter[10350] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.56408
[2024-02-27 06:16:45,601][PyLogger][INFO]: Rank[0]: Epoch[107/500], iter[10350] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.53775
[2024-02-27 06:17:12,347][PyLogger][INFO]: Rank[0]: Epoch[108/500], iter[10400] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.53679
[2024-02-27 06:17:12,347][PyLogger][INFO]: Rank[1]: Epoch[108/500], iter[10400] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.53073
[2024-02-27 06:17:38,494][PyLogger][INFO]: Rank[1]: Epoch[108/500], iter[10450] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.54972
[2024-02-27 06:17:38,494][PyLogger][INFO]: Rank[0]: Epoch[108/500], iter[10450] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.56058
[2024-02-27 06:18:05,360][PyLogger][INFO]: Rank[1]: Epoch[109/500], iter[10500] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.53273
[2024-02-27 06:18:05,360][PyLogger][INFO]: Rank[0]: Epoch[109/500], iter[10500] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.52606
[2024-02-27 06:18:31,576][PyLogger][INFO]: Rank[1]: Epoch[109/500], iter[10550] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.56224
[2024-02-27 06:18:31,577][PyLogger][INFO]: Rank[0]: Epoch[109/500], iter[10550] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.55852
[2024-02-27 06:18:58,288][PyLogger][INFO]: Rank[1]: Epoch[110/500], iter[10600] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.53408
[2024-02-27 06:18:58,288][PyLogger][INFO]: Rank[0]: Epoch[110/500], iter[10600] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.54317
[2024-02-27 06:19:24,574][PyLogger][INFO]: Rank[0]: Epoch[110/500], iter[10650] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.54281
[2024-02-27 06:19:24,574][PyLogger][INFO]: Rank[1]: Epoch[110/500], iter[10650] - train_accuracy: 0.98065 - val_accuracy: 0.89860 - train_loss: 0.54542 - val_loss: 0.76497 - loss: 0.54015
[2024-02-27 06:20:18,881][PyLogger][INFO]: Rank[0]: Epoch[111/500], iter[10700] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.51796
[2024-02-27 06:20:18,881][PyLogger][INFO]: Rank[1]: Epoch[111/500], iter[10700] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.53219
[2024-02-27 06:20:45,144][PyLogger][INFO]: Rank[1]: Epoch[111/500], iter[10750] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.54376
[2024-02-27 06:20:45,144][PyLogger][INFO]: Rank[0]: Epoch[111/500], iter[10750] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.53323
[2024-02-27 06:21:11,820][PyLogger][INFO]: Rank[1]: Epoch[112/500], iter[10800] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.55324
[2024-02-27 06:21:11,820][PyLogger][INFO]: Rank[0]: Epoch[112/500], iter[10800] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.56039
[2024-02-27 06:21:37,906][PyLogger][INFO]: Rank[1]: Epoch[112/500], iter[10850] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.53993
[2024-02-27 06:21:37,906][PyLogger][INFO]: Rank[0]: Epoch[112/500], iter[10850] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.53834
[2024-02-27 06:22:04,618][PyLogger][INFO]: Rank[0]: Epoch[113/500], iter[10900] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.51519
[2024-02-27 06:22:04,618][PyLogger][INFO]: Rank[1]: Epoch[113/500], iter[10900] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.54538
[2024-02-27 06:22:30,872][PyLogger][INFO]: Rank[0]: Epoch[113/500], iter[10950] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.53639
[2024-02-27 06:22:30,872][PyLogger][INFO]: Rank[1]: Epoch[113/500], iter[10950] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.53545
[2024-02-27 06:22:57,755][PyLogger][INFO]: Rank[1]: Epoch[114/500], iter[11000] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.55861
[2024-02-27 06:22:57,755][PyLogger][INFO]: Rank[0]: Epoch[114/500], iter[11000] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.52302
[2024-02-27 06:23:24,009][PyLogger][INFO]: Rank[1]: Epoch[114/500], iter[11050] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.53599
[2024-02-27 06:23:24,009][PyLogger][INFO]: Rank[0]: Epoch[114/500], iter[11050] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.53636
[2024-02-27 06:23:50,913][PyLogger][INFO]: Rank[1]: Epoch[115/500], iter[11100] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.53090
[2024-02-27 06:23:50,913][PyLogger][INFO]: Rank[0]: Epoch[115/500], iter[11100] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.55046
[2024-02-27 06:24:17,194][PyLogger][INFO]: Rank[1]: Epoch[115/500], iter[11150] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.52529
[2024-02-27 06:24:17,194][PyLogger][INFO]: Rank[0]: Epoch[115/500], iter[11150] - train_accuracy: 0.98164 - val_accuracy: 0.90300 - train_loss: 0.54345 - val_loss: 0.76121 - loss: 0.55389
[2024-02-27 06:25:11,218][PyLogger][INFO]: Rank[1]: Epoch[116/500], iter[11200] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.54221
[2024-02-27 06:25:11,218][PyLogger][INFO]: Rank[0]: Epoch[116/500], iter[11200] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.52623
[2024-02-27 06:25:37,507][PyLogger][INFO]: Rank[0]: Epoch[116/500], iter[11250] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.53513
[2024-02-27 06:25:37,507][PyLogger][INFO]: Rank[1]: Epoch[116/500], iter[11250] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.55046
[2024-02-27 06:26:04,116][PyLogger][INFO]: Rank[0]: Epoch[117/500], iter[11300] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.54855
[2024-02-27 06:26:04,116][PyLogger][INFO]: Rank[1]: Epoch[117/500], iter[11300] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.55416
[2024-02-27 06:26:30,691][PyLogger][INFO]: Rank[1]: Epoch[118/500], iter[11350] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.54099
[2024-02-27 06:26:30,691][PyLogger][INFO]: Rank[0]: Epoch[118/500], iter[11350] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.52241
[2024-02-27 06:26:57,075][PyLogger][INFO]: Rank[0]: Epoch[118/500], iter[11400] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.53496
[2024-02-27 06:26:57,075][PyLogger][INFO]: Rank[1]: Epoch[118/500], iter[11400] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.55125
[2024-02-27 06:27:23,714][PyLogger][INFO]: Rank[1]: Epoch[119/500], iter[11450] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.52573
[2024-02-27 06:27:23,714][PyLogger][INFO]: Rank[0]: Epoch[119/500], iter[11450] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.54682
[2024-02-27 06:27:49,951][PyLogger][INFO]: Rank[1]: Epoch[119/500], iter[11500] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.53420
[2024-02-27 06:27:49,951][PyLogger][INFO]: Rank[0]: Epoch[119/500], iter[11500] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.57091
[2024-02-27 06:28:16,637][PyLogger][INFO]: Rank[0]: Epoch[120/500], iter[11550] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.53748
[2024-02-27 06:28:16,637][PyLogger][INFO]: Rank[1]: Epoch[120/500], iter[11550] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.53622
[2024-02-27 06:28:42,707][PyLogger][INFO]: Rank[1]: Epoch[120/500], iter[11600] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.51272
[2024-02-27 06:28:42,707][PyLogger][INFO]: Rank[0]: Epoch[120/500], iter[11600] - train_accuracy: 0.98073 - val_accuracy: 0.90400 - train_loss: 0.54439 - val_loss: 0.76004 - loss: 0.53142
[2024-02-27 06:29:36,644][PyLogger][INFO]: Rank[1]: Epoch[121/500], iter[11650] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.55479
[2024-02-27 06:29:36,644][PyLogger][INFO]: Rank[0]: Epoch[121/500], iter[11650] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.51748
[2024-02-27 06:30:03,000][PyLogger][INFO]: Rank[1]: Epoch[121/500], iter[11700] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.54044
[2024-02-27 06:30:03,000][PyLogger][INFO]: Rank[0]: Epoch[121/500], iter[11700] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.51984
[2024-02-27 06:30:29,694][PyLogger][INFO]: Rank[0]: Epoch[122/500], iter[11750] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.54499
[2024-02-27 06:30:29,694][PyLogger][INFO]: Rank[1]: Epoch[122/500], iter[11750] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.54027
[2024-02-27 06:30:55,853][PyLogger][INFO]: Rank[1]: Epoch[122/500], iter[11800] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.53656
[2024-02-27 06:30:55,853][PyLogger][INFO]: Rank[0]: Epoch[122/500], iter[11800] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.55668
[2024-02-27 06:31:22,515][PyLogger][INFO]: Rank[0]: Epoch[123/500], iter[11850] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.51167
[2024-02-27 06:31:22,515][PyLogger][INFO]: Rank[1]: Epoch[123/500], iter[11850] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.51691
[2024-02-27 06:31:48,826][PyLogger][INFO]: Rank[1]: Epoch[123/500], iter[11900] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.57249
[2024-02-27 06:31:48,826][PyLogger][INFO]: Rank[0]: Epoch[123/500], iter[11900] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.52911
[2024-02-27 06:32:15,475][PyLogger][INFO]: Rank[1]: Epoch[124/500], iter[11950] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.51459
[2024-02-27 06:32:15,476][PyLogger][INFO]: Rank[0]: Epoch[124/500], iter[11950] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.53741
[2024-02-27 06:32:41,831][PyLogger][INFO]: Rank[1]: Epoch[124/500], iter[12000] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.53026
[2024-02-27 06:32:41,831][PyLogger][INFO]: Rank[0]: Epoch[124/500], iter[12000] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.51936
[2024-02-27 06:33:08,503][PyLogger][INFO]: Rank[1]: Epoch[125/500], iter[12050] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.53259
[2024-02-27 06:33:08,503][PyLogger][INFO]: Rank[0]: Epoch[125/500], iter[12050] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.52071
[2024-02-27 06:33:34,652][PyLogger][INFO]: Rank[0]: Epoch[125/500], iter[12100] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.52768
[2024-02-27 06:33:34,652][PyLogger][INFO]: Rank[1]: Epoch[125/500], iter[12100] - train_accuracy: 0.97918 - val_accuracy: 0.89420 - train_loss: 0.54928 - val_loss: 0.78206 - loss: 0.54902
[2024-02-27 06:34:28,414][PyLogger][INFO]: Rank[0]: Epoch[126/500], iter[12150] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.52631
[2024-02-27 06:34:28,414][PyLogger][INFO]: Rank[1]: Epoch[126/500], iter[12150] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.53147
[2024-02-27 06:34:54,637][PyLogger][INFO]: Rank[1]: Epoch[126/500], iter[12200] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.52591
[2024-02-27 06:34:54,638][PyLogger][INFO]: Rank[0]: Epoch[126/500], iter[12200] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.52443
[2024-02-27 06:35:21,358][PyLogger][INFO]: Rank[0]: Epoch[127/500], iter[12250] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.52243
[2024-02-27 06:35:21,358][PyLogger][INFO]: Rank[1]: Epoch[127/500], iter[12250] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.53695
[2024-02-27 06:35:47,634][PyLogger][INFO]: Rank[1]: Epoch[127/500], iter[12300] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.52865
[2024-02-27 06:35:47,634][PyLogger][INFO]: Rank[0]: Epoch[127/500], iter[12300] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.53072
[2024-02-27 06:36:14,418][PyLogger][INFO]: Rank[0]: Epoch[128/500], iter[12350] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.53616
[2024-02-27 06:36:14,418][PyLogger][INFO]: Rank[1]: Epoch[128/500], iter[12350] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.53950
[2024-02-27 06:36:40,645][PyLogger][INFO]: Rank[1]: Epoch[128/500], iter[12400] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.52378
[2024-02-27 06:36:40,645][PyLogger][INFO]: Rank[0]: Epoch[128/500], iter[12400] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.53831
[2024-02-27 06:37:07,366][PyLogger][INFO]: Rank[1]: Epoch[129/500], iter[12450] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.53919
[2024-02-27 06:37:07,366][PyLogger][INFO]: Rank[0]: Epoch[129/500], iter[12450] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.52409
[2024-02-27 06:37:33,731][PyLogger][INFO]: Rank[0]: Epoch[129/500], iter[12500] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.54448
[2024-02-27 06:37:33,731][PyLogger][INFO]: Rank[1]: Epoch[129/500], iter[12500] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.52950
[2024-02-27 06:38:00,412][PyLogger][INFO]: Rank[1]: Epoch[130/500], iter[12550] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.53345
[2024-02-27 06:38:00,412][PyLogger][INFO]: Rank[0]: Epoch[130/500], iter[12550] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.52590
[2024-02-27 06:38:26,688][PyLogger][INFO]: Rank[0]: Epoch[130/500], iter[12600] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.52201
[2024-02-27 06:38:26,688][PyLogger][INFO]: Rank[1]: Epoch[130/500], iter[12600] - train_accuracy: 0.97761 - val_accuracy: 0.90080 - train_loss: 0.55180 - val_loss: 0.77142 - loss: 0.51825
[2024-02-27 06:39:20,622][PyLogger][INFO]: Rank[1]: Epoch[131/500], iter[12650] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.52524
[2024-02-27 06:39:20,622][PyLogger][INFO]: Rank[0]: Epoch[131/500], iter[12650] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.51500
[2024-02-27 06:39:46,899][PyLogger][INFO]: Rank[0]: Epoch[131/500], iter[12700] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.53219
[2024-02-27 06:39:46,899][PyLogger][INFO]: Rank[1]: Epoch[131/500], iter[12700] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.53600
[2024-02-27 06:40:13,583][PyLogger][INFO]: Rank[0]: Epoch[132/500], iter[12750] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.52608
[2024-02-27 06:40:13,583][PyLogger][INFO]: Rank[1]: Epoch[132/500], iter[12750] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.55900
[2024-02-27 06:40:39,773][PyLogger][INFO]: Rank[1]: Epoch[132/500], iter[12800] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.51112
[2024-02-27 06:40:39,773][PyLogger][INFO]: Rank[0]: Epoch[132/500], iter[12800] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.54041
[2024-02-27 06:41:06,459][PyLogger][INFO]: Rank[0]: Epoch[133/500], iter[12850] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.51566
[2024-02-27 06:41:06,459][PyLogger][INFO]: Rank[1]: Epoch[133/500], iter[12850] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.53104
[2024-02-27 06:41:32,584][PyLogger][INFO]: Rank[1]: Epoch[133/500], iter[12900] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.54445
[2024-02-27 06:41:32,584][PyLogger][INFO]: Rank[0]: Epoch[133/500], iter[12900] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.54612
[2024-02-27 06:41:59,273][PyLogger][INFO]: Rank[0]: Epoch[134/500], iter[12950] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.52638
[2024-02-27 06:41:59,273][PyLogger][INFO]: Rank[1]: Epoch[134/500], iter[12950] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.52019
[2024-02-27 06:42:25,770][PyLogger][INFO]: Rank[0]: Epoch[135/500], iter[13000] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.50825
[2024-02-27 06:42:25,770][PyLogger][INFO]: Rank[1]: Epoch[135/500], iter[13000] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.51681
[2024-02-27 06:42:52,042][PyLogger][INFO]: Rank[1]: Epoch[135/500], iter[13050] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.52544
[2024-02-27 06:42:52,042][PyLogger][INFO]: Rank[0]: Epoch[135/500], iter[13050] - train_accuracy: 0.97576 - val_accuracy: 0.90190 - train_loss: 0.55573 - val_loss: 0.77191 - loss: 0.53983
[2024-02-27 06:43:45,766][PyLogger][INFO]: Rank[1]: Epoch[136/500], iter[13100] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.52653
[2024-02-27 06:43:45,766][PyLogger][INFO]: Rank[0]: Epoch[136/500], iter[13100] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.53571
[2024-02-27 06:44:11,936][PyLogger][INFO]: Rank[1]: Epoch[136/500], iter[13150] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.51841
[2024-02-27 06:44:11,936][PyLogger][INFO]: Rank[0]: Epoch[136/500], iter[13150] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.51548
[2024-02-27 06:44:38,523][PyLogger][INFO]: Rank[0]: Epoch[137/500], iter[13200] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.53298
[2024-02-27 06:44:38,523][PyLogger][INFO]: Rank[1]: Epoch[137/500], iter[13200] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.52961
[2024-02-27 06:45:04,838][PyLogger][INFO]: Rank[1]: Epoch[137/500], iter[13250] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.51929
[2024-02-27 06:45:04,838][PyLogger][INFO]: Rank[0]: Epoch[137/500], iter[13250] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.51869
[2024-02-27 06:45:31,558][PyLogger][INFO]: Rank[1]: Epoch[138/500], iter[13300] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.51523
[2024-02-27 06:45:31,558][PyLogger][INFO]: Rank[0]: Epoch[138/500], iter[13300] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.52175
[2024-02-27 06:45:57,688][PyLogger][INFO]: Rank[0]: Epoch[138/500], iter[13350] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.52203
[2024-02-27 06:45:57,688][PyLogger][INFO]: Rank[1]: Epoch[138/500], iter[13350] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.54906
[2024-02-27 06:46:24,406][PyLogger][INFO]: Rank[1]: Epoch[139/500], iter[13400] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.54997
[2024-02-27 06:46:24,406][PyLogger][INFO]: Rank[0]: Epoch[139/500], iter[13400] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.53997
[2024-02-27 06:46:50,619][PyLogger][INFO]: Rank[0]: Epoch[139/500], iter[13450] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.51911
[2024-02-27 06:46:50,619][PyLogger][INFO]: Rank[1]: Epoch[139/500], iter[13450] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.51547
[2024-02-27 06:47:17,383][PyLogger][INFO]: Rank[1]: Epoch[140/500], iter[13500] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.52288
[2024-02-27 06:47:17,383][PyLogger][INFO]: Rank[0]: Epoch[140/500], iter[13500] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.52954
[2024-02-27 06:47:43,525][PyLogger][INFO]: Rank[1]: Epoch[140/500], iter[13550] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.54701
[2024-02-27 06:47:43,525][PyLogger][INFO]: Rank[0]: Epoch[140/500], iter[13550] - train_accuracy: 0.99076 - val_accuracy: 0.91920 - train_loss: 0.52302 - val_loss: 0.72887 - loss: 0.54725
[2024-02-27 06:48:37,489][PyLogger][INFO]: Rank[0]: Epoch[141/500], iter[13600] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.54041
[2024-02-27 06:48:37,489][PyLogger][INFO]: Rank[1]: Epoch[141/500], iter[13600] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.53390
[2024-02-27 06:49:03,720][PyLogger][INFO]: Rank[1]: Epoch[141/500], iter[13650] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.51740
[2024-02-27 06:49:03,720][PyLogger][INFO]: Rank[0]: Epoch[141/500], iter[13650] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.51769
[2024-02-27 06:49:30,568][PyLogger][INFO]: Rank[0]: Epoch[142/500], iter[13700] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.54081
[2024-02-27 06:49:30,568][PyLogger][INFO]: Rank[1]: Epoch[142/500], iter[13700] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.53124
[2024-02-27 06:49:56,793][PyLogger][INFO]: Rank[1]: Epoch[142/500], iter[13750] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.55103
[2024-02-27 06:49:56,793][PyLogger][INFO]: Rank[0]: Epoch[142/500], iter[13750] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.51066
[2024-02-27 06:50:23,450][PyLogger][INFO]: Rank[0]: Epoch[143/500], iter[13800] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.52914
[2024-02-27 06:50:23,450][PyLogger][INFO]: Rank[1]: Epoch[143/500], iter[13800] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.54173
[2024-02-27 06:50:49,655][PyLogger][INFO]: Rank[0]: Epoch[143/500], iter[13850] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.52470
[2024-02-27 06:50:49,655][PyLogger][INFO]: Rank[1]: Epoch[143/500], iter[13850] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.53728
[2024-02-27 06:51:16,120][PyLogger][INFO]: Rank[1]: Epoch[144/500], iter[13900] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.52137
[2024-02-27 06:51:16,120][PyLogger][INFO]: Rank[0]: Epoch[144/500], iter[13900] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.51579
[2024-02-27 06:51:42,349][PyLogger][INFO]: Rank[1]: Epoch[144/500], iter[13950] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.56963
[2024-02-27 06:51:42,349][PyLogger][INFO]: Rank[0]: Epoch[144/500], iter[13950] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.54593
[2024-02-27 06:52:08,928][PyLogger][INFO]: Rank[1]: Epoch[145/500], iter[14000] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.50804
[2024-02-27 06:52:08,928][PyLogger][INFO]: Rank[0]: Epoch[145/500], iter[14000] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.52025
[2024-02-27 06:52:35,210][PyLogger][INFO]: Rank[1]: Epoch[145/500], iter[14050] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.51888
[2024-02-27 06:52:35,210][PyLogger][INFO]: Rank[0]: Epoch[145/500], iter[14050] - train_accuracy: 0.97825 - val_accuracy: 0.89730 - train_loss: 0.55294 - val_loss: 0.78567 - loss: 0.51720
[2024-02-27 06:53:28,952][PyLogger][INFO]: Rank[0]: Epoch[146/500], iter[14100] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.52907
[2024-02-27 06:53:28,952][PyLogger][INFO]: Rank[1]: Epoch[146/500], iter[14100] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.53179
[2024-02-27 06:53:55,035][PyLogger][INFO]: Rank[0]: Epoch[146/500], iter[14150] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.54918
[2024-02-27 06:53:55,035][PyLogger][INFO]: Rank[1]: Epoch[146/500], iter[14150] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.51388
[2024-02-27 06:54:21,634][PyLogger][INFO]: Rank[1]: Epoch[147/500], iter[14200] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.51566
[2024-02-27 06:54:21,634][PyLogger][INFO]: Rank[0]: Epoch[147/500], iter[14200] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.51433
[2024-02-27 06:54:47,987][PyLogger][INFO]: Rank[0]: Epoch[147/500], iter[14250] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.52030
[2024-02-27 06:54:47,987][PyLogger][INFO]: Rank[1]: Epoch[147/500], iter[14250] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.54200
[2024-02-27 06:55:14,701][PyLogger][INFO]: Rank[1]: Epoch[148/500], iter[14300] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.53713
[2024-02-27 06:55:14,701][PyLogger][INFO]: Rank[0]: Epoch[148/500], iter[14300] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.53650
[2024-02-27 06:55:40,891][PyLogger][INFO]: Rank[1]: Epoch[148/500], iter[14350] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.52498
[2024-02-27 06:55:40,891][PyLogger][INFO]: Rank[0]: Epoch[148/500], iter[14350] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.51160
[2024-02-27 06:56:07,742][PyLogger][INFO]: Rank[0]: Epoch[149/500], iter[14400] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.53935
[2024-02-27 06:56:07,742][PyLogger][INFO]: Rank[1]: Epoch[149/500], iter[14400] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.54107
[2024-02-27 06:56:34,068][PyLogger][INFO]: Rank[1]: Epoch[149/500], iter[14450] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.54180
[2024-02-27 06:56:34,068][PyLogger][INFO]: Rank[0]: Epoch[149/500], iter[14450] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.51730
[2024-02-27 06:57:00,868][PyLogger][INFO]: Rank[0]: Epoch[150/500], iter[14500] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.53069
[2024-02-27 06:57:00,868][PyLogger][INFO]: Rank[1]: Epoch[150/500], iter[14500] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.54057
[2024-02-27 06:57:27,136][PyLogger][INFO]: Rank[1]: Epoch[150/500], iter[14550] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.53680
[2024-02-27 06:57:27,137][PyLogger][INFO]: Rank[0]: Epoch[150/500], iter[14550] - train_accuracy: 0.98659 - val_accuracy: 0.90390 - train_loss: 0.53142 - val_loss: 0.76032 - loss: 0.52688
[2024-02-27 06:58:21,046][PyLogger][INFO]: Rank[1]: Epoch[151/500], iter[14600] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.51273
[2024-02-27 06:58:21,046][PyLogger][INFO]: Rank[0]: Epoch[151/500], iter[14600] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.52906
[2024-02-27 06:58:47,738][PyLogger][INFO]: Rank[0]: Epoch[152/500], iter[14650] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.52485
[2024-02-27 06:58:47,738][PyLogger][INFO]: Rank[1]: Epoch[152/500], iter[14650] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.52485
[2024-02-27 06:59:13,959][PyLogger][INFO]: Rank[1]: Epoch[152/500], iter[14700] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.51686
[2024-02-27 06:59:13,959][PyLogger][INFO]: Rank[0]: Epoch[152/500], iter[14700] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.52259
[2024-02-27 06:59:40,595][PyLogger][INFO]: Rank[0]: Epoch[153/500], iter[14750] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.53283
[2024-02-27 06:59:40,595][PyLogger][INFO]: Rank[1]: Epoch[153/500], iter[14750] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.52045
[2024-02-27 07:00:06,798][PyLogger][INFO]: Rank[1]: Epoch[153/500], iter[14800] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.52566
[2024-02-27 07:00:06,798][PyLogger][INFO]: Rank[0]: Epoch[153/500], iter[14800] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.52437
[2024-02-27 07:00:33,382][PyLogger][INFO]: Rank[0]: Epoch[154/500], iter[14850] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.52053
[2024-02-27 07:00:33,382][PyLogger][INFO]: Rank[1]: Epoch[154/500], iter[14850] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.50562
[2024-02-27 07:00:59,599][PyLogger][INFO]: Rank[1]: Epoch[154/500], iter[14900] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.52171
[2024-02-27 07:00:59,599][PyLogger][INFO]: Rank[0]: Epoch[154/500], iter[14900] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.52962
[2024-02-27 07:01:26,446][PyLogger][INFO]: Rank[1]: Epoch[155/500], iter[14950] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.50961
[2024-02-27 07:01:26,447][PyLogger][INFO]: Rank[0]: Epoch[155/500], iter[14950] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.51746
[2024-02-27 07:01:52,640][PyLogger][INFO]: Rank[0]: Epoch[155/500], iter[15000] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.54124
[2024-02-27 07:01:52,640][PyLogger][INFO]: Rank[1]: Epoch[155/500], iter[15000] - train_accuracy: 0.98526 - val_accuracy: 0.90320 - train_loss: 0.53523 - val_loss: 0.75430 - loss: 0.52432
[2024-02-27 07:02:46,552][PyLogger][INFO]: Rank[1]: Epoch[156/500], iter[15050] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.51236
[2024-02-27 07:02:46,552][PyLogger][INFO]: Rank[0]: Epoch[156/500], iter[15050] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.52358
[2024-02-27 07:03:12,746][PyLogger][INFO]: Rank[0]: Epoch[156/500], iter[15100] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.51963
[2024-02-27 07:03:12,746][PyLogger][INFO]: Rank[1]: Epoch[156/500], iter[15100] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.50897
[2024-02-27 07:03:39,510][PyLogger][INFO]: Rank[1]: Epoch[157/500], iter[15150] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.52453
[2024-02-27 07:03:39,511][PyLogger][INFO]: Rank[0]: Epoch[157/500], iter[15150] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.53555
[2024-02-27 07:04:05,658][PyLogger][INFO]: Rank[0]: Epoch[157/500], iter[15200] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.52272
[2024-02-27 07:04:05,658][PyLogger][INFO]: Rank[1]: Epoch[157/500], iter[15200] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.51497
[2024-02-27 07:04:32,376][PyLogger][INFO]: Rank[0]: Epoch[158/500], iter[15250] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.51881
[2024-02-27 07:04:32,376][PyLogger][INFO]: Rank[1]: Epoch[158/500], iter[15250] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.51659
[2024-02-27 07:04:58,633][PyLogger][INFO]: Rank[1]: Epoch[158/500], iter[15300] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.54499
[2024-02-27 07:04:58,634][PyLogger][INFO]: Rank[0]: Epoch[158/500], iter[15300] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.52955
[2024-02-27 07:05:25,302][PyLogger][INFO]: Rank[0]: Epoch[159/500], iter[15350] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.52836
[2024-02-27 07:05:25,302][PyLogger][INFO]: Rank[1]: Epoch[159/500], iter[15350] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.50700
[2024-02-27 07:05:51,580][PyLogger][INFO]: Rank[1]: Epoch[159/500], iter[15400] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.51539
[2024-02-27 07:05:51,580][PyLogger][INFO]: Rank[0]: Epoch[159/500], iter[15400] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.51166
[2024-02-27 07:06:18,297][PyLogger][INFO]: Rank[1]: Epoch[160/500], iter[15450] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.52173
[2024-02-27 07:06:18,297][PyLogger][INFO]: Rank[0]: Epoch[160/500], iter[15450] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.52312
[2024-02-27 07:06:44,639][PyLogger][INFO]: Rank[0]: Epoch[160/500], iter[15500] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.56358
[2024-02-27 07:06:44,639][PyLogger][INFO]: Rank[1]: Epoch[160/500], iter[15500] - train_accuracy: 0.98607 - val_accuracy: 0.90500 - train_loss: 0.53285 - val_loss: 0.75604 - loss: 0.53523
[2024-02-27 07:07:38,667][PyLogger][INFO]: Rank[1]: Epoch[161/500], iter[15550] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.52544
[2024-02-27 07:07:38,667][PyLogger][INFO]: Rank[0]: Epoch[161/500], iter[15550] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.53976
[2024-02-27 07:08:04,926][PyLogger][INFO]: Rank[0]: Epoch[161/500], iter[15600] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.52743
[2024-02-27 07:08:04,926][PyLogger][INFO]: Rank[1]: Epoch[161/500], iter[15600] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.51778
[2024-02-27 07:08:31,567][PyLogger][INFO]: Rank[1]: Epoch[162/500], iter[15650] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.51210
[2024-02-27 07:08:31,568][PyLogger][INFO]: Rank[0]: Epoch[162/500], iter[15650] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.52858
[2024-02-27 07:08:57,776][PyLogger][INFO]: Rank[0]: Epoch[162/500], iter[15700] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.53539
[2024-02-27 07:08:57,776][PyLogger][INFO]: Rank[1]: Epoch[162/500], iter[15700] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.52368
[2024-02-27 07:09:24,494][PyLogger][INFO]: Rank[1]: Epoch[163/500], iter[15750] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.51167
[2024-02-27 07:09:24,494][PyLogger][INFO]: Rank[0]: Epoch[163/500], iter[15750] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.50498
[2024-02-27 07:09:50,517][PyLogger][INFO]: Rank[0]: Epoch[163/500], iter[15800] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.53390
[2024-02-27 07:09:50,517][PyLogger][INFO]: Rank[1]: Epoch[163/500], iter[15800] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.54996
[2024-02-27 07:10:17,327][PyLogger][INFO]: Rank[1]: Epoch[164/500], iter[15850] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.51663
[2024-02-27 07:10:17,327][PyLogger][INFO]: Rank[0]: Epoch[164/500], iter[15850] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.51716
[2024-02-27 07:10:43,602][PyLogger][INFO]: Rank[1]: Epoch[164/500], iter[15900] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.51852
[2024-02-27 07:10:43,602][PyLogger][INFO]: Rank[0]: Epoch[164/500], iter[15900] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.52063
[2024-02-27 07:11:10,358][PyLogger][INFO]: Rank[0]: Epoch[165/500], iter[15950] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.52911
[2024-02-27 07:11:10,358][PyLogger][INFO]: Rank[1]: Epoch[165/500], iter[15950] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.51343
[2024-02-27 07:11:36,666][PyLogger][INFO]: Rank[1]: Epoch[165/500], iter[16000] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.53266
[2024-02-27 07:11:36,666][PyLogger][INFO]: Rank[0]: Epoch[165/500], iter[16000] - train_accuracy: 0.98649 - val_accuracy: 0.90670 - train_loss: 0.53319 - val_loss: 0.75502 - loss: 0.51185
[2024-02-27 07:12:30,622][PyLogger][INFO]: Rank[1]: Epoch[166/500], iter[16050] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.52164
[2024-02-27 07:12:30,622][PyLogger][INFO]: Rank[0]: Epoch[166/500], iter[16050] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.50754
[2024-02-27 07:12:56,996][PyLogger][INFO]: Rank[0]: Epoch[166/500], iter[16100] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.55166
[2024-02-27 07:12:56,997][PyLogger][INFO]: Rank[1]: Epoch[166/500], iter[16100] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.51572
[2024-02-27 07:13:23,589][PyLogger][INFO]: Rank[1]: Epoch[167/500], iter[16150] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.52236
[2024-02-27 07:13:23,589][PyLogger][INFO]: Rank[0]: Epoch[167/500], iter[16150] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.53319
[2024-02-27 07:13:50,297][PyLogger][INFO]: Rank[0]: Epoch[168/500], iter[16200] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.53780
[2024-02-27 07:13:50,297][PyLogger][INFO]: Rank[1]: Epoch[168/500], iter[16200] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.51817
[2024-02-27 07:14:16,541][PyLogger][INFO]: Rank[1]: Epoch[168/500], iter[16250] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.51463
[2024-02-27 07:14:16,541][PyLogger][INFO]: Rank[0]: Epoch[168/500], iter[16250] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.52772
[2024-02-27 07:14:43,243][PyLogger][INFO]: Rank[1]: Epoch[169/500], iter[16300] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.52772
[2024-02-27 07:14:43,243][PyLogger][INFO]: Rank[0]: Epoch[169/500], iter[16300] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.51302
[2024-02-27 07:15:09,542][PyLogger][INFO]: Rank[0]: Epoch[169/500], iter[16350] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.51420
[2024-02-27 07:15:09,542][PyLogger][INFO]: Rank[1]: Epoch[169/500], iter[16350] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.51349
[2024-02-27 07:15:36,320][PyLogger][INFO]: Rank[1]: Epoch[170/500], iter[16400] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.51442
[2024-02-27 07:15:36,320][PyLogger][INFO]: Rank[0]: Epoch[170/500], iter[16400] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.51483
[2024-02-27 07:16:02,514][PyLogger][INFO]: Rank[1]: Epoch[170/500], iter[16450] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.53045
[2024-02-27 07:16:02,514][PyLogger][INFO]: Rank[0]: Epoch[170/500], iter[16450] - train_accuracy: 0.98981 - val_accuracy: 0.90560 - train_loss: 0.52456 - val_loss: 0.76029 - loss: 0.52015
[2024-02-27 07:16:56,487][PyLogger][INFO]: Rank[1]: Epoch[171/500], iter[16500] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.53630
[2024-02-27 07:16:56,487][PyLogger][INFO]: Rank[0]: Epoch[171/500], iter[16500] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.51805
[2024-02-27 07:17:22,716][PyLogger][INFO]: Rank[0]: Epoch[171/500], iter[16550] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.51599
[2024-02-27 07:17:22,716][PyLogger][INFO]: Rank[1]: Epoch[171/500], iter[16550] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.55026
[2024-02-27 07:17:49,423][PyLogger][INFO]: Rank[1]: Epoch[172/500], iter[16600] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.51725
[2024-02-27 07:17:49,423][PyLogger][INFO]: Rank[0]: Epoch[172/500], iter[16600] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.51122
[2024-02-27 07:18:15,656][PyLogger][INFO]: Rank[0]: Epoch[172/500], iter[16650] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.50809
[2024-02-27 07:18:15,656][PyLogger][INFO]: Rank[1]: Epoch[172/500], iter[16650] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.52142
[2024-02-27 07:18:42,356][PyLogger][INFO]: Rank[0]: Epoch[173/500], iter[16700] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.52246
[2024-02-27 07:18:42,356][PyLogger][INFO]: Rank[1]: Epoch[173/500], iter[16700] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.51596
[2024-02-27 07:19:08,581][PyLogger][INFO]: Rank[1]: Epoch[173/500], iter[16750] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.52111
[2024-02-27 07:19:08,581][PyLogger][INFO]: Rank[0]: Epoch[173/500], iter[16750] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.52599
[2024-02-27 07:19:35,231][PyLogger][INFO]: Rank[0]: Epoch[174/500], iter[16800] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.52029
[2024-02-27 07:19:35,231][PyLogger][INFO]: Rank[1]: Epoch[174/500], iter[16800] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.52221
[2024-02-27 07:20:01,459][PyLogger][INFO]: Rank[1]: Epoch[174/500], iter[16850] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.51505
[2024-02-27 07:20:01,459][PyLogger][INFO]: Rank[0]: Epoch[174/500], iter[16850] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.52603
[2024-02-27 07:20:28,209][PyLogger][INFO]: Rank[1]: Epoch[175/500], iter[16900] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.51884
[2024-02-27 07:20:28,209][PyLogger][INFO]: Rank[0]: Epoch[175/500], iter[16900] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.53176
[2024-02-27 07:20:54,359][PyLogger][INFO]: Rank[0]: Epoch[175/500], iter[16950] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.54194
[2024-02-27 07:20:54,359][PyLogger][INFO]: Rank[1]: Epoch[175/500], iter[16950] - train_accuracy: 0.99042 - val_accuracy: 0.91270 - train_loss: 0.52397 - val_loss: 0.74280 - loss: 0.54885
[2024-02-27 07:21:48,207][PyLogger][INFO]: Rank[1]: Epoch[176/500], iter[17000] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.53079
[2024-02-27 07:21:48,207][PyLogger][INFO]: Rank[0]: Epoch[176/500], iter[17000] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.51540
[2024-02-27 07:22:14,405][PyLogger][INFO]: Rank[1]: Epoch[176/500], iter[17050] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.52905
[2024-02-27 07:22:14,405][PyLogger][INFO]: Rank[0]: Epoch[176/500], iter[17050] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.53694
[2024-02-27 07:22:41,027][PyLogger][INFO]: Rank[1]: Epoch[177/500], iter[17100] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.52163
[2024-02-27 07:22:41,027][PyLogger][INFO]: Rank[0]: Epoch[177/500], iter[17100] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.51539
[2024-02-27 07:23:07,137][PyLogger][INFO]: Rank[1]: Epoch[177/500], iter[17150] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.52253
[2024-02-27 07:23:07,137][PyLogger][INFO]: Rank[0]: Epoch[177/500], iter[17150] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.51108
[2024-02-27 07:23:33,771][PyLogger][INFO]: Rank[0]: Epoch[178/500], iter[17200] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.52646
[2024-02-27 07:23:33,771][PyLogger][INFO]: Rank[1]: Epoch[178/500], iter[17200] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.50956
[2024-02-27 07:24:00,130][PyLogger][INFO]: Rank[1]: Epoch[178/500], iter[17250] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.52961
[2024-02-27 07:24:00,130][PyLogger][INFO]: Rank[0]: Epoch[178/500], iter[17250] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.52526
[2024-02-27 07:24:26,802][PyLogger][INFO]: Rank[1]: Epoch[179/500], iter[17300] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.51459
[2024-02-27 07:24:26,802][PyLogger][INFO]: Rank[0]: Epoch[179/500], iter[17300] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.51589
[2024-02-27 07:24:53,029][PyLogger][INFO]: Rank[1]: Epoch[179/500], iter[17350] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.53002
[2024-02-27 07:24:53,029][PyLogger][INFO]: Rank[0]: Epoch[179/500], iter[17350] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.52422
[2024-02-27 07:25:19,730][PyLogger][INFO]: Rank[1]: Epoch[180/500], iter[17400] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.51296
[2024-02-27 07:25:19,730][PyLogger][INFO]: Rank[0]: Epoch[180/500], iter[17400] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.51505
[2024-02-27 07:25:45,964][PyLogger][INFO]: Rank[1]: Epoch[180/500], iter[17450] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.53474
[2024-02-27 07:25:45,964][PyLogger][INFO]: Rank[0]: Epoch[180/500], iter[17450] - train_accuracy: 0.98721 - val_accuracy: 0.90650 - train_loss: 0.53026 - val_loss: 0.75082 - loss: 0.50672
[2024-02-27 07:26:39,885][PyLogger][INFO]: Rank[1]: Epoch[181/500], iter[17500] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.54084
[2024-02-27 07:26:39,885][PyLogger][INFO]: Rank[0]: Epoch[181/500], iter[17500] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.53330
[2024-02-27 07:27:06,168][PyLogger][INFO]: Rank[1]: Epoch[181/500], iter[17550] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.51483
[2024-02-27 07:27:06,168][PyLogger][INFO]: Rank[0]: Epoch[181/500], iter[17550] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.53985
[2024-02-27 07:27:32,810][PyLogger][INFO]: Rank[0]: Epoch[182/500], iter[17600] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.51544
[2024-02-27 07:27:32,810][PyLogger][INFO]: Rank[1]: Epoch[182/500], iter[17600] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.52375
[2024-02-27 07:27:59,054][PyLogger][INFO]: Rank[1]: Epoch[182/500], iter[17650] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.51087
[2024-02-27 07:27:59,054][PyLogger][INFO]: Rank[0]: Epoch[182/500], iter[17650] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.51624
[2024-02-27 07:28:25,961][PyLogger][INFO]: Rank[1]: Epoch[183/500], iter[17700] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.54033
[2024-02-27 07:28:25,961][PyLogger][INFO]: Rank[0]: Epoch[183/500], iter[17700] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.52507
[2024-02-27 07:28:52,216][PyLogger][INFO]: Rank[0]: Epoch[183/500], iter[17750] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.52012
[2024-02-27 07:28:52,216][PyLogger][INFO]: Rank[1]: Epoch[183/500], iter[17750] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.51510
[2024-02-27 07:29:18,880][PyLogger][INFO]: Rank[1]: Epoch[184/500], iter[17800] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.51104
[2024-02-27 07:29:18,881][PyLogger][INFO]: Rank[0]: Epoch[184/500], iter[17800] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.51486
[2024-02-27 07:29:45,717][PyLogger][INFO]: Rank[0]: Epoch[185/500], iter[17850] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.54381
[2024-02-27 07:29:45,717][PyLogger][INFO]: Rank[1]: Epoch[185/500], iter[17850] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.50629
[2024-02-27 07:30:11,868][PyLogger][INFO]: Rank[0]: Epoch[185/500], iter[17900] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.51044
[2024-02-27 07:30:11,868][PyLogger][INFO]: Rank[1]: Epoch[185/500], iter[17900] - train_accuracy: 0.98454 - val_accuracy: 0.90470 - train_loss: 0.53501 - val_loss: 0.75326 - loss: 0.51583
[2024-02-27 07:31:05,968][PyLogger][INFO]: Rank[1]: Epoch[186/500], iter[17950] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.50883
[2024-02-27 07:31:05,968][PyLogger][INFO]: Rank[0]: Epoch[186/500], iter[17950] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.51079
[2024-02-27 07:31:32,154][PyLogger][INFO]: Rank[1]: Epoch[186/500], iter[18000] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.51966
[2024-02-27 07:31:32,154][PyLogger][INFO]: Rank[0]: Epoch[186/500], iter[18000] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.52289
[2024-02-27 07:31:58,948][PyLogger][INFO]: Rank[1]: Epoch[187/500], iter[18050] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.50598
[2024-02-27 07:31:58,948][PyLogger][INFO]: Rank[0]: Epoch[187/500], iter[18050] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.52807
[2024-02-27 07:32:25,066][PyLogger][INFO]: Rank[0]: Epoch[187/500], iter[18100] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.52935
[2024-02-27 07:32:25,066][PyLogger][INFO]: Rank[1]: Epoch[187/500], iter[18100] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.50511
[2024-02-27 07:32:51,891][PyLogger][INFO]: Rank[1]: Epoch[188/500], iter[18150] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.51590
[2024-02-27 07:32:51,891][PyLogger][INFO]: Rank[0]: Epoch[188/500], iter[18150] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.53681
[2024-02-27 07:33:18,090][PyLogger][INFO]: Rank[0]: Epoch[188/500], iter[18200] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.51936
[2024-02-27 07:33:18,090][PyLogger][INFO]: Rank[1]: Epoch[188/500], iter[18200] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.52450
[2024-02-27 07:33:44,772][PyLogger][INFO]: Rank[1]: Epoch[189/500], iter[18250] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.50862
[2024-02-27 07:33:44,772][PyLogger][INFO]: Rank[0]: Epoch[189/500], iter[18250] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.53770
[2024-02-27 07:34:10,918][PyLogger][INFO]: Rank[1]: Epoch[189/500], iter[18300] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.54899
[2024-02-27 07:34:10,918][PyLogger][INFO]: Rank[0]: Epoch[189/500], iter[18300] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.54389
[2024-02-27 07:34:37,642][PyLogger][INFO]: Rank[1]: Epoch[190/500], iter[18350] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.52056
[2024-02-27 07:34:37,642][PyLogger][INFO]: Rank[0]: Epoch[190/500], iter[18350] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.51732
[2024-02-27 07:35:03,855][PyLogger][INFO]: Rank[0]: Epoch[190/500], iter[18400] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.51620
[2024-02-27 07:35:03,855][PyLogger][INFO]: Rank[1]: Epoch[190/500], iter[18400] - train_accuracy: 0.98307 - val_accuracy: 0.90010 - train_loss: 0.54012 - val_loss: 0.76853 - loss: 0.50947
[2024-02-27 07:35:58,023][PyLogger][INFO]: Rank[0]: Epoch[191/500], iter[18450] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.50796
[2024-02-27 07:35:58,023][PyLogger][INFO]: Rank[1]: Epoch[191/500], iter[18450] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.52611
[2024-02-27 07:36:24,190][PyLogger][INFO]: Rank[0]: Epoch[191/500], iter[18500] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.50852
[2024-02-27 07:36:24,190][PyLogger][INFO]: Rank[1]: Epoch[191/500], iter[18500] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.51324
[2024-02-27 07:36:50,906][PyLogger][INFO]: Rank[1]: Epoch[192/500], iter[18550] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.51571
[2024-02-27 07:36:50,906][PyLogger][INFO]: Rank[0]: Epoch[192/500], iter[18550] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.52765
[2024-02-27 07:37:17,002][PyLogger][INFO]: Rank[1]: Epoch[192/500], iter[18600] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.53617
[2024-02-27 07:37:17,002][PyLogger][INFO]: Rank[0]: Epoch[192/500], iter[18600] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.52057
[2024-02-27 07:37:43,638][PyLogger][INFO]: Rank[1]: Epoch[193/500], iter[18650] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.51441
[2024-02-27 07:37:43,638][PyLogger][INFO]: Rank[0]: Epoch[193/500], iter[18650] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.54318
[2024-02-27 07:38:09,974][PyLogger][INFO]: Rank[0]: Epoch[193/500], iter[18700] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.51376
[2024-02-27 07:38:09,974][PyLogger][INFO]: Rank[1]: Epoch[193/500], iter[18700] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.50700
[2024-02-27 07:38:36,513][PyLogger][INFO]: Rank[1]: Epoch[194/500], iter[18750] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.52252
[2024-02-27 07:38:36,513][PyLogger][INFO]: Rank[0]: Epoch[194/500], iter[18750] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.52509
[2024-02-27 07:39:02,760][PyLogger][INFO]: Rank[1]: Epoch[194/500], iter[18800] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.51531
[2024-02-27 07:39:02,760][PyLogger][INFO]: Rank[0]: Epoch[194/500], iter[18800] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.51011
[2024-02-27 07:39:29,422][PyLogger][INFO]: Rank[0]: Epoch[195/500], iter[18850] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.51330
[2024-02-27 07:39:29,422][PyLogger][INFO]: Rank[1]: Epoch[195/500], iter[18850] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.51611
[2024-02-27 07:39:55,729][PyLogger][INFO]: Rank[1]: Epoch[195/500], iter[18900] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.52619
[2024-02-27 07:39:55,729][PyLogger][INFO]: Rank[0]: Epoch[195/500], iter[18900] - train_accuracy: 0.99064 - val_accuracy: 0.91050 - train_loss: 0.52289 - val_loss: 0.74693 - loss: 0.53459
[2024-02-27 07:40:49,549][PyLogger][INFO]: Rank[0]: Epoch[196/500], iter[18950] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.54695
[2024-02-27 07:40:49,549][PyLogger][INFO]: Rank[1]: Epoch[196/500], iter[18950] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.51736
[2024-02-27 07:41:15,710][PyLogger][INFO]: Rank[1]: Epoch[196/500], iter[19000] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.54455
[2024-02-27 07:41:15,710][PyLogger][INFO]: Rank[0]: Epoch[196/500], iter[19000] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.52899
[2024-02-27 07:41:42,353][PyLogger][INFO]: Rank[0]: Epoch[197/500], iter[19050] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.51119
[2024-02-27 07:41:42,353][PyLogger][INFO]: Rank[1]: Epoch[197/500], iter[19050] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.52160
[2024-02-27 07:42:08,538][PyLogger][INFO]: Rank[1]: Epoch[197/500], iter[19100] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.53413
[2024-02-27 07:42:08,538][PyLogger][INFO]: Rank[0]: Epoch[197/500], iter[19100] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.51261
[2024-02-27 07:42:35,242][PyLogger][INFO]: Rank[0]: Epoch[198/500], iter[19150] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.51536
[2024-02-27 07:42:35,242][PyLogger][INFO]: Rank[1]: Epoch[198/500], iter[19150] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.53315
[2024-02-27 07:43:01,523][PyLogger][INFO]: Rank[1]: Epoch[198/500], iter[19200] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.51269
[2024-02-27 07:43:01,523][PyLogger][INFO]: Rank[0]: Epoch[198/500], iter[19200] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.53455
[2024-02-27 07:43:28,302][PyLogger][INFO]: Rank[1]: Epoch[199/500], iter[19250] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.52076
[2024-02-27 07:43:28,302][PyLogger][INFO]: Rank[0]: Epoch[199/500], iter[19250] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.54545
[2024-02-27 07:43:54,551][PyLogger][INFO]: Rank[1]: Epoch[199/500], iter[19300] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.51649
[2024-02-27 07:43:54,552][PyLogger][INFO]: Rank[0]: Epoch[199/500], iter[19300] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.51022
[2024-02-27 07:44:21,182][PyLogger][INFO]: Rank[1]: Epoch[200/500], iter[19350] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.53002
[2024-02-27 07:44:21,182][PyLogger][INFO]: Rank[0]: Epoch[200/500], iter[19350] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.53317
[2024-02-27 07:44:47,361][PyLogger][INFO]: Rank[0]: Epoch[200/500], iter[19400] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.54410
[2024-02-27 07:44:47,361][PyLogger][INFO]: Rank[1]: Epoch[200/500], iter[19400] - train_accuracy: 0.99313 - val_accuracy: 0.91260 - train_loss: 0.51757 - val_loss: 0.73713 - loss: 0.53365
[2024-02-27 07:45:41,440][PyLogger][INFO]: Rank[1]: Epoch[201/500], iter[19450] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.51708
[2024-02-27 07:45:41,441][PyLogger][INFO]: Rank[0]: Epoch[201/500], iter[19450] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.52369
[2024-02-27 07:46:08,165][PyLogger][INFO]: Rank[0]: Epoch[202/500], iter[19500] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.51412
[2024-02-27 07:46:08,165][PyLogger][INFO]: Rank[1]: Epoch[202/500], iter[19500] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.53340
[2024-02-27 07:46:34,262][PyLogger][INFO]: Rank[1]: Epoch[202/500], iter[19550] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.51430
[2024-02-27 07:46:34,263][PyLogger][INFO]: Rank[0]: Epoch[202/500], iter[19550] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.52467
[2024-02-27 07:47:01,146][PyLogger][INFO]: Rank[0]: Epoch[203/500], iter[19600] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.51791
[2024-02-27 07:47:01,146][PyLogger][INFO]: Rank[1]: Epoch[203/500], iter[19600] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.53616
[2024-02-27 07:47:27,283][PyLogger][INFO]: Rank[1]: Epoch[203/500], iter[19650] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.50763
[2024-02-27 07:47:27,284][PyLogger][INFO]: Rank[0]: Epoch[203/500], iter[19650] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.55056
[2024-02-27 07:47:54,314][PyLogger][INFO]: Rank[1]: Epoch[204/500], iter[19700] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.50965
[2024-02-27 07:47:54,314][PyLogger][INFO]: Rank[0]: Epoch[204/500], iter[19700] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.50562
[2024-02-27 07:48:20,569][PyLogger][INFO]: Rank[1]: Epoch[204/500], iter[19750] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.50967
[2024-02-27 07:48:20,569][PyLogger][INFO]: Rank[0]: Epoch[204/500], iter[19750] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.50836
[2024-02-27 07:48:47,338][PyLogger][INFO]: Rank[1]: Epoch[205/500], iter[19800] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.50901
[2024-02-27 07:48:47,338][PyLogger][INFO]: Rank[0]: Epoch[205/500], iter[19800] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.51132
[2024-02-27 07:49:13,576][PyLogger][INFO]: Rank[0]: Epoch[205/500], iter[19850] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.50760
[2024-02-27 07:49:13,576][PyLogger][INFO]: Rank[1]: Epoch[205/500], iter[19850] - train_accuracy: 0.98647 - val_accuracy: 0.90830 - train_loss: 0.53246 - val_loss: 0.75245 - loss: 0.51474
[2024-02-27 07:50:07,542][PyLogger][INFO]: Rank[0]: Epoch[206/500], iter[19900] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.51553
[2024-02-27 07:50:07,542][PyLogger][INFO]: Rank[1]: Epoch[206/500], iter[19900] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.51035
[2024-02-27 07:50:33,728][PyLogger][INFO]: Rank[1]: Epoch[206/500], iter[19950] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.53092
[2024-02-27 07:50:33,728][PyLogger][INFO]: Rank[0]: Epoch[206/500], iter[19950] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.50859
[2024-02-27 07:51:00,414][PyLogger][INFO]: Rank[1]: Epoch[207/500], iter[20000] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.51260
[2024-02-27 07:51:00,414][PyLogger][INFO]: Rank[0]: Epoch[207/500], iter[20000] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.51383
[2024-02-27 07:51:26,622][PyLogger][INFO]: Rank[1]: Epoch[207/500], iter[20050] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.50511
[2024-02-27 07:51:26,622][PyLogger][INFO]: Rank[0]: Epoch[207/500], iter[20050] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.51070
[2024-02-27 07:51:53,233][PyLogger][INFO]: Rank[1]: Epoch[208/500], iter[20100] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.53616
[2024-02-27 07:51:53,233][PyLogger][INFO]: Rank[0]: Epoch[208/500], iter[20100] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.51429
[2024-02-27 07:52:19,346][PyLogger][INFO]: Rank[1]: Epoch[208/500], iter[20150] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.53761
[2024-02-27 07:52:19,346][PyLogger][INFO]: Rank[0]: Epoch[208/500], iter[20150] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.53229
[2024-02-27 07:52:46,109][PyLogger][INFO]: Rank[0]: Epoch[209/500], iter[20200] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.51994
[2024-02-27 07:52:46,109][PyLogger][INFO]: Rank[1]: Epoch[209/500], iter[20200] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.50790
[2024-02-27 07:53:12,262][PyLogger][INFO]: Rank[1]: Epoch[209/500], iter[20250] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.53352
[2024-02-27 07:53:12,262][PyLogger][INFO]: Rank[0]: Epoch[209/500], iter[20250] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.51177
[2024-02-27 07:53:38,970][PyLogger][INFO]: Rank[1]: Epoch[210/500], iter[20300] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.55232
[2024-02-27 07:53:38,971][PyLogger][INFO]: Rank[0]: Epoch[210/500], iter[20300] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.53339
[2024-02-27 07:54:05,183][PyLogger][INFO]: Rank[0]: Epoch[210/500], iter[20350] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.52573
[2024-02-27 07:54:05,183][PyLogger][INFO]: Rank[1]: Epoch[210/500], iter[20350] - train_accuracy: 0.98299 - val_accuracy: 0.89730 - train_loss: 0.53915 - val_loss: 0.78762 - loss: 0.51720
[2024-02-27 07:54:59,084][PyLogger][INFO]: Rank[1]: Epoch[211/500], iter[20400] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.51306
[2024-02-27 07:54:59,084][PyLogger][INFO]: Rank[0]: Epoch[211/500], iter[20400] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.52167
[2024-02-27 07:55:25,346][PyLogger][INFO]: Rank[1]: Epoch[211/500], iter[20450] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.51005
[2024-02-27 07:55:25,346][PyLogger][INFO]: Rank[0]: Epoch[211/500], iter[20450] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.51284
[2024-02-27 07:55:52,077][PyLogger][INFO]: Rank[1]: Epoch[212/500], iter[20500] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.51547
[2024-02-27 07:55:52,077][PyLogger][INFO]: Rank[0]: Epoch[212/500], iter[20500] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.52058
[2024-02-27 07:56:18,356][PyLogger][INFO]: Rank[0]: Epoch[212/500], iter[20550] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.55570
[2024-02-27 07:56:18,356][PyLogger][INFO]: Rank[1]: Epoch[212/500], iter[20550] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.51132
[2024-02-27 07:56:45,014][PyLogger][INFO]: Rank[1]: Epoch[213/500], iter[20600] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.50274
[2024-02-27 07:56:45,014][PyLogger][INFO]: Rank[0]: Epoch[213/500], iter[20600] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.50663
[2024-02-27 07:57:11,265][PyLogger][INFO]: Rank[1]: Epoch[213/500], iter[20650] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.51734
[2024-02-27 07:57:11,265][PyLogger][INFO]: Rank[0]: Epoch[213/500], iter[20650] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.52497
[2024-02-27 07:57:38,104][PyLogger][INFO]: Rank[0]: Epoch[214/500], iter[20700] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.50592
[2024-02-27 07:57:38,104][PyLogger][INFO]: Rank[1]: Epoch[214/500], iter[20700] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.52891
[2024-02-27 07:58:04,326][PyLogger][INFO]: Rank[1]: Epoch[214/500], iter[20750] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.51307
[2024-02-27 07:58:04,326][PyLogger][INFO]: Rank[0]: Epoch[214/500], iter[20750] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.51236
[2024-02-27 07:58:31,134][PyLogger][INFO]: Rank[0]: Epoch[215/500], iter[20800] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.50300
[2024-02-27 07:58:31,134][PyLogger][INFO]: Rank[1]: Epoch[215/500], iter[20800] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.52595
[2024-02-27 07:58:57,244][PyLogger][INFO]: Rank[1]: Epoch[215/500], iter[20850] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.51527
[2024-02-27 07:58:57,244][PyLogger][INFO]: Rank[0]: Epoch[215/500], iter[20850] - train_accuracy: 0.98756 - val_accuracy: 0.90920 - train_loss: 0.52833 - val_loss: 0.75369 - loss: 0.53319
[2024-02-27 07:59:51,234][PyLogger][INFO]: Rank[1]: Epoch[216/500], iter[20900] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.52500
[2024-02-27 07:59:51,234][PyLogger][INFO]: Rank[0]: Epoch[216/500], iter[20900] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.51843
[2024-02-27 08:00:17,396][PyLogger][INFO]: Rank[1]: Epoch[216/500], iter[20950] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.52885
[2024-02-27 08:00:17,396][PyLogger][INFO]: Rank[0]: Epoch[216/500], iter[20950] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.53651
[2024-02-27 08:00:44,158][PyLogger][INFO]: Rank[1]: Epoch[217/500], iter[21000] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.51719
[2024-02-27 08:00:44,158][PyLogger][INFO]: Rank[0]: Epoch[217/500], iter[21000] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.51116
[2024-02-27 08:01:10,920][PyLogger][INFO]: Rank[0]: Epoch[218/500], iter[21050] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.51591
[2024-02-27 08:01:10,920][PyLogger][INFO]: Rank[1]: Epoch[218/500], iter[21050] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.52357
[2024-02-27 08:01:37,127][PyLogger][INFO]: Rank[1]: Epoch[218/500], iter[21100] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.51142
[2024-02-27 08:01:37,127][PyLogger][INFO]: Rank[0]: Epoch[218/500], iter[21100] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.51063
[2024-02-27 08:02:03,877][PyLogger][INFO]: Rank[1]: Epoch[219/500], iter[21150] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.52355
[2024-02-27 08:02:03,877][PyLogger][INFO]: Rank[0]: Epoch[219/500], iter[21150] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.51533
[2024-02-27 08:02:30,037][PyLogger][INFO]: Rank[1]: Epoch[219/500], iter[21200] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.52434
[2024-02-27 08:02:30,037][PyLogger][INFO]: Rank[0]: Epoch[219/500], iter[21200] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.50911
[2024-02-27 08:02:56,797][PyLogger][INFO]: Rank[1]: Epoch[220/500], iter[21250] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.50621
[2024-02-27 08:02:56,797][PyLogger][INFO]: Rank[0]: Epoch[220/500], iter[21250] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.50793
[2024-02-27 08:03:23,047][PyLogger][INFO]: Rank[0]: Epoch[220/500], iter[21300] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.51145
[2024-02-27 08:03:23,047][PyLogger][INFO]: Rank[1]: Epoch[220/500], iter[21300] - train_accuracy: 0.98899 - val_accuracy: 0.90580 - train_loss: 0.52759 - val_loss: 0.77129 - loss: 0.50277
[2024-02-27 08:04:17,004][PyLogger][INFO]: Rank[1]: Epoch[221/500], iter[21350] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.52693
[2024-02-27 08:04:17,005][PyLogger][INFO]: Rank[0]: Epoch[221/500], iter[21350] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51622
[2024-02-27 08:04:43,234][PyLogger][INFO]: Rank[0]: Epoch[221/500], iter[21400] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.50736
[2024-02-27 08:04:43,234][PyLogger][INFO]: Rank[1]: Epoch[221/500], iter[21400] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51154
[2024-02-27 08:05:09,871][PyLogger][INFO]: Rank[1]: Epoch[222/500], iter[21450] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51652
[2024-02-27 08:05:09,871][PyLogger][INFO]: Rank[0]: Epoch[222/500], iter[21450] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51896
[2024-02-27 08:05:35,958][PyLogger][INFO]: Rank[0]: Epoch[222/500], iter[21500] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51828
[2024-02-27 08:05:35,958][PyLogger][INFO]: Rank[1]: Epoch[222/500], iter[21500] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.50707
[2024-02-27 08:06:02,602][PyLogger][INFO]: Rank[1]: Epoch[223/500], iter[21550] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51119
[2024-02-27 08:06:02,602][PyLogger][INFO]: Rank[0]: Epoch[223/500], iter[21550] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51160
[2024-02-27 08:06:28,775][PyLogger][INFO]: Rank[1]: Epoch[223/500], iter[21600] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51042
[2024-02-27 08:06:28,775][PyLogger][INFO]: Rank[0]: Epoch[223/500], iter[21600] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51730
[2024-02-27 08:06:55,514][PyLogger][INFO]: Rank[1]: Epoch[224/500], iter[21650] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51184
[2024-02-27 08:06:55,514][PyLogger][INFO]: Rank[0]: Epoch[224/500], iter[21650] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.52545
[2024-02-27 08:07:21,804][PyLogger][INFO]: Rank[1]: Epoch[224/500], iter[21700] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51834
[2024-02-27 08:07:21,804][PyLogger][INFO]: Rank[0]: Epoch[224/500], iter[21700] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51233
[2024-02-27 08:07:48,460][PyLogger][INFO]: Rank[1]: Epoch[225/500], iter[21750] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51772
[2024-02-27 08:07:48,460][PyLogger][INFO]: Rank[0]: Epoch[225/500], iter[21750] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.51355
[2024-02-27 08:08:14,605][PyLogger][INFO]: Rank[1]: Epoch[225/500], iter[21800] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.52231
[2024-02-27 08:08:14,605][PyLogger][INFO]: Rank[0]: Epoch[225/500], iter[21800] - train_accuracy: 0.99452 - val_accuracy: 0.91300 - train_loss: 0.51452 - val_loss: 0.73761 - loss: 0.52706
[2024-02-27 08:09:08,312][PyLogger][INFO]: Rank[1]: Epoch[226/500], iter[21850] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.50368
[2024-02-27 08:09:08,312][PyLogger][INFO]: Rank[0]: Epoch[226/500], iter[21850] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.50696
[2024-02-27 08:09:34,510][PyLogger][INFO]: Rank[1]: Epoch[226/500], iter[21900] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.51159
[2024-02-27 08:09:34,510][PyLogger][INFO]: Rank[0]: Epoch[226/500], iter[21900] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.51577
[2024-02-27 08:10:01,085][PyLogger][INFO]: Rank[0]: Epoch[227/500], iter[21950] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.51430
[2024-02-27 08:10:01,085][PyLogger][INFO]: Rank[1]: Epoch[227/500], iter[21950] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.50445
[2024-02-27 08:10:27,351][PyLogger][INFO]: Rank[1]: Epoch[227/500], iter[22000] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.51580
[2024-02-27 08:10:27,351][PyLogger][INFO]: Rank[0]: Epoch[227/500], iter[22000] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.51981
[2024-02-27 08:10:53,989][PyLogger][INFO]: Rank[0]: Epoch[228/500], iter[22050] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.55351
[2024-02-27 08:10:53,989][PyLogger][INFO]: Rank[1]: Epoch[228/500], iter[22050] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.53046
[2024-02-27 08:11:20,196][PyLogger][INFO]: Rank[1]: Epoch[228/500], iter[22100] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.52179
[2024-02-27 08:11:20,196][PyLogger][INFO]: Rank[0]: Epoch[228/500], iter[22100] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.50668
[2024-02-27 08:11:46,825][PyLogger][INFO]: Rank[1]: Epoch[229/500], iter[22150] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.50481
[2024-02-27 08:11:46,825][PyLogger][INFO]: Rank[0]: Epoch[229/500], iter[22150] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.53800
[2024-02-27 08:12:13,143][PyLogger][INFO]: Rank[1]: Epoch[229/500], iter[22200] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.50714
[2024-02-27 08:12:13,143][PyLogger][INFO]: Rank[0]: Epoch[229/500], iter[22200] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.55273
[2024-02-27 08:12:39,842][PyLogger][INFO]: Rank[1]: Epoch[230/500], iter[22250] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.51415
[2024-02-27 08:12:39,842][PyLogger][INFO]: Rank[0]: Epoch[230/500], iter[22250] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.51667
[2024-02-27 08:13:06,091][PyLogger][INFO]: Rank[1]: Epoch[230/500], iter[22300] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.50503
[2024-02-27 08:13:06,091][PyLogger][INFO]: Rank[0]: Epoch[230/500], iter[22300] - train_accuracy: 0.98778 - val_accuracy: 0.90880 - train_loss: 0.52975 - val_loss: 0.76513 - loss: 0.50573
[2024-02-27 08:14:00,214][PyLogger][INFO]: Rank[0]: Epoch[231/500], iter[22350] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.50520
[2024-02-27 08:14:00,214][PyLogger][INFO]: Rank[1]: Epoch[231/500], iter[22350] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.50564
[2024-02-27 08:14:26,435][PyLogger][INFO]: Rank[1]: Epoch[231/500], iter[22400] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.51393
[2024-02-27 08:14:26,435][PyLogger][INFO]: Rank[0]: Epoch[231/500], iter[22400] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.52826
[2024-02-27 08:14:53,116][PyLogger][INFO]: Rank[1]: Epoch[232/500], iter[22450] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.50457
[2024-02-27 08:14:53,116][PyLogger][INFO]: Rank[0]: Epoch[232/500], iter[22450] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.52140
[2024-02-27 08:15:19,304][PyLogger][INFO]: Rank[1]: Epoch[232/500], iter[22500] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.50852
[2024-02-27 08:15:19,304][PyLogger][INFO]: Rank[0]: Epoch[232/500], iter[22500] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.51635
[2024-02-27 08:15:46,086][PyLogger][INFO]: Rank[1]: Epoch[233/500], iter[22550] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.51059
[2024-02-27 08:15:46,086][PyLogger][INFO]: Rank[0]: Epoch[233/500], iter[22550] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.52998
[2024-02-27 08:16:12,392][PyLogger][INFO]: Rank[0]: Epoch[233/500], iter[22600] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.51117
[2024-02-27 08:16:12,392][PyLogger][INFO]: Rank[1]: Epoch[233/500], iter[22600] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.51553
[2024-02-27 08:16:39,179][PyLogger][INFO]: Rank[1]: Epoch[234/500], iter[22650] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.50748
[2024-02-27 08:16:39,179][PyLogger][INFO]: Rank[0]: Epoch[234/500], iter[22650] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.52072
[2024-02-27 08:17:05,944][PyLogger][INFO]: Rank[1]: Epoch[235/500], iter[22700] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.50730
[2024-02-27 08:17:05,944][PyLogger][INFO]: Rank[0]: Epoch[235/500], iter[22700] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.50823
[2024-02-27 08:17:32,207][PyLogger][INFO]: Rank[0]: Epoch[235/500], iter[22750] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.51102
[2024-02-27 08:17:32,207][PyLogger][INFO]: Rank[1]: Epoch[235/500], iter[22750] - train_accuracy: 0.99517 - val_accuracy: 0.91780 - train_loss: 0.51262 - val_loss: 0.72597 - loss: 0.51302
[2024-02-27 08:18:26,326][PyLogger][INFO]: Rank[1]: Epoch[236/500], iter[22800] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.50958
[2024-02-27 08:18:26,326][PyLogger][INFO]: Rank[0]: Epoch[236/500], iter[22800] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.50733
[2024-02-27 08:18:52,703][PyLogger][INFO]: Rank[0]: Epoch[236/500], iter[22850] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.51611
[2024-02-27 08:18:52,703][PyLogger][INFO]: Rank[1]: Epoch[236/500], iter[22850] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.51071
[2024-02-27 08:19:19,400][PyLogger][INFO]: Rank[0]: Epoch[237/500], iter[22900] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.51183
[2024-02-27 08:19:19,400][PyLogger][INFO]: Rank[1]: Epoch[237/500], iter[22900] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.51576
[2024-02-27 08:19:45,636][PyLogger][INFO]: Rank[1]: Epoch[237/500], iter[22950] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.51279
[2024-02-27 08:19:45,636][PyLogger][INFO]: Rank[0]: Epoch[237/500], iter[22950] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.50960
[2024-02-27 08:20:12,315][PyLogger][INFO]: Rank[0]: Epoch[238/500], iter[23000] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.50499
[2024-02-27 08:20:12,315][PyLogger][INFO]: Rank[1]: Epoch[238/500], iter[23000] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.50377
[2024-02-27 08:20:38,507][PyLogger][INFO]: Rank[0]: Epoch[238/500], iter[23050] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.51680
[2024-02-27 08:20:38,507][PyLogger][INFO]: Rank[1]: Epoch[238/500], iter[23050] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.51648
[2024-02-27 08:21:05,202][PyLogger][INFO]: Rank[1]: Epoch[239/500], iter[23100] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.50915
[2024-02-27 08:21:05,202][PyLogger][INFO]: Rank[0]: Epoch[239/500], iter[23100] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.50472
[2024-02-27 08:21:31,418][PyLogger][INFO]: Rank[1]: Epoch[239/500], iter[23150] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.51179
[2024-02-27 08:21:31,418][PyLogger][INFO]: Rank[0]: Epoch[239/500], iter[23150] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.53392
[2024-02-27 08:21:58,127][PyLogger][INFO]: Rank[0]: Epoch[240/500], iter[23200] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.51162
[2024-02-27 08:21:58,127][PyLogger][INFO]: Rank[1]: Epoch[240/500], iter[23200] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.51260
[2024-02-27 08:22:24,367][PyLogger][INFO]: Rank[0]: Epoch[240/500], iter[23250] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.50932
[2024-02-27 08:22:24,367][PyLogger][INFO]: Rank[1]: Epoch[240/500], iter[23250] - train_accuracy: 0.99499 - val_accuracy: 0.91670 - train_loss: 0.51265 - val_loss: 0.73240 - loss: 0.50253
[2024-02-27 08:23:18,312][PyLogger][INFO]: Rank[1]: Epoch[241/500], iter[23300] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.50731
[2024-02-27 08:23:18,312][PyLogger][INFO]: Rank[0]: Epoch[241/500], iter[23300] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.51151
[2024-02-27 08:23:44,469][PyLogger][INFO]: Rank[1]: Epoch[241/500], iter[23350] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.53226
[2024-02-27 08:23:44,469][PyLogger][INFO]: Rank[0]: Epoch[241/500], iter[23350] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.53217
[2024-02-27 08:24:11,135][PyLogger][INFO]: Rank[0]: Epoch[242/500], iter[23400] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.51451
[2024-02-27 08:24:11,135][PyLogger][INFO]: Rank[1]: Epoch[242/500], iter[23400] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.50594
[2024-02-27 08:24:37,511][PyLogger][INFO]: Rank[1]: Epoch[242/500], iter[23450] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.51349
[2024-02-27 08:24:37,511][PyLogger][INFO]: Rank[0]: Epoch[242/500], iter[23450] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.52084
[2024-02-27 08:25:04,176][PyLogger][INFO]: Rank[0]: Epoch[243/500], iter[23500] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.51676
[2024-02-27 08:25:04,176][PyLogger][INFO]: Rank[1]: Epoch[243/500], iter[23500] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.51446
[2024-02-27 08:25:30,360][PyLogger][INFO]: Rank[1]: Epoch[243/500], iter[23550] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.50879
[2024-02-27 08:25:30,360][PyLogger][INFO]: Rank[0]: Epoch[243/500], iter[23550] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.50399
[2024-02-27 08:25:57,131][PyLogger][INFO]: Rank[0]: Epoch[244/500], iter[23600] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.50983
[2024-02-27 08:25:57,131][PyLogger][INFO]: Rank[1]: Epoch[244/500], iter[23600] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.53112
[2024-02-27 08:26:23,218][PyLogger][INFO]: Rank[1]: Epoch[244/500], iter[23650] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.50229
[2024-02-27 08:26:23,218][PyLogger][INFO]: Rank[0]: Epoch[244/500], iter[23650] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.51777
[2024-02-27 08:26:49,929][PyLogger][INFO]: Rank[1]: Epoch[245/500], iter[23700] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.52087
[2024-02-27 08:26:49,929][PyLogger][INFO]: Rank[0]: Epoch[245/500], iter[23700] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.52046
[2024-02-27 08:27:16,289][PyLogger][INFO]: Rank[1]: Epoch[245/500], iter[23750] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.51465
[2024-02-27 08:27:16,289][PyLogger][INFO]: Rank[0]: Epoch[245/500], iter[23750] - train_accuracy: 0.99478 - val_accuracy: 0.91820 - train_loss: 0.51329 - val_loss: 0.73512 - loss: 0.53350
[2024-02-27 08:28:09,885][PyLogger][INFO]: Rank[0]: Epoch[246/500], iter[23800] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50804
[2024-02-27 08:28:09,885][PyLogger][INFO]: Rank[1]: Epoch[246/500], iter[23800] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.51189
[2024-02-27 08:28:35,923][PyLogger][INFO]: Rank[0]: Epoch[246/500], iter[23850] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.51734
[2024-02-27 08:28:35,923][PyLogger][INFO]: Rank[1]: Epoch[246/500], iter[23850] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50414
[2024-02-27 08:29:02,613][PyLogger][INFO]: Rank[1]: Epoch[247/500], iter[23900] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.52080
[2024-02-27 08:29:02,613][PyLogger][INFO]: Rank[0]: Epoch[247/500], iter[23900] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.51297
[2024-02-27 08:29:28,791][PyLogger][INFO]: Rank[1]: Epoch[247/500], iter[23950] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50577
[2024-02-27 08:29:28,791][PyLogger][INFO]: Rank[0]: Epoch[247/500], iter[23950] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50192
[2024-02-27 08:29:55,425][PyLogger][INFO]: Rank[0]: Epoch[248/500], iter[24000] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.51047
[2024-02-27 08:29:55,425][PyLogger][INFO]: Rank[1]: Epoch[248/500], iter[24000] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50385
[2024-02-27 08:30:21,730][PyLogger][INFO]: Rank[1]: Epoch[248/500], iter[24050] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.51409
[2024-02-27 08:30:21,731][PyLogger][INFO]: Rank[0]: Epoch[248/500], iter[24050] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50847
[2024-02-27 08:30:48,445][PyLogger][INFO]: Rank[1]: Epoch[249/500], iter[24100] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.51899
[2024-02-27 08:30:48,445][PyLogger][INFO]: Rank[0]: Epoch[249/500], iter[24100] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50734
[2024-02-27 08:31:14,581][PyLogger][INFO]: Rank[0]: Epoch[249/500], iter[24150] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50795
[2024-02-27 08:31:14,581][PyLogger][INFO]: Rank[1]: Epoch[249/500], iter[24150] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50172
[2024-02-27 08:31:41,389][PyLogger][INFO]: Rank[1]: Epoch[250/500], iter[24200] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50379
[2024-02-27 08:31:41,389][PyLogger][INFO]: Rank[0]: Epoch[250/500], iter[24200] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.51014
[2024-02-27 08:32:07,651][PyLogger][INFO]: Rank[0]: Epoch[250/500], iter[24250] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50974
[2024-02-27 08:32:07,651][PyLogger][INFO]: Rank[1]: Epoch[250/500], iter[24250] - train_accuracy: 0.99565 - val_accuracy: 0.91770 - train_loss: 0.51095 - val_loss: 0.73114 - loss: 0.50690
[2024-02-27 08:33:01,498][PyLogger][INFO]: Rank[1]: Epoch[251/500], iter[24300] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.51852
[2024-02-27 08:33:01,498][PyLogger][INFO]: Rank[0]: Epoch[251/500], iter[24300] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50660
[2024-02-27 08:33:28,118][PyLogger][INFO]: Rank[0]: Epoch[252/500], iter[24350] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50232
[2024-02-27 08:33:28,118][PyLogger][INFO]: Rank[1]: Epoch[252/500], iter[24350] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.51998
[2024-02-27 08:33:54,309][PyLogger][INFO]: Rank[1]: Epoch[252/500], iter[24400] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.51496
[2024-02-27 08:33:54,310][PyLogger][INFO]: Rank[0]: Epoch[252/500], iter[24400] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50351
[2024-02-27 08:34:20,885][PyLogger][INFO]: Rank[0]: Epoch[253/500], iter[24450] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50154
[2024-02-27 08:34:20,885][PyLogger][INFO]: Rank[1]: Epoch[253/500], iter[24450] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50450
[2024-02-27 08:34:47,106][PyLogger][INFO]: Rank[1]: Epoch[253/500], iter[24500] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.52183
[2024-02-27 08:34:47,106][PyLogger][INFO]: Rank[0]: Epoch[253/500], iter[24500] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50216
[2024-02-27 08:35:13,646][PyLogger][INFO]: Rank[1]: Epoch[254/500], iter[24550] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.51930
[2024-02-27 08:35:13,646][PyLogger][INFO]: Rank[0]: Epoch[254/500], iter[24550] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50378
[2024-02-27 08:35:39,674][PyLogger][INFO]: Rank[0]: Epoch[254/500], iter[24600] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50167
[2024-02-27 08:35:39,674][PyLogger][INFO]: Rank[1]: Epoch[254/500], iter[24600] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50802
[2024-02-27 08:36:06,301][PyLogger][INFO]: Rank[0]: Epoch[255/500], iter[24650] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50973
[2024-02-27 08:36:06,302][PyLogger][INFO]: Rank[1]: Epoch[255/500], iter[24650] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50629
[2024-02-27 08:36:32,691][PyLogger][INFO]: Rank[0]: Epoch[255/500], iter[24700] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50468
[2024-02-27 08:36:32,691][PyLogger][INFO]: Rank[1]: Epoch[255/500], iter[24700] - train_accuracy: 0.99549 - val_accuracy: 0.91260 - train_loss: 0.51183 - val_loss: 0.74912 - loss: 0.50507
[2024-02-27 08:37:26,536][PyLogger][INFO]: Rank[1]: Epoch[256/500], iter[24750] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.50279
[2024-02-27 08:37:26,536][PyLogger][INFO]: Rank[0]: Epoch[256/500], iter[24750] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.52930
[2024-02-27 08:37:52,725][PyLogger][INFO]: Rank[0]: Epoch[256/500], iter[24800] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.54443
[2024-02-27 08:37:52,725][PyLogger][INFO]: Rank[1]: Epoch[256/500], iter[24800] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.51902
[2024-02-27 08:38:19,350][PyLogger][INFO]: Rank[1]: Epoch[257/500], iter[24850] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.50494
[2024-02-27 08:38:19,350][PyLogger][INFO]: Rank[0]: Epoch[257/500], iter[24850] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.50491
[2024-02-27 08:38:45,570][PyLogger][INFO]: Rank[1]: Epoch[257/500], iter[24900] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.50739
[2024-02-27 08:38:45,570][PyLogger][INFO]: Rank[0]: Epoch[257/500], iter[24900] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.51476
[2024-02-27 08:39:12,336][PyLogger][INFO]: Rank[1]: Epoch[258/500], iter[24950] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.50209
[2024-02-27 08:39:12,336][PyLogger][INFO]: Rank[0]: Epoch[258/500], iter[24950] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.50204
[2024-02-27 08:39:38,437][PyLogger][INFO]: Rank[1]: Epoch[258/500], iter[25000] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.50409
[2024-02-27 08:39:38,437][PyLogger][INFO]: Rank[0]: Epoch[258/500], iter[25000] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.51382
[2024-02-27 08:40:05,073][PyLogger][INFO]: Rank[1]: Epoch[259/500], iter[25050] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.50182
[2024-02-27 08:40:05,073][PyLogger][INFO]: Rank[0]: Epoch[259/500], iter[25050] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.50155
[2024-02-27 08:40:31,275][PyLogger][INFO]: Rank[0]: Epoch[259/500], iter[25100] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.53089
[2024-02-27 08:40:31,275][PyLogger][INFO]: Rank[1]: Epoch[259/500], iter[25100] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.50656
[2024-02-27 08:40:57,971][PyLogger][INFO]: Rank[1]: Epoch[260/500], iter[25150] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.52505
[2024-02-27 08:40:57,971][PyLogger][INFO]: Rank[0]: Epoch[260/500], iter[25150] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.50577
[2024-02-27 08:41:24,287][PyLogger][INFO]: Rank[0]: Epoch[260/500], iter[25200] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.51584
[2024-02-27 08:41:24,287][PyLogger][INFO]: Rank[1]: Epoch[260/500], iter[25200] - train_accuracy: 0.99517 - val_accuracy: 0.91250 - train_loss: 0.51298 - val_loss: 0.74468 - loss: 0.51853
[2024-02-27 08:42:18,130][PyLogger][INFO]: Rank[1]: Epoch[261/500], iter[25250] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.50669
[2024-02-27 08:42:18,130][PyLogger][INFO]: Rank[0]: Epoch[261/500], iter[25250] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.51742
[2024-02-27 08:42:44,296][PyLogger][INFO]: Rank[0]: Epoch[261/500], iter[25300] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.52578
[2024-02-27 08:42:44,296][PyLogger][INFO]: Rank[1]: Epoch[261/500], iter[25300] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.51180
[2024-02-27 08:43:10,919][PyLogger][INFO]: Rank[1]: Epoch[262/500], iter[25350] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.50309
[2024-02-27 08:43:10,919][PyLogger][INFO]: Rank[0]: Epoch[262/500], iter[25350] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.50276
[2024-02-27 08:43:37,103][PyLogger][INFO]: Rank[1]: Epoch[262/500], iter[25400] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.51389
[2024-02-27 08:43:37,103][PyLogger][INFO]: Rank[0]: Epoch[262/500], iter[25400] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.52902
[2024-02-27 08:44:03,887][PyLogger][INFO]: Rank[1]: Epoch[263/500], iter[25450] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.50448
[2024-02-27 08:44:03,887][PyLogger][INFO]: Rank[0]: Epoch[263/500], iter[25450] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.51824
[2024-02-27 08:44:30,066][PyLogger][INFO]: Rank[1]: Epoch[263/500], iter[25500] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.50796
[2024-02-27 08:44:30,066][PyLogger][INFO]: Rank[0]: Epoch[263/500], iter[25500] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.50150
[2024-02-27 08:44:56,569][PyLogger][INFO]: Rank[0]: Epoch[264/500], iter[25550] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.50641
[2024-02-27 08:44:56,569][PyLogger][INFO]: Rank[1]: Epoch[264/500], iter[25550] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.51272
[2024-02-27 08:45:22,933][PyLogger][INFO]: Rank[1]: Epoch[264/500], iter[25600] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.50282
[2024-02-27 08:45:22,933][PyLogger][INFO]: Rank[0]: Epoch[264/500], iter[25600] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.50172
[2024-02-27 08:45:49,591][PyLogger][INFO]: Rank[1]: Epoch[265/500], iter[25650] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.51104
[2024-02-27 08:45:49,591][PyLogger][INFO]: Rank[0]: Epoch[265/500], iter[25650] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.50698
[2024-02-27 08:46:15,782][PyLogger][INFO]: Rank[0]: Epoch[265/500], iter[25700] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.51487
[2024-02-27 08:46:15,782][PyLogger][INFO]: Rank[1]: Epoch[265/500], iter[25700] - train_accuracy: 0.99287 - val_accuracy: 0.91350 - train_loss: 0.51832 - val_loss: 0.75105 - loss: 0.52136
[2024-02-27 08:47:09,865][PyLogger][INFO]: Rank[1]: Epoch[266/500], iter[25750] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50541
[2024-02-27 08:47:09,865][PyLogger][INFO]: Rank[0]: Epoch[266/500], iter[25750] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.52325
[2024-02-27 08:47:35,985][PyLogger][INFO]: Rank[0]: Epoch[266/500], iter[25800] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.51067
[2024-02-27 08:47:35,985][PyLogger][INFO]: Rank[1]: Epoch[266/500], iter[25800] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.51538
[2024-02-27 08:48:02,642][PyLogger][INFO]: Rank[1]: Epoch[267/500], iter[25850] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50855
[2024-02-27 08:48:02,642][PyLogger][INFO]: Rank[0]: Epoch[267/500], iter[25850] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50802
[2024-02-27 08:48:29,387][PyLogger][INFO]: Rank[1]: Epoch[268/500], iter[25900] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50630
[2024-02-27 08:48:29,387][PyLogger][INFO]: Rank[0]: Epoch[268/500], iter[25900] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50518
[2024-02-27 08:48:55,581][PyLogger][INFO]: Rank[0]: Epoch[268/500], iter[25950] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50619
[2024-02-27 08:48:55,581][PyLogger][INFO]: Rank[1]: Epoch[268/500], iter[25950] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50791
[2024-02-27 08:49:22,340][PyLogger][INFO]: Rank[1]: Epoch[269/500], iter[26000] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50565
[2024-02-27 08:49:22,340][PyLogger][INFO]: Rank[0]: Epoch[269/500], iter[26000] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50226
[2024-02-27 08:49:48,549][PyLogger][INFO]: Rank[0]: Epoch[269/500], iter[26050] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.51111
[2024-02-27 08:49:48,549][PyLogger][INFO]: Rank[1]: Epoch[269/500], iter[26050] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.52037
[2024-02-27 08:50:15,225][PyLogger][INFO]: Rank[1]: Epoch[270/500], iter[26100] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.52672
[2024-02-27 08:50:15,225][PyLogger][INFO]: Rank[0]: Epoch[270/500], iter[26100] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50229
[2024-02-27 08:50:41,376][PyLogger][INFO]: Rank[0]: Epoch[270/500], iter[26150] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50885
[2024-02-27 08:50:41,377][PyLogger][INFO]: Rank[1]: Epoch[270/500], iter[26150] - train_accuracy: 0.99360 - val_accuracy: 0.91330 - train_loss: 0.51517 - val_loss: 0.75061 - loss: 0.50638
[2024-02-27 08:51:35,248][PyLogger][INFO]: Rank[1]: Epoch[271/500], iter[26200] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.51882
[2024-02-27 08:51:35,248][PyLogger][INFO]: Rank[0]: Epoch[271/500], iter[26200] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.50233
[2024-02-27 08:52:01,542][PyLogger][INFO]: Rank[1]: Epoch[271/500], iter[26250] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.50639
[2024-02-27 08:52:01,542][PyLogger][INFO]: Rank[0]: Epoch[271/500], iter[26250] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.50838
[2024-02-27 08:52:28,223][PyLogger][INFO]: Rank[0]: Epoch[272/500], iter[26300] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.52420
[2024-02-27 08:52:28,223][PyLogger][INFO]: Rank[1]: Epoch[272/500], iter[26300] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.51012
[2024-02-27 08:52:54,459][PyLogger][INFO]: Rank[1]: Epoch[272/500], iter[26350] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.51111
[2024-02-27 08:52:54,459][PyLogger][INFO]: Rank[0]: Epoch[272/500], iter[26350] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.50470
[2024-02-27 08:53:21,210][PyLogger][INFO]: Rank[1]: Epoch[273/500], iter[26400] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.50741
[2024-02-27 08:53:21,211][PyLogger][INFO]: Rank[0]: Epoch[273/500], iter[26400] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.51734
[2024-02-27 08:53:47,439][PyLogger][INFO]: Rank[0]: Epoch[273/500], iter[26450] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.51350
[2024-02-27 08:53:47,439][PyLogger][INFO]: Rank[1]: Epoch[273/500], iter[26450] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.50232
[2024-02-27 08:54:14,125][PyLogger][INFO]: Rank[1]: Epoch[274/500], iter[26500] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.50332
[2024-02-27 08:54:14,125][PyLogger][INFO]: Rank[0]: Epoch[274/500], iter[26500] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.51472
[2024-02-27 08:54:40,300][PyLogger][INFO]: Rank[0]: Epoch[274/500], iter[26550] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.51147
[2024-02-27 08:54:40,300][PyLogger][INFO]: Rank[1]: Epoch[274/500], iter[26550] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.50773
[2024-02-27 08:55:06,920][PyLogger][INFO]: Rank[1]: Epoch[275/500], iter[26600] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.50351
[2024-02-27 08:55:06,920][PyLogger][INFO]: Rank[0]: Epoch[275/500], iter[26600] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.51303
[2024-02-27 08:55:33,072][PyLogger][INFO]: Rank[1]: Epoch[275/500], iter[26650] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.51880
[2024-02-27 08:55:33,072][PyLogger][INFO]: Rank[0]: Epoch[275/500], iter[26650] - train_accuracy: 0.99535 - val_accuracy: 0.91530 - train_loss: 0.51245 - val_loss: 0.74084 - loss: 0.50649
[2024-02-27 08:56:27,244][PyLogger][INFO]: Rank[1]: Epoch[276/500], iter[26700] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50233
[2024-02-27 08:56:27,244][PyLogger][INFO]: Rank[0]: Epoch[276/500], iter[26700] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50270
[2024-02-27 08:56:53,426][PyLogger][INFO]: Rank[1]: Epoch[276/500], iter[26750] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50186
[2024-02-27 08:56:53,426][PyLogger][INFO]: Rank[0]: Epoch[276/500], iter[26750] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50346
[2024-02-27 08:57:20,206][PyLogger][INFO]: Rank[1]: Epoch[277/500], iter[26800] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50753
[2024-02-27 08:57:20,206][PyLogger][INFO]: Rank[0]: Epoch[277/500], iter[26800] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.51256
[2024-02-27 08:57:46,387][PyLogger][INFO]: Rank[1]: Epoch[277/500], iter[26850] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.51590
[2024-02-27 08:57:46,387][PyLogger][INFO]: Rank[0]: Epoch[277/500], iter[26850] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50269
[2024-02-27 08:58:13,041][PyLogger][INFO]: Rank[1]: Epoch[278/500], iter[26900] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50683
[2024-02-27 08:58:13,041][PyLogger][INFO]: Rank[0]: Epoch[278/500], iter[26900] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50299
[2024-02-27 08:58:39,247][PyLogger][INFO]: Rank[0]: Epoch[278/500], iter[26950] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50850
[2024-02-27 08:58:39,247][PyLogger][INFO]: Rank[1]: Epoch[278/500], iter[26950] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50840
[2024-02-27 08:59:05,845][PyLogger][INFO]: Rank[1]: Epoch[279/500], iter[27000] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50484
[2024-02-27 08:59:05,845][PyLogger][INFO]: Rank[0]: Epoch[279/500], iter[27000] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.51864
[2024-02-27 08:59:32,073][PyLogger][INFO]: Rank[1]: Epoch[279/500], iter[27050] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.52018
[2024-02-27 08:59:32,073][PyLogger][INFO]: Rank[0]: Epoch[279/500], iter[27050] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50441
[2024-02-27 08:59:58,789][PyLogger][INFO]: Rank[0]: Epoch[280/500], iter[27100] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50141
[2024-02-27 08:59:58,789][PyLogger][INFO]: Rank[1]: Epoch[280/500], iter[27100] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50296
[2024-02-27 09:00:24,993][PyLogger][INFO]: Rank[1]: Epoch[280/500], iter[27150] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50100
[2024-02-27 09:00:24,993][PyLogger][INFO]: Rank[0]: Epoch[280/500], iter[27150] - train_accuracy: 0.99531 - val_accuracy: 0.91880 - train_loss: 0.51167 - val_loss: 0.73376 - loss: 0.50268
[2024-02-27 09:01:18,865][PyLogger][INFO]: Rank[1]: Epoch[281/500], iter[27200] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.51060
[2024-02-27 09:01:18,865][PyLogger][INFO]: Rank[0]: Epoch[281/500], iter[27200] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50256
[2024-02-27 09:01:45,068][PyLogger][INFO]: Rank[1]: Epoch[281/500], iter[27250] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50941
[2024-02-27 09:01:45,068][PyLogger][INFO]: Rank[0]: Epoch[281/500], iter[27250] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.51184
[2024-02-27 09:02:11,794][PyLogger][INFO]: Rank[0]: Epoch[282/500], iter[27300] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50587
[2024-02-27 09:02:11,794][PyLogger][INFO]: Rank[1]: Epoch[282/500], iter[27300] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50204
[2024-02-27 09:02:37,972][PyLogger][INFO]: Rank[1]: Epoch[282/500], iter[27350] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50309
[2024-02-27 09:02:37,972][PyLogger][INFO]: Rank[0]: Epoch[282/500], iter[27350] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.51779
[2024-02-27 09:03:04,586][PyLogger][INFO]: Rank[0]: Epoch[283/500], iter[27400] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.51370
[2024-02-27 09:03:04,586][PyLogger][INFO]: Rank[1]: Epoch[283/500], iter[27400] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50906
[2024-02-27 09:03:30,756][PyLogger][INFO]: Rank[1]: Epoch[283/500], iter[27450] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50503
[2024-02-27 09:03:30,757][PyLogger][INFO]: Rank[0]: Epoch[283/500], iter[27450] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.52390
[2024-02-27 09:03:57,453][PyLogger][INFO]: Rank[1]: Epoch[284/500], iter[27500] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50783
[2024-02-27 09:03:57,453][PyLogger][INFO]: Rank[0]: Epoch[284/500], iter[27500] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50263
[2024-02-27 09:04:24,132][PyLogger][INFO]: Rank[1]: Epoch[285/500], iter[27550] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50116
[2024-02-27 09:04:24,132][PyLogger][INFO]: Rank[0]: Epoch[285/500], iter[27550] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50085
[2024-02-27 09:04:50,304][PyLogger][INFO]: Rank[1]: Epoch[285/500], iter[27600] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50361
[2024-02-27 09:04:50,304][PyLogger][INFO]: Rank[0]: Epoch[285/500], iter[27600] - train_accuracy: 0.99507 - val_accuracy: 0.91420 - train_loss: 0.51301 - val_loss: 0.74981 - loss: 0.50177
[2024-02-27 09:05:44,148][PyLogger][INFO]: Rank[0]: Epoch[286/500], iter[27650] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.51391
[2024-02-27 09:05:44,148][PyLogger][INFO]: Rank[1]: Epoch[286/500], iter[27650] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.50228
[2024-02-27 09:06:10,475][PyLogger][INFO]: Rank[1]: Epoch[286/500], iter[27700] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.51339
[2024-02-27 09:06:10,476][PyLogger][INFO]: Rank[0]: Epoch[286/500], iter[27700] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.50387
[2024-02-27 09:06:37,208][PyLogger][INFO]: Rank[1]: Epoch[287/500], iter[27750] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.51752
[2024-02-27 09:06:37,208][PyLogger][INFO]: Rank[0]: Epoch[287/500], iter[27750] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.53287
[2024-02-27 09:07:03,344][PyLogger][INFO]: Rank[0]: Epoch[287/500], iter[27800] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.50701
[2024-02-27 09:07:03,344][PyLogger][INFO]: Rank[1]: Epoch[287/500], iter[27800] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.51386
[2024-02-27 09:07:30,074][PyLogger][INFO]: Rank[1]: Epoch[288/500], iter[27850] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.51697
[2024-02-27 09:07:30,074][PyLogger][INFO]: Rank[0]: Epoch[288/500], iter[27850] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.50485
[2024-02-27 09:07:56,437][PyLogger][INFO]: Rank[1]: Epoch[288/500], iter[27900] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.50690
[2024-02-27 09:07:56,437][PyLogger][INFO]: Rank[0]: Epoch[288/500], iter[27900] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.50727
[2024-02-27 09:08:23,138][PyLogger][INFO]: Rank[0]: Epoch[289/500], iter[27950] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.50931
[2024-02-27 09:08:23,138][PyLogger][INFO]: Rank[1]: Epoch[289/500], iter[27950] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.50785
[2024-02-27 09:08:49,344][PyLogger][INFO]: Rank[0]: Epoch[289/500], iter[28000] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.52360
[2024-02-27 09:08:49,344][PyLogger][INFO]: Rank[1]: Epoch[289/500], iter[28000] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.50598
[2024-02-27 09:09:16,070][PyLogger][INFO]: Rank[1]: Epoch[290/500], iter[28050] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.51390
[2024-02-27 09:09:16,070][PyLogger][INFO]: Rank[0]: Epoch[290/500], iter[28050] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.51232
[2024-02-27 09:09:42,196][PyLogger][INFO]: Rank[0]: Epoch[290/500], iter[28100] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.50782
[2024-02-27 09:09:42,196][PyLogger][INFO]: Rank[1]: Epoch[290/500], iter[28100] - train_accuracy: 0.99738 - val_accuracy: 0.91870 - train_loss: 0.50669 - val_loss: 0.73566 - loss: 0.50798
[2024-02-27 09:10:36,189][PyLogger][INFO]: Rank[1]: Epoch[291/500], iter[28150] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50286
[2024-02-27 09:10:36,189][PyLogger][INFO]: Rank[0]: Epoch[291/500], iter[28150] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50200
[2024-02-27 09:11:02,375][PyLogger][INFO]: Rank[0]: Epoch[291/500], iter[28200] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50183
[2024-02-27 09:11:02,375][PyLogger][INFO]: Rank[1]: Epoch[291/500], iter[28200] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50534
[2024-02-27 09:11:29,009][PyLogger][INFO]: Rank[1]: Epoch[292/500], iter[28250] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50111
[2024-02-27 09:11:29,009][PyLogger][INFO]: Rank[0]: Epoch[292/500], iter[28250] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.51637
[2024-02-27 09:11:55,242][PyLogger][INFO]: Rank[0]: Epoch[292/500], iter[28300] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50104
[2024-02-27 09:11:55,242][PyLogger][INFO]: Rank[1]: Epoch[292/500], iter[28300] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50133
[2024-02-27 09:12:21,979][PyLogger][INFO]: Rank[1]: Epoch[293/500], iter[28350] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.51541
[2024-02-27 09:12:21,979][PyLogger][INFO]: Rank[0]: Epoch[293/500], iter[28350] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50552
[2024-02-27 09:12:48,256][PyLogger][INFO]: Rank[1]: Epoch[293/500], iter[28400] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50712
[2024-02-27 09:12:48,256][PyLogger][INFO]: Rank[0]: Epoch[293/500], iter[28400] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.51271
[2024-02-27 09:13:15,024][PyLogger][INFO]: Rank[1]: Epoch[294/500], iter[28450] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50430
[2024-02-27 09:13:15,024][PyLogger][INFO]: Rank[0]: Epoch[294/500], iter[28450] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50558
[2024-02-27 09:13:41,283][PyLogger][INFO]: Rank[1]: Epoch[294/500], iter[28500] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.51156
[2024-02-27 09:13:41,283][PyLogger][INFO]: Rank[0]: Epoch[294/500], iter[28500] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50115
[2024-02-27 09:14:07,939][PyLogger][INFO]: Rank[0]: Epoch[295/500], iter[28550] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50751
[2024-02-27 09:14:07,939][PyLogger][INFO]: Rank[1]: Epoch[295/500], iter[28550] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50099
[2024-02-27 09:14:34,142][PyLogger][INFO]: Rank[1]: Epoch[295/500], iter[28600] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50174
[2024-02-27 09:14:34,142][PyLogger][INFO]: Rank[0]: Epoch[295/500], iter[28600] - train_accuracy: 0.99752 - val_accuracy: 0.91770 - train_loss: 0.50687 - val_loss: 0.73685 - loss: 0.50111
[2024-02-27 09:15:27,968][PyLogger][INFO]: Rank[0]: Epoch[296/500], iter[28650] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.51326
[2024-02-27 09:15:27,968][PyLogger][INFO]: Rank[1]: Epoch[296/500], iter[28650] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.52056
[2024-02-27 09:15:54,256][PyLogger][INFO]: Rank[1]: Epoch[296/500], iter[28700] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50295
[2024-02-27 09:15:54,257][PyLogger][INFO]: Rank[0]: Epoch[296/500], iter[28700] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.51153
[2024-02-27 09:16:21,000][PyLogger][INFO]: Rank[0]: Epoch[297/500], iter[28750] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50367
[2024-02-27 09:16:21,001][PyLogger][INFO]: Rank[1]: Epoch[297/500], iter[28750] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50231
[2024-02-27 09:16:47,301][PyLogger][INFO]: Rank[1]: Epoch[297/500], iter[28800] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50245
[2024-02-27 09:16:47,301][PyLogger][INFO]: Rank[0]: Epoch[297/500], iter[28800] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50454
[2024-02-27 09:17:14,035][PyLogger][INFO]: Rank[1]: Epoch[298/500], iter[28850] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50127
[2024-02-27 09:17:14,035][PyLogger][INFO]: Rank[0]: Epoch[298/500], iter[28850] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50257
[2024-02-27 09:17:40,225][PyLogger][INFO]: Rank[0]: Epoch[298/500], iter[28900] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50144
[2024-02-27 09:17:40,225][PyLogger][INFO]: Rank[1]: Epoch[298/500], iter[28900] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50286
[2024-02-27 09:18:06,959][PyLogger][INFO]: Rank[1]: Epoch[299/500], iter[28950] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50092
[2024-02-27 09:18:06,959][PyLogger][INFO]: Rank[0]: Epoch[299/500], iter[28950] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50064
[2024-02-27 09:18:33,241][PyLogger][INFO]: Rank[0]: Epoch[299/500], iter[29000] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50178
[2024-02-27 09:18:33,241][PyLogger][INFO]: Rank[1]: Epoch[299/500], iter[29000] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50073
[2024-02-27 09:18:59,950][PyLogger][INFO]: Rank[0]: Epoch[300/500], iter[29050] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.51189
[2024-02-27 09:18:59,950][PyLogger][INFO]: Rank[1]: Epoch[300/500], iter[29050] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.51228
[2024-02-27 09:19:26,181][PyLogger][INFO]: Rank[1]: Epoch[300/500], iter[29100] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50845
[2024-02-27 09:19:26,181][PyLogger][INFO]: Rank[0]: Epoch[300/500], iter[29100] - train_accuracy: 0.99497 - val_accuracy: 0.91370 - train_loss: 0.51151 - val_loss: 0.74686 - loss: 0.50280
[2024-02-27 09:20:19,942][PyLogger][INFO]: Rank[0]: Epoch[301/500], iter[29150] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50675
[2024-02-27 09:20:19,942][PyLogger][INFO]: Rank[1]: Epoch[301/500], iter[29150] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.53085
[2024-02-27 09:20:46,619][PyLogger][INFO]: Rank[1]: Epoch[302/500], iter[29200] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50892
[2024-02-27 09:20:46,619][PyLogger][INFO]: Rank[0]: Epoch[302/500], iter[29200] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50189
[2024-02-27 09:21:12,847][PyLogger][INFO]: Rank[0]: Epoch[302/500], iter[29250] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50112
[2024-02-27 09:21:12,847][PyLogger][INFO]: Rank[1]: Epoch[302/500], iter[29250] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50432
[2024-02-27 09:21:39,445][PyLogger][INFO]: Rank[1]: Epoch[303/500], iter[29300] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50251
[2024-02-27 09:21:39,445][PyLogger][INFO]: Rank[0]: Epoch[303/500], iter[29300] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50115
[2024-02-27 09:22:05,713][PyLogger][INFO]: Rank[0]: Epoch[303/500], iter[29350] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50218
[2024-02-27 09:22:05,713][PyLogger][INFO]: Rank[1]: Epoch[303/500], iter[29350] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50079
[2024-02-27 09:22:32,382][PyLogger][INFO]: Rank[0]: Epoch[304/500], iter[29400] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50176
[2024-02-27 09:22:32,382][PyLogger][INFO]: Rank[1]: Epoch[304/500], iter[29400] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.51055
[2024-02-27 09:22:58,587][PyLogger][INFO]: Rank[0]: Epoch[304/500], iter[29450] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50152
[2024-02-27 09:22:58,587][PyLogger][INFO]: Rank[1]: Epoch[304/500], iter[29450] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50165
[2024-02-27 09:23:25,168][PyLogger][INFO]: Rank[1]: Epoch[305/500], iter[29500] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50356
[2024-02-27 09:23:25,168][PyLogger][INFO]: Rank[0]: Epoch[305/500], iter[29500] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50205
[2024-02-27 09:23:51,440][PyLogger][INFO]: Rank[1]: Epoch[305/500], iter[29550] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50868
[2024-02-27 09:23:51,440][PyLogger][INFO]: Rank[0]: Epoch[305/500], iter[29550] - train_accuracy: 0.99776 - val_accuracy: 0.92370 - train_loss: 0.50570 - val_loss: 0.72193 - loss: 0.50401
[2024-02-27 09:24:45,342][PyLogger][INFO]: Rank[1]: Epoch[306/500], iter[29600] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50554
[2024-02-27 09:24:45,343][PyLogger][INFO]: Rank[0]: Epoch[306/500], iter[29600] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50646
[2024-02-27 09:25:11,471][PyLogger][INFO]: Rank[1]: Epoch[306/500], iter[29650] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50089
[2024-02-27 09:25:11,471][PyLogger][INFO]: Rank[0]: Epoch[306/500], iter[29650] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50371
[2024-02-27 09:25:38,201][PyLogger][INFO]: Rank[1]: Epoch[307/500], iter[29700] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.51073
[2024-02-27 09:25:38,201][PyLogger][INFO]: Rank[0]: Epoch[307/500], iter[29700] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50233
[2024-02-27 09:26:04,455][PyLogger][INFO]: Rank[0]: Epoch[307/500], iter[29750] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50240
[2024-02-27 09:26:04,455][PyLogger][INFO]: Rank[1]: Epoch[307/500], iter[29750] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.51272
[2024-02-27 09:26:31,282][PyLogger][INFO]: Rank[1]: Epoch[308/500], iter[29800] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50338
[2024-02-27 09:26:31,282][PyLogger][INFO]: Rank[0]: Epoch[308/500], iter[29800] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50160
[2024-02-27 09:26:57,584][PyLogger][INFO]: Rank[1]: Epoch[308/500], iter[29850] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50305
[2024-02-27 09:26:57,584][PyLogger][INFO]: Rank[0]: Epoch[308/500], iter[29850] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50092
[2024-02-27 09:27:24,171][PyLogger][INFO]: Rank[1]: Epoch[309/500], iter[29900] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50162
[2024-02-27 09:27:24,172][PyLogger][INFO]: Rank[0]: Epoch[309/500], iter[29900] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50809
[2024-02-27 09:27:50,372][PyLogger][INFO]: Rank[1]: Epoch[309/500], iter[29950] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50471
[2024-02-27 09:27:50,372][PyLogger][INFO]: Rank[0]: Epoch[309/500], iter[29950] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50185
[2024-02-27 09:28:17,074][PyLogger][INFO]: Rank[0]: Epoch[310/500], iter[30000] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50177
[2024-02-27 09:28:17,074][PyLogger][INFO]: Rank[1]: Epoch[310/500], iter[30000] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50453
[2024-02-27 09:28:43,214][PyLogger][INFO]: Rank[1]: Epoch[310/500], iter[30050] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50103
[2024-02-27 09:28:43,214][PyLogger][INFO]: Rank[0]: Epoch[310/500], iter[30050] - train_accuracy: 0.99853 - val_accuracy: 0.92270 - train_loss: 0.50383 - val_loss: 0.72246 - loss: 0.50435
[2024-02-27 09:29:37,299][PyLogger][INFO]: Rank[1]: Epoch[311/500], iter[30100] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50062
[2024-02-27 09:29:37,299][PyLogger][INFO]: Rank[0]: Epoch[311/500], iter[30100] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50171
[2024-02-27 09:30:03,395][PyLogger][INFO]: Rank[0]: Epoch[311/500], iter[30150] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50366
[2024-02-27 09:30:03,395][PyLogger][INFO]: Rank[1]: Epoch[311/500], iter[30150] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50283
[2024-02-27 09:30:29,945][PyLogger][INFO]: Rank[1]: Epoch[312/500], iter[30200] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50089
[2024-02-27 09:30:29,945][PyLogger][INFO]: Rank[0]: Epoch[312/500], iter[30200] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50117
[2024-02-27 09:30:56,100][PyLogger][INFO]: Rank[0]: Epoch[312/500], iter[30250] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50404
[2024-02-27 09:30:56,100][PyLogger][INFO]: Rank[1]: Epoch[312/500], iter[30250] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50860
[2024-02-27 09:31:22,860][PyLogger][INFO]: Rank[1]: Epoch[313/500], iter[30300] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50165
[2024-02-27 09:31:22,860][PyLogger][INFO]: Rank[0]: Epoch[313/500], iter[30300] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50103
[2024-02-27 09:31:49,069][PyLogger][INFO]: Rank[1]: Epoch[313/500], iter[30350] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50091
[2024-02-27 09:31:49,069][PyLogger][INFO]: Rank[0]: Epoch[313/500], iter[30350] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50066
[2024-02-27 09:32:15,722][PyLogger][INFO]: Rank[0]: Epoch[314/500], iter[30400] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.51019
[2024-02-27 09:32:15,722][PyLogger][INFO]: Rank[1]: Epoch[314/500], iter[30400] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50393
[2024-02-27 09:32:41,829][PyLogger][INFO]: Rank[1]: Epoch[314/500], iter[30450] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50287
[2024-02-27 09:32:41,829][PyLogger][INFO]: Rank[0]: Epoch[314/500], iter[30450] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50130
[2024-02-27 09:33:08,722][PyLogger][INFO]: Rank[0]: Epoch[315/500], iter[30500] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50522
[2024-02-27 09:33:08,722][PyLogger][INFO]: Rank[1]: Epoch[315/500], iter[30500] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50116
[2024-02-27 09:33:34,969][PyLogger][INFO]: Rank[0]: Epoch[315/500], iter[30550] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.50464
[2024-02-27 09:33:34,969][PyLogger][INFO]: Rank[1]: Epoch[315/500], iter[30550] - train_accuracy: 0.99871 - val_accuracy: 0.92400 - train_loss: 0.50372 - val_loss: 0.72547 - loss: 0.51748
[2024-02-27 09:34:28,982][PyLogger][INFO]: Rank[1]: Epoch[316/500], iter[30600] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50222
[2024-02-27 09:34:28,982][PyLogger][INFO]: Rank[0]: Epoch[316/500], iter[30600] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50601
[2024-02-27 09:34:55,222][PyLogger][INFO]: Rank[1]: Epoch[316/500], iter[30650] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50110
[2024-02-27 09:34:55,222][PyLogger][INFO]: Rank[0]: Epoch[316/500], iter[30650] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50083
[2024-02-27 09:35:21,819][PyLogger][INFO]: Rank[1]: Epoch[317/500], iter[30700] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50249
[2024-02-27 09:35:21,819][PyLogger][INFO]: Rank[0]: Epoch[317/500], iter[30700] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50163
[2024-02-27 09:35:48,533][PyLogger][INFO]: Rank[0]: Epoch[318/500], iter[30750] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50479
[2024-02-27 09:35:48,533][PyLogger][INFO]: Rank[1]: Epoch[318/500], iter[30750] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50241
[2024-02-27 09:36:14,837][PyLogger][INFO]: Rank[1]: Epoch[318/500], iter[30800] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50113
[2024-02-27 09:36:14,837][PyLogger][INFO]: Rank[0]: Epoch[318/500], iter[30800] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50586
[2024-02-27 09:36:41,436][PyLogger][INFO]: Rank[0]: Epoch[319/500], iter[30850] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.51113
[2024-02-27 09:36:41,436][PyLogger][INFO]: Rank[1]: Epoch[319/500], iter[30850] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50091
[2024-02-27 09:37:07,702][PyLogger][INFO]: Rank[1]: Epoch[319/500], iter[30900] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50116
[2024-02-27 09:37:07,702][PyLogger][INFO]: Rank[0]: Epoch[319/500], iter[30900] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50069
[2024-02-27 09:37:34,409][PyLogger][INFO]: Rank[1]: Epoch[320/500], iter[30950] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50140
[2024-02-27 09:37:34,409][PyLogger][INFO]: Rank[0]: Epoch[320/500], iter[30950] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50083
[2024-02-27 09:38:00,775][PyLogger][INFO]: Rank[1]: Epoch[320/500], iter[31000] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.50071
[2024-02-27 09:38:00,775][PyLogger][INFO]: Rank[0]: Epoch[320/500], iter[31000] - train_accuracy: 0.99734 - val_accuracy: 0.92300 - train_loss: 0.50661 - val_loss: 0.73137 - loss: 0.51964
[2024-02-27 09:38:54,973][PyLogger][INFO]: Rank[0]: Epoch[321/500], iter[31050] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50132
[2024-02-27 09:38:54,973][PyLogger][INFO]: Rank[1]: Epoch[321/500], iter[31050] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50074
[2024-02-27 09:39:21,091][PyLogger][INFO]: Rank[1]: Epoch[321/500], iter[31100] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50197
[2024-02-27 09:39:21,091][PyLogger][INFO]: Rank[0]: Epoch[321/500], iter[31100] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50414
[2024-02-27 09:39:48,175][PyLogger][INFO]: Rank[0]: Epoch[322/500], iter[31150] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50143
[2024-02-27 09:39:48,175][PyLogger][INFO]: Rank[1]: Epoch[322/500], iter[31150] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50072
[2024-02-27 09:40:14,295][PyLogger][INFO]: Rank[1]: Epoch[322/500], iter[31200] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50198
[2024-02-27 09:40:14,295][PyLogger][INFO]: Rank[0]: Epoch[322/500], iter[31200] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50064
[2024-02-27 09:40:40,964][PyLogger][INFO]: Rank[1]: Epoch[323/500], iter[31250] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50899
[2024-02-27 09:40:40,964][PyLogger][INFO]: Rank[0]: Epoch[323/500], iter[31250] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50298
[2024-02-27 09:41:07,109][PyLogger][INFO]: Rank[0]: Epoch[323/500], iter[31300] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50082
[2024-02-27 09:41:07,109][PyLogger][INFO]: Rank[1]: Epoch[323/500], iter[31300] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50308
[2024-02-27 09:41:33,968][PyLogger][INFO]: Rank[1]: Epoch[324/500], iter[31350] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50066
[2024-02-27 09:41:33,968][PyLogger][INFO]: Rank[0]: Epoch[324/500], iter[31350] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50528
[2024-02-27 09:42:00,252][PyLogger][INFO]: Rank[1]: Epoch[324/500], iter[31400] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50072
[2024-02-27 09:42:00,252][PyLogger][INFO]: Rank[0]: Epoch[324/500], iter[31400] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50206
[2024-02-27 09:42:26,775][PyLogger][INFO]: Rank[1]: Epoch[325/500], iter[31450] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50057
[2024-02-27 09:42:26,775][PyLogger][INFO]: Rank[0]: Epoch[325/500], iter[31450] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50057
[2024-02-27 09:42:53,010][PyLogger][INFO]: Rank[1]: Epoch[325/500], iter[31500] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50148
[2024-02-27 09:42:53,010][PyLogger][INFO]: Rank[0]: Epoch[325/500], iter[31500] - train_accuracy: 0.99917 - val_accuracy: 0.92560 - train_loss: 0.50253 - val_loss: 0.72129 - loss: 0.50122
[2024-02-27 09:43:46,849][PyLogger][INFO]: Rank[1]: Epoch[326/500], iter[31550] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50075
[2024-02-27 09:43:46,849][PyLogger][INFO]: Rank[0]: Epoch[326/500], iter[31550] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50587
[2024-02-27 09:44:12,976][PyLogger][INFO]: Rank[0]: Epoch[326/500], iter[31600] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50064
[2024-02-27 09:44:12,976][PyLogger][INFO]: Rank[1]: Epoch[326/500], iter[31600] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50139
[2024-02-27 09:44:39,495][PyLogger][INFO]: Rank[0]: Epoch[327/500], iter[31650] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50070
[2024-02-27 09:44:39,495][PyLogger][INFO]: Rank[1]: Epoch[327/500], iter[31650] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50084
[2024-02-27 09:45:05,706][PyLogger][INFO]: Rank[1]: Epoch[327/500], iter[31700] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50922
[2024-02-27 09:45:05,706][PyLogger][INFO]: Rank[0]: Epoch[327/500], iter[31700] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50079
[2024-02-27 09:45:32,292][PyLogger][INFO]: Rank[1]: Epoch[328/500], iter[31750] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50089
[2024-02-27 09:45:32,292][PyLogger][INFO]: Rank[0]: Epoch[328/500], iter[31750] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50054
[2024-02-27 09:45:58,649][PyLogger][INFO]: Rank[1]: Epoch[328/500], iter[31800] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50074
[2024-02-27 09:45:58,649][PyLogger][INFO]: Rank[0]: Epoch[328/500], iter[31800] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50545
[2024-02-27 09:46:25,329][PyLogger][INFO]: Rank[1]: Epoch[329/500], iter[31850] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50078
[2024-02-27 09:46:25,329][PyLogger][INFO]: Rank[0]: Epoch[329/500], iter[31850] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50059
[2024-02-27 09:46:51,518][PyLogger][INFO]: Rank[1]: Epoch[329/500], iter[31900] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50073
[2024-02-27 09:46:51,518][PyLogger][INFO]: Rank[0]: Epoch[329/500], iter[31900] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50141
[2024-02-27 09:47:18,287][PyLogger][INFO]: Rank[1]: Epoch[330/500], iter[31950] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50064
[2024-02-27 09:47:18,287][PyLogger][INFO]: Rank[0]: Epoch[330/500], iter[31950] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50060
[2024-02-27 09:47:44,448][PyLogger][INFO]: Rank[1]: Epoch[330/500], iter[32000] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50063
[2024-02-27 09:47:44,448][PyLogger][INFO]: Rank[0]: Epoch[330/500], iter[32000] - train_accuracy: 0.99881 - val_accuracy: 0.92160 - train_loss: 0.50342 - val_loss: 0.73437 - loss: 0.50090
[2024-02-27 09:48:38,570][PyLogger][INFO]: Rank[0]: Epoch[331/500], iter[32050] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50054
[2024-02-27 09:48:38,570][PyLogger][INFO]: Rank[1]: Epoch[331/500], iter[32050] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50078
[2024-02-27 09:49:04,767][PyLogger][INFO]: Rank[1]: Epoch[331/500], iter[32100] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50190
[2024-02-27 09:49:04,767][PyLogger][INFO]: Rank[0]: Epoch[331/500], iter[32100] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50483
[2024-02-27 09:49:31,412][PyLogger][INFO]: Rank[0]: Epoch[332/500], iter[32150] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50058
[2024-02-27 09:49:31,412][PyLogger][INFO]: Rank[1]: Epoch[332/500], iter[32150] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50358
[2024-02-27 09:49:57,545][PyLogger][INFO]: Rank[1]: Epoch[332/500], iter[32200] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50078
[2024-02-27 09:49:57,545][PyLogger][INFO]: Rank[0]: Epoch[332/500], iter[32200] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50088
[2024-02-27 09:50:24,421][PyLogger][INFO]: Rank[0]: Epoch[333/500], iter[32250] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50133
[2024-02-27 09:50:24,421][PyLogger][INFO]: Rank[1]: Epoch[333/500], iter[32250] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50055
[2024-02-27 09:50:50,689][PyLogger][INFO]: Rank[1]: Epoch[333/500], iter[32300] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50186
[2024-02-27 09:50:50,689][PyLogger][INFO]: Rank[0]: Epoch[333/500], iter[32300] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50195
[2024-02-27 09:51:17,272][PyLogger][INFO]: Rank[0]: Epoch[334/500], iter[32350] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50178
[2024-02-27 09:51:17,272][PyLogger][INFO]: Rank[1]: Epoch[334/500], iter[32350] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50083
[2024-02-27 09:51:44,030][PyLogger][INFO]: Rank[1]: Epoch[335/500], iter[32400] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50093
[2024-02-27 09:51:44,030][PyLogger][INFO]: Rank[0]: Epoch[335/500], iter[32400] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50070
[2024-02-27 09:52:10,276][PyLogger][INFO]: Rank[0]: Epoch[335/500], iter[32450] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50156
[2024-02-27 09:52:10,276][PyLogger][INFO]: Rank[1]: Epoch[335/500], iter[32450] - train_accuracy: 0.99974 - val_accuracy: 0.93070 - train_loss: 0.50121 - val_loss: 0.71008 - loss: 0.50380
[2024-02-27 09:53:04,415][PyLogger][INFO]: Rank[0]: Epoch[336/500], iter[32500] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50089
[2024-02-27 09:53:04,415][PyLogger][INFO]: Rank[1]: Epoch[336/500], iter[32500] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50060
[2024-02-27 09:53:30,628][PyLogger][INFO]: Rank[1]: Epoch[336/500], iter[32550] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50570
[2024-02-27 09:53:30,628][PyLogger][INFO]: Rank[0]: Epoch[336/500], iter[32550] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50128
[2024-02-27 09:53:57,311][PyLogger][INFO]: Rank[1]: Epoch[337/500], iter[32600] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50672
[2024-02-27 09:53:57,311][PyLogger][INFO]: Rank[0]: Epoch[337/500], iter[32600] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50091
[2024-02-27 09:54:23,660][PyLogger][INFO]: Rank[1]: Epoch[337/500], iter[32650] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50400
[2024-02-27 09:54:23,660][PyLogger][INFO]: Rank[0]: Epoch[337/500], iter[32650] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50515
[2024-02-27 09:54:50,291][PyLogger][INFO]: Rank[0]: Epoch[338/500], iter[32700] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50058
[2024-02-27 09:54:50,291][PyLogger][INFO]: Rank[1]: Epoch[338/500], iter[32700] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50052
[2024-02-27 09:55:16,430][PyLogger][INFO]: Rank[0]: Epoch[338/500], iter[32750] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50105
[2024-02-27 09:55:16,430][PyLogger][INFO]: Rank[1]: Epoch[338/500], iter[32750] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50046
[2024-02-27 09:55:43,214][PyLogger][INFO]: Rank[1]: Epoch[339/500], iter[32800] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50082
[2024-02-27 09:55:43,214][PyLogger][INFO]: Rank[0]: Epoch[339/500], iter[32800] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50104
[2024-02-27 09:56:09,525][PyLogger][INFO]: Rank[0]: Epoch[339/500], iter[32850] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50184
[2024-02-27 09:56:09,525][PyLogger][INFO]: Rank[1]: Epoch[339/500], iter[32850] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50470
[2024-02-27 09:56:36,531][PyLogger][INFO]: Rank[1]: Epoch[340/500], iter[32900] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50105
[2024-02-27 09:56:36,531][PyLogger][INFO]: Rank[0]: Epoch[340/500], iter[32900] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50090
[2024-02-27 09:57:02,757][PyLogger][INFO]: Rank[0]: Epoch[340/500], iter[32950] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50090
[2024-02-27 09:57:02,757][PyLogger][INFO]: Rank[1]: Epoch[340/500], iter[32950] - train_accuracy: 0.99960 - val_accuracy: 0.92980 - train_loss: 0.50136 - val_loss: 0.70576 - loss: 0.50619
[2024-02-27 09:57:56,705][PyLogger][INFO]: Rank[0]: Epoch[341/500], iter[33000] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50094
[2024-02-27 09:57:56,705][PyLogger][INFO]: Rank[1]: Epoch[341/500], iter[33000] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50049
[2024-02-27 09:58:23,037][PyLogger][INFO]: Rank[0]: Epoch[341/500], iter[33050] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50469
[2024-02-27 09:58:23,037][PyLogger][INFO]: Rank[1]: Epoch[341/500], iter[33050] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50050
[2024-02-27 09:58:49,725][PyLogger][INFO]: Rank[1]: Epoch[342/500], iter[33100] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50054
[2024-02-27 09:58:49,725][PyLogger][INFO]: Rank[0]: Epoch[342/500], iter[33100] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50055
[2024-02-27 09:59:16,005][PyLogger][INFO]: Rank[0]: Epoch[342/500], iter[33150] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50099
[2024-02-27 09:59:16,005][PyLogger][INFO]: Rank[1]: Epoch[342/500], iter[33150] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50134
[2024-02-27 09:59:42,917][PyLogger][INFO]: Rank[0]: Epoch[343/500], iter[33200] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.51135
[2024-02-27 09:59:42,917][PyLogger][INFO]: Rank[1]: Epoch[343/500], iter[33200] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50088
[2024-02-27 10:00:09,167][PyLogger][INFO]: Rank[1]: Epoch[343/500], iter[33250] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50052
[2024-02-27 10:00:09,167][PyLogger][INFO]: Rank[0]: Epoch[343/500], iter[33250] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50115
[2024-02-27 10:00:35,814][PyLogger][INFO]: Rank[0]: Epoch[344/500], iter[33300] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50108
[2024-02-27 10:00:35,814][PyLogger][INFO]: Rank[1]: Epoch[344/500], iter[33300] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50100
[2024-02-27 10:01:02,128][PyLogger][INFO]: Rank[1]: Epoch[344/500], iter[33350] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50055
[2024-02-27 10:01:02,128][PyLogger][INFO]: Rank[0]: Epoch[344/500], iter[33350] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.51212
[2024-02-27 10:01:28,860][PyLogger][INFO]: Rank[1]: Epoch[345/500], iter[33400] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50059
[2024-02-27 10:01:28,860][PyLogger][INFO]: Rank[0]: Epoch[345/500], iter[33400] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50168
[2024-02-27 10:01:54,992][PyLogger][INFO]: Rank[0]: Epoch[345/500], iter[33450] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50150
[2024-02-27 10:01:54,992][PyLogger][INFO]: Rank[1]: Epoch[345/500], iter[33450] - train_accuracy: 0.99962 - val_accuracy: 0.92840 - train_loss: 0.50143 - val_loss: 0.71665 - loss: 0.50128
[2024-02-27 10:02:49,115][PyLogger][INFO]: Rank[0]: Epoch[346/500], iter[33500] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50053
[2024-02-27 10:02:49,115][PyLogger][INFO]: Rank[1]: Epoch[346/500], iter[33500] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50055
[2024-02-27 10:03:15,258][PyLogger][INFO]: Rank[1]: Epoch[346/500], iter[33550] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50083
[2024-02-27 10:03:15,258][PyLogger][INFO]: Rank[0]: Epoch[346/500], iter[33550] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50968
[2024-02-27 10:03:41,975][PyLogger][INFO]: Rank[0]: Epoch[347/500], iter[33600] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50048
[2024-02-27 10:03:41,975][PyLogger][INFO]: Rank[1]: Epoch[347/500], iter[33600] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50074
[2024-02-27 10:04:08,281][PyLogger][INFO]: Rank[0]: Epoch[347/500], iter[33650] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50935
[2024-02-27 10:04:08,281][PyLogger][INFO]: Rank[1]: Epoch[347/500], iter[33650] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50062
[2024-02-27 10:04:34,910][PyLogger][INFO]: Rank[1]: Epoch[348/500], iter[33700] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50092
[2024-02-27 10:04:34,910][PyLogger][INFO]: Rank[0]: Epoch[348/500], iter[33700] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50139
[2024-02-27 10:05:01,130][PyLogger][INFO]: Rank[0]: Epoch[348/500], iter[33750] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50060
[2024-02-27 10:05:01,130][PyLogger][INFO]: Rank[1]: Epoch[348/500], iter[33750] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.51080
[2024-02-27 10:05:27,881][PyLogger][INFO]: Rank[1]: Epoch[349/500], iter[33800] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50049
[2024-02-27 10:05:27,881][PyLogger][INFO]: Rank[0]: Epoch[349/500], iter[33800] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50356
[2024-02-27 10:05:54,074][PyLogger][INFO]: Rank[1]: Epoch[349/500], iter[33850] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50086
[2024-02-27 10:05:54,074][PyLogger][INFO]: Rank[0]: Epoch[349/500], iter[33850] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50054
[2024-02-27 10:06:20,753][PyLogger][INFO]: Rank[1]: Epoch[350/500], iter[33900] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50057
[2024-02-27 10:06:20,753][PyLogger][INFO]: Rank[0]: Epoch[350/500], iter[33900] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50087
[2024-02-27 10:06:46,935][PyLogger][INFO]: Rank[0]: Epoch[350/500], iter[33950] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50284
[2024-02-27 10:06:46,935][PyLogger][INFO]: Rank[1]: Epoch[350/500], iter[33950] - train_accuracy: 0.99960 - val_accuracy: 0.93140 - train_loss: 0.50139 - val_loss: 0.70300 - loss: 0.50046
[2024-02-27 10:07:41,060][PyLogger][INFO]: Rank[1]: Epoch[351/500], iter[34000] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50044
[2024-02-27 10:07:41,060][PyLogger][INFO]: Rank[0]: Epoch[351/500], iter[34000] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50634
[2024-02-27 10:08:07,966][PyLogger][INFO]: Rank[0]: Epoch[352/500], iter[34050] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50060
[2024-02-27 10:08:07,966][PyLogger][INFO]: Rank[1]: Epoch[352/500], iter[34050] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50049
[2024-02-27 10:08:34,244][PyLogger][INFO]: Rank[1]: Epoch[352/500], iter[34100] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50058
[2024-02-27 10:08:34,244][PyLogger][INFO]: Rank[0]: Epoch[352/500], iter[34100] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50049
[2024-02-27 10:09:01,041][PyLogger][INFO]: Rank[1]: Epoch[353/500], iter[34150] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50048
[2024-02-27 10:09:01,041][PyLogger][INFO]: Rank[0]: Epoch[353/500], iter[34150] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50089
[2024-02-27 10:09:27,327][PyLogger][INFO]: Rank[0]: Epoch[353/500], iter[34200] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50170
[2024-02-27 10:09:27,327][PyLogger][INFO]: Rank[1]: Epoch[353/500], iter[34200] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50053
[2024-02-27 10:09:53,959][PyLogger][INFO]: Rank[1]: Epoch[354/500], iter[34250] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50052
[2024-02-27 10:09:53,959][PyLogger][INFO]: Rank[0]: Epoch[354/500], iter[34250] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50056
[2024-02-27 10:10:20,145][PyLogger][INFO]: Rank[1]: Epoch[354/500], iter[34300] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50054
[2024-02-27 10:10:20,145][PyLogger][INFO]: Rank[0]: Epoch[354/500], iter[34300] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50050
[2024-02-27 10:10:47,157][PyLogger][INFO]: Rank[0]: Epoch[355/500], iter[34350] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50127
[2024-02-27 10:10:47,157][PyLogger][INFO]: Rank[1]: Epoch[355/500], iter[34350] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50050
[2024-02-27 10:11:13,310][PyLogger][INFO]: Rank[1]: Epoch[355/500], iter[34400] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50051
[2024-02-27 10:11:13,310][PyLogger][INFO]: Rank[0]: Epoch[355/500], iter[34400] - train_accuracy: 0.99980 - val_accuracy: 0.92930 - train_loss: 0.50089 - val_loss: 0.71021 - loss: 0.50048
[2024-02-27 10:12:07,350][PyLogger][INFO]: Rank[0]: Epoch[356/500], iter[34450] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50054
[2024-02-27 10:12:07,350][PyLogger][INFO]: Rank[1]: Epoch[356/500], iter[34450] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50048
[2024-02-27 10:12:33,481][PyLogger][INFO]: Rank[0]: Epoch[356/500], iter[34500] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50042
[2024-02-27 10:12:33,481][PyLogger][INFO]: Rank[1]: Epoch[356/500], iter[34500] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50056
[2024-02-27 10:13:00,465][PyLogger][INFO]: Rank[0]: Epoch[357/500], iter[34550] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50059
[2024-02-27 10:13:00,465][PyLogger][INFO]: Rank[1]: Epoch[357/500], iter[34550] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50050
[2024-02-27 10:13:26,749][PyLogger][INFO]: Rank[1]: Epoch[357/500], iter[34600] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50050
[2024-02-27 10:13:26,749][PyLogger][INFO]: Rank[0]: Epoch[357/500], iter[34600] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50054
[2024-02-27 10:13:53,343][PyLogger][INFO]: Rank[1]: Epoch[358/500], iter[34650] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50054
[2024-02-27 10:13:53,343][PyLogger][INFO]: Rank[0]: Epoch[358/500], iter[34650] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50048
[2024-02-27 10:14:19,456][PyLogger][INFO]: Rank[0]: Epoch[358/500], iter[34700] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50044
[2024-02-27 10:14:19,456][PyLogger][INFO]: Rank[1]: Epoch[358/500], iter[34700] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50046
[2024-02-27 10:14:46,182][PyLogger][INFO]: Rank[1]: Epoch[359/500], iter[34750] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50038
[2024-02-27 10:14:46,182][PyLogger][INFO]: Rank[0]: Epoch[359/500], iter[34750] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50127
[2024-02-27 10:15:12,468][PyLogger][INFO]: Rank[0]: Epoch[359/500], iter[34800] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50051
[2024-02-27 10:15:12,468][PyLogger][INFO]: Rank[1]: Epoch[359/500], iter[34800] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50041
[2024-02-27 10:15:39,143][PyLogger][INFO]: Rank[1]: Epoch[360/500], iter[34850] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50045
[2024-02-27 10:15:39,143][PyLogger][INFO]: Rank[0]: Epoch[360/500], iter[34850] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50057
[2024-02-27 10:16:05,364][PyLogger][INFO]: Rank[1]: Epoch[360/500], iter[34900] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50044
[2024-02-27 10:16:05,364][PyLogger][INFO]: Rank[0]: Epoch[360/500], iter[34900] - train_accuracy: 0.99974 - val_accuracy: 0.93040 - train_loss: 0.50107 - val_loss: 0.71544 - loss: 0.50046
[2024-02-27 10:16:59,284][PyLogger][INFO]: Rank[1]: Epoch[361/500], iter[34950] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50312
[2024-02-27 10:16:59,284][PyLogger][INFO]: Rank[0]: Epoch[361/500], iter[34950] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50051
[2024-02-27 10:17:25,615][PyLogger][INFO]: Rank[1]: Epoch[361/500], iter[35000] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50046
[2024-02-27 10:17:25,615][PyLogger][INFO]: Rank[0]: Epoch[361/500], iter[35000] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50048
[2024-02-27 10:17:52,150][PyLogger][INFO]: Rank[1]: Epoch[362/500], iter[35050] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50055
[2024-02-27 10:17:52,150][PyLogger][INFO]: Rank[0]: Epoch[362/500], iter[35050] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50042
[2024-02-27 10:18:18,341][PyLogger][INFO]: Rank[1]: Epoch[362/500], iter[35100] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50040
[2024-02-27 10:18:18,341][PyLogger][INFO]: Rank[0]: Epoch[362/500], iter[35100] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50275
[2024-02-27 10:18:45,016][PyLogger][INFO]: Rank[0]: Epoch[363/500], iter[35150] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50050
[2024-02-27 10:18:45,016][PyLogger][INFO]: Rank[1]: Epoch[363/500], iter[35150] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50258
[2024-02-27 10:19:11,290][PyLogger][INFO]: Rank[1]: Epoch[363/500], iter[35200] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50040
[2024-02-27 10:19:11,290][PyLogger][INFO]: Rank[0]: Epoch[363/500], iter[35200] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50061
[2024-02-27 10:19:38,127][PyLogger][INFO]: Rank[1]: Epoch[364/500], iter[35250] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50044
[2024-02-27 10:19:38,127][PyLogger][INFO]: Rank[0]: Epoch[364/500], iter[35250] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50043
[2024-02-27 10:20:04,345][PyLogger][INFO]: Rank[0]: Epoch[364/500], iter[35300] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50049
[2024-02-27 10:20:04,345][PyLogger][INFO]: Rank[1]: Epoch[364/500], iter[35300] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50056
[2024-02-27 10:20:31,033][PyLogger][INFO]: Rank[1]: Epoch[365/500], iter[35350] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50052
[2024-02-27 10:20:31,033][PyLogger][INFO]: Rank[0]: Epoch[365/500], iter[35350] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50079
[2024-02-27 10:20:57,317][PyLogger][INFO]: Rank[0]: Epoch[365/500], iter[35400] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50127
[2024-02-27 10:20:57,317][PyLogger][INFO]: Rank[1]: Epoch[365/500], iter[35400] - train_accuracy: 1.00000 - val_accuracy: 0.93320 - train_loss: 0.50044 - val_loss: 0.70786 - loss: 0.50043
[2024-02-27 10:21:51,531][PyLogger][INFO]: Rank[0]: Epoch[366/500], iter[35450] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50180
[2024-02-27 10:21:51,531][PyLogger][INFO]: Rank[1]: Epoch[366/500], iter[35450] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50053
[2024-02-27 10:22:17,816][PyLogger][INFO]: Rank[1]: Epoch[366/500], iter[35500] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50041
[2024-02-27 10:22:17,816][PyLogger][INFO]: Rank[0]: Epoch[366/500], iter[35500] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50038
[2024-02-27 10:22:44,459][PyLogger][INFO]: Rank[0]: Epoch[367/500], iter[35550] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50042
[2024-02-27 10:22:44,458][PyLogger][INFO]: Rank[1]: Epoch[367/500], iter[35550] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50043
[2024-02-27 10:23:11,114][PyLogger][INFO]: Rank[1]: Epoch[368/500], iter[35600] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50067
[2024-02-27 10:23:11,114][PyLogger][INFO]: Rank[0]: Epoch[368/500], iter[35600] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50047
[2024-02-27 10:23:37,346][PyLogger][INFO]: Rank[1]: Epoch[368/500], iter[35650] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50042
[2024-02-27 10:23:37,347][PyLogger][INFO]: Rank[0]: Epoch[368/500], iter[35650] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50040
[2024-02-27 10:24:04,126][PyLogger][INFO]: Rank[1]: Epoch[369/500], iter[35700] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50042
[2024-02-27 10:24:04,126][PyLogger][INFO]: Rank[0]: Epoch[369/500], iter[35700] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50044
[2024-02-27 10:24:30,383][PyLogger][INFO]: Rank[1]: Epoch[369/500], iter[35750] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50040
[2024-02-27 10:24:30,383][PyLogger][INFO]: Rank[0]: Epoch[369/500], iter[35750] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50041
[2024-02-27 10:24:57,107][PyLogger][INFO]: Rank[0]: Epoch[370/500], iter[35800] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50043
[2024-02-27 10:24:57,107][PyLogger][INFO]: Rank[1]: Epoch[370/500], iter[35800] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50054
[2024-02-27 10:25:23,290][PyLogger][INFO]: Rank[1]: Epoch[370/500], iter[35850] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50042
[2024-02-27 10:25:23,290][PyLogger][INFO]: Rank[0]: Epoch[370/500], iter[35850] - train_accuracy: 0.99964 - val_accuracy: 0.92920 - train_loss: 0.50133 - val_loss: 0.71907 - loss: 0.50040
[2024-02-27 10:26:17,314][PyLogger][INFO]: Rank[1]: Epoch[371/500], iter[35900] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50040
[2024-02-27 10:26:17,315][PyLogger][INFO]: Rank[0]: Epoch[371/500], iter[35900] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50046
[2024-02-27 10:26:43,609][PyLogger][INFO]: Rank[0]: Epoch[371/500], iter[35950] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50053
[2024-02-27 10:26:43,609][PyLogger][INFO]: Rank[1]: Epoch[371/500], iter[35950] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50060
[2024-02-27 10:27:10,234][PyLogger][INFO]: Rank[1]: Epoch[372/500], iter[36000] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50043
[2024-02-27 10:27:10,234][PyLogger][INFO]: Rank[0]: Epoch[372/500], iter[36000] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50045
[2024-02-27 10:27:36,376][PyLogger][INFO]: Rank[1]: Epoch[372/500], iter[36050] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50256
[2024-02-27 10:27:36,376][PyLogger][INFO]: Rank[0]: Epoch[372/500], iter[36050] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50040
[2024-02-27 10:28:02,921][PyLogger][INFO]: Rank[0]: Epoch[373/500], iter[36100] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50044
[2024-02-27 10:28:02,921][PyLogger][INFO]: Rank[1]: Epoch[373/500], iter[36100] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50041
[2024-02-27 10:28:29,145][PyLogger][INFO]: Rank[1]: Epoch[373/500], iter[36150] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50484
[2024-02-27 10:28:29,145][PyLogger][INFO]: Rank[0]: Epoch[373/500], iter[36150] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50039
[2024-02-27 10:28:55,863][PyLogger][INFO]: Rank[0]: Epoch[374/500], iter[36200] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50045
[2024-02-27 10:28:55,863][PyLogger][INFO]: Rank[1]: Epoch[374/500], iter[36200] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50042
[2024-02-27 10:29:22,177][PyLogger][INFO]: Rank[1]: Epoch[374/500], iter[36250] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50039
[2024-02-27 10:29:22,177][PyLogger][INFO]: Rank[0]: Epoch[374/500], iter[36250] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50328
[2024-02-27 10:29:49,218][PyLogger][INFO]: Rank[0]: Epoch[375/500], iter[36300] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50044
[2024-02-27 10:29:49,218][PyLogger][INFO]: Rank[1]: Epoch[375/500], iter[36300] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50038
[2024-02-27 10:30:15,451][PyLogger][INFO]: Rank[1]: Epoch[375/500], iter[36350] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50039
[2024-02-27 10:30:15,451][PyLogger][INFO]: Rank[0]: Epoch[375/500], iter[36350] - train_accuracy: 0.99998 - val_accuracy: 0.93330 - train_loss: 0.50042 - val_loss: 0.70312 - loss: 0.50040
[2024-02-27 10:31:09,249][PyLogger][INFO]: Rank[0]: Epoch[376/500], iter[36400] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50046
[2024-02-27 10:31:09,249][PyLogger][INFO]: Rank[1]: Epoch[376/500], iter[36400] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50040
[2024-02-27 10:31:35,439][PyLogger][INFO]: Rank[1]: Epoch[376/500], iter[36450] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50042
[2024-02-27 10:31:35,439][PyLogger][INFO]: Rank[0]: Epoch[376/500], iter[36450] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50044
[2024-02-27 10:32:02,092][PyLogger][INFO]: Rank[1]: Epoch[377/500], iter[36500] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50054
[2024-02-27 10:32:02,093][PyLogger][INFO]: Rank[0]: Epoch[377/500], iter[36500] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50048
[2024-02-27 10:32:28,200][PyLogger][INFO]: Rank[1]: Epoch[377/500], iter[36550] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50042
[2024-02-27 10:32:28,200][PyLogger][INFO]: Rank[0]: Epoch[377/500], iter[36550] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50040
[2024-02-27 10:32:54,828][PyLogger][INFO]: Rank[1]: Epoch[378/500], iter[36600] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50039
[2024-02-27 10:32:54,828][PyLogger][INFO]: Rank[0]: Epoch[378/500], iter[36600] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50041
[2024-02-27 10:33:20,994][PyLogger][INFO]: Rank[0]: Epoch[378/500], iter[36650] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50041
[2024-02-27 10:33:20,994][PyLogger][INFO]: Rank[1]: Epoch[378/500], iter[36650] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50037
[2024-02-27 10:33:47,703][PyLogger][INFO]: Rank[1]: Epoch[379/500], iter[36700] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50045
[2024-02-27 10:33:47,703][PyLogger][INFO]: Rank[0]: Epoch[379/500], iter[36700] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50039
[2024-02-27 10:34:14,029][PyLogger][INFO]: Rank[1]: Epoch[379/500], iter[36750] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50036
[2024-02-27 10:34:14,030][PyLogger][INFO]: Rank[0]: Epoch[379/500], iter[36750] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50041
[2024-02-27 10:34:40,812][PyLogger][INFO]: Rank[1]: Epoch[380/500], iter[36800] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50053
[2024-02-27 10:34:40,813][PyLogger][INFO]: Rank[0]: Epoch[380/500], iter[36800] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50038
[2024-02-27 10:35:06,990][PyLogger][INFO]: Rank[1]: Epoch[380/500], iter[36850] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50043
[2024-02-27 10:35:06,990][PyLogger][INFO]: Rank[0]: Epoch[380/500], iter[36850] - train_accuracy: 0.99992 - val_accuracy: 0.93110 - train_loss: 0.50050 - val_loss: 0.70829 - loss: 0.50041
[2024-02-27 10:36:01,009][PyLogger][INFO]: Rank[0]: Epoch[381/500], iter[36900] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50045
[2024-02-27 10:36:01,009][PyLogger][INFO]: Rank[1]: Epoch[381/500], iter[36900] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50039
[2024-02-27 10:36:27,237][PyLogger][INFO]: Rank[0]: Epoch[381/500], iter[36950] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50047
[2024-02-27 10:36:27,237][PyLogger][INFO]: Rank[1]: Epoch[381/500], iter[36950] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50041
[2024-02-27 10:36:53,998][PyLogger][INFO]: Rank[0]: Epoch[382/500], iter[37000] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50038
[2024-02-27 10:36:53,998][PyLogger][INFO]: Rank[1]: Epoch[382/500], iter[37000] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50038
[2024-02-27 10:37:20,261][PyLogger][INFO]: Rank[1]: Epoch[382/500], iter[37050] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50044
[2024-02-27 10:37:20,261][PyLogger][INFO]: Rank[0]: Epoch[382/500], iter[37050] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50068
[2024-02-27 10:37:46,974][PyLogger][INFO]: Rank[1]: Epoch[383/500], iter[37100] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50036
[2024-02-27 10:37:46,974][PyLogger][INFO]: Rank[0]: Epoch[383/500], iter[37100] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50038
[2024-02-27 10:38:13,138][PyLogger][INFO]: Rank[1]: Epoch[383/500], iter[37150] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50040
[2024-02-27 10:38:13,139][PyLogger][INFO]: Rank[0]: Epoch[383/500], iter[37150] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50041
[2024-02-27 10:38:39,910][PyLogger][INFO]: Rank[0]: Epoch[384/500], iter[37200] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50038
[2024-02-27 10:38:39,910][PyLogger][INFO]: Rank[1]: Epoch[384/500], iter[37200] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50040
[2024-02-27 10:39:06,620][PyLogger][INFO]: Rank[1]: Epoch[385/500], iter[37250] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50040
[2024-02-27 10:39:06,620][PyLogger][INFO]: Rank[0]: Epoch[385/500], iter[37250] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50043
[2024-02-27 10:39:32,989][PyLogger][INFO]: Rank[0]: Epoch[385/500], iter[37300] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50048
[2024-02-27 10:39:32,989][PyLogger][INFO]: Rank[1]: Epoch[385/500], iter[37300] - train_accuracy: 0.99998 - val_accuracy: 0.93310 - train_loss: 0.50041 - val_loss: 0.70547 - loss: 0.50038
[2024-02-27 10:40:27,181][PyLogger][INFO]: Rank[0]: Epoch[386/500], iter[37350] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50036
[2024-02-27 10:40:27,181][PyLogger][INFO]: Rank[1]: Epoch[386/500], iter[37350] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50041
[2024-02-27 10:40:53,331][PyLogger][INFO]: Rank[1]: Epoch[386/500], iter[37400] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50039
[2024-02-27 10:40:53,331][PyLogger][INFO]: Rank[0]: Epoch[386/500], iter[37400] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50046
[2024-02-27 10:41:20,062][PyLogger][INFO]: Rank[1]: Epoch[387/500], iter[37450] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50037
[2024-02-27 10:41:20,062][PyLogger][INFO]: Rank[0]: Epoch[387/500], iter[37450] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50040
[2024-02-27 10:41:46,266][PyLogger][INFO]: Rank[1]: Epoch[387/500], iter[37500] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50042
[2024-02-27 10:41:46,266][PyLogger][INFO]: Rank[0]: Epoch[387/500], iter[37500] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50126
[2024-02-27 10:42:13,161][PyLogger][INFO]: Rank[0]: Epoch[388/500], iter[37550] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50036
[2024-02-27 10:42:13,161][PyLogger][INFO]: Rank[1]: Epoch[388/500], iter[37550] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50037
[2024-02-27 10:42:39,354][PyLogger][INFO]: Rank[1]: Epoch[388/500], iter[37600] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50038
[2024-02-27 10:42:39,354][PyLogger][INFO]: Rank[0]: Epoch[388/500], iter[37600] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50037
[2024-02-27 10:43:05,934][PyLogger][INFO]: Rank[0]: Epoch[389/500], iter[37650] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50043
[2024-02-27 10:43:05,934][PyLogger][INFO]: Rank[1]: Epoch[389/500], iter[37650] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50042
[2024-02-27 10:43:32,173][PyLogger][INFO]: Rank[1]: Epoch[389/500], iter[37700] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50040
[2024-02-27 10:43:32,173][PyLogger][INFO]: Rank[0]: Epoch[389/500], iter[37700] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50045
[2024-02-27 10:43:58,752][PyLogger][INFO]: Rank[0]: Epoch[390/500], iter[37750] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50052
[2024-02-27 10:43:58,752][PyLogger][INFO]: Rank[1]: Epoch[390/500], iter[37750] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50082
[2024-02-27 10:44:24,874][PyLogger][INFO]: Rank[1]: Epoch[390/500], iter[37800] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50039
[2024-02-27 10:44:24,874][PyLogger][INFO]: Rank[0]: Epoch[390/500], iter[37800] - train_accuracy: 0.99994 - val_accuracy: 0.93440 - train_loss: 0.50042 - val_loss: 0.70299 - loss: 0.50045
[2024-02-27 10:45:18,810][PyLogger][INFO]: Rank[1]: Epoch[391/500], iter[37850] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50038
[2024-02-27 10:45:18,810][PyLogger][INFO]: Rank[0]: Epoch[391/500], iter[37850] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50046
[2024-02-27 10:45:45,155][PyLogger][INFO]: Rank[0]: Epoch[391/500], iter[37900] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50037
[2024-02-27 10:45:45,155][PyLogger][INFO]: Rank[1]: Epoch[391/500], iter[37900] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50039
[2024-02-27 10:46:11,761][PyLogger][INFO]: Rank[1]: Epoch[392/500], iter[37950] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50039
[2024-02-27 10:46:11,761][PyLogger][INFO]: Rank[0]: Epoch[392/500], iter[37950] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50039
[2024-02-27 10:46:37,893][PyLogger][INFO]: Rank[1]: Epoch[392/500], iter[38000] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50041
[2024-02-27 10:46:37,893][PyLogger][INFO]: Rank[0]: Epoch[392/500], iter[38000] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50040
[2024-02-27 10:47:04,866][PyLogger][INFO]: Rank[1]: Epoch[393/500], iter[38050] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50359
[2024-02-27 10:47:04,866][PyLogger][INFO]: Rank[0]: Epoch[393/500], iter[38050] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50038
[2024-02-27 10:47:31,058][PyLogger][INFO]: Rank[1]: Epoch[393/500], iter[38100] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50041
[2024-02-27 10:47:31,058][PyLogger][INFO]: Rank[0]: Epoch[393/500], iter[38100] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50045
[2024-02-27 10:47:57,707][PyLogger][INFO]: Rank[1]: Epoch[394/500], iter[38150] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50037
[2024-02-27 10:47:57,707][PyLogger][INFO]: Rank[0]: Epoch[394/500], iter[38150] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50037
[2024-02-27 10:48:23,916][PyLogger][INFO]: Rank[0]: Epoch[394/500], iter[38200] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50040
[2024-02-27 10:48:23,916][PyLogger][INFO]: Rank[1]: Epoch[394/500], iter[38200] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50042
[2024-02-27 10:48:50,639][PyLogger][INFO]: Rank[1]: Epoch[395/500], iter[38250] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50038
[2024-02-27 10:48:50,639][PyLogger][INFO]: Rank[0]: Epoch[395/500], iter[38250] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50048
[2024-02-27 10:49:16,932][PyLogger][INFO]: Rank[1]: Epoch[395/500], iter[38300] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50040
[2024-02-27 10:49:16,933][PyLogger][INFO]: Rank[0]: Epoch[395/500], iter[38300] - train_accuracy: 1.00000 - val_accuracy: 0.93670 - train_loss: 0.50035 - val_loss: 0.69899 - loss: 0.50040
[2024-02-27 10:50:10,988][PyLogger][INFO]: Rank[0]: Epoch[396/500], iter[38350] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50038
[2024-02-27 10:50:10,988][PyLogger][INFO]: Rank[1]: Epoch[396/500], iter[38350] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50041
[2024-02-27 10:50:37,318][PyLogger][INFO]: Rank[1]: Epoch[396/500], iter[38400] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50038
[2024-02-27 10:50:37,318][PyLogger][INFO]: Rank[0]: Epoch[396/500], iter[38400] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50037
[2024-02-27 10:51:03,990][PyLogger][INFO]: Rank[0]: Epoch[397/500], iter[38450] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50037
[2024-02-27 10:51:03,990][PyLogger][INFO]: Rank[1]: Epoch[397/500], iter[38450] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50037
[2024-02-27 10:51:30,017][PyLogger][INFO]: Rank[0]: Epoch[397/500], iter[38500] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50038
[2024-02-27 10:51:30,017][PyLogger][INFO]: Rank[1]: Epoch[397/500], iter[38500] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50036
[2024-02-27 10:51:56,766][PyLogger][INFO]: Rank[1]: Epoch[398/500], iter[38550] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50037
[2024-02-27 10:51:56,766][PyLogger][INFO]: Rank[0]: Epoch[398/500], iter[38550] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50045
[2024-02-27 10:52:22,896][PyLogger][INFO]: Rank[0]: Epoch[398/500], iter[38600] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50424
[2024-02-27 10:52:22,896][PyLogger][INFO]: Rank[1]: Epoch[398/500], iter[38600] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50035
[2024-02-27 10:52:49,515][PyLogger][INFO]: Rank[1]: Epoch[399/500], iter[38650] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50045
[2024-02-27 10:52:49,515][PyLogger][INFO]: Rank[0]: Epoch[399/500], iter[38650] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50045
[2024-02-27 10:53:15,809][PyLogger][INFO]: Rank[1]: Epoch[399/500], iter[38700] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50036
[2024-02-27 10:53:15,809][PyLogger][INFO]: Rank[0]: Epoch[399/500], iter[38700] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50039
[2024-02-27 10:53:42,324][PyLogger][INFO]: Rank[0]: Epoch[400/500], iter[38750] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50039
[2024-02-27 10:53:42,324][PyLogger][INFO]: Rank[1]: Epoch[400/500], iter[38750] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50037
[2024-02-27 10:54:08,517][PyLogger][INFO]: Rank[1]: Epoch[400/500], iter[38800] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50036
[2024-02-27 10:54:08,517][PyLogger][INFO]: Rank[0]: Epoch[400/500], iter[38800] - train_accuracy: 0.99996 - val_accuracy: 0.93380 - train_loss: 0.50041 - val_loss: 0.70000 - loss: 0.50038
[2024-02-27 10:55:02,566][PyLogger][INFO]: Rank[0]: Epoch[401/500], iter[38850] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50041
[2024-02-27 10:55:02,566][PyLogger][INFO]: Rank[1]: Epoch[401/500], iter[38850] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50038
[2024-02-27 10:55:29,218][PyLogger][INFO]: Rank[1]: Epoch[402/500], iter[38900] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50039
[2024-02-27 10:55:29,218][PyLogger][INFO]: Rank[0]: Epoch[402/500], iter[38900] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50037
[2024-02-27 10:55:55,394][PyLogger][INFO]: Rank[0]: Epoch[402/500], iter[38950] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50042
[2024-02-27 10:55:55,394][PyLogger][INFO]: Rank[1]: Epoch[402/500], iter[38950] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50037
[2024-02-27 10:56:22,019][PyLogger][INFO]: Rank[1]: Epoch[403/500], iter[39000] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50037
[2024-02-27 10:56:22,019][PyLogger][INFO]: Rank[0]: Epoch[403/500], iter[39000] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50039
[2024-02-27 10:56:48,198][PyLogger][INFO]: Rank[0]: Epoch[403/500], iter[39050] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50045
[2024-02-27 10:56:48,198][PyLogger][INFO]: Rank[1]: Epoch[403/500], iter[39050] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50035
[2024-02-27 10:57:14,803][PyLogger][INFO]: Rank[0]: Epoch[404/500], iter[39100] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50034
[2024-02-27 10:57:14,803][PyLogger][INFO]: Rank[1]: Epoch[404/500], iter[39100] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50108
[2024-02-27 10:57:41,061][PyLogger][INFO]: Rank[0]: Epoch[404/500], iter[39150] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50036
[2024-02-27 10:57:41,061][PyLogger][INFO]: Rank[1]: Epoch[404/500], iter[39150] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50040
[2024-02-27 10:58:07,781][PyLogger][INFO]: Rank[0]: Epoch[405/500], iter[39200] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50037
[2024-02-27 10:58:07,781][PyLogger][INFO]: Rank[1]: Epoch[405/500], iter[39200] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50037
[2024-02-27 10:58:33,955][PyLogger][INFO]: Rank[0]: Epoch[405/500], iter[39250] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50035
[2024-02-27 10:58:33,955][PyLogger][INFO]: Rank[1]: Epoch[405/500], iter[39250] - train_accuracy: 0.99998 - val_accuracy: 0.93380 - train_loss: 0.50038 - val_loss: 0.69957 - loss: 0.50040
[2024-02-27 10:59:28,008][PyLogger][INFO]: Rank[0]: Epoch[406/500], iter[39300] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50040
[2024-02-27 10:59:28,008][PyLogger][INFO]: Rank[1]: Epoch[406/500], iter[39300] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50039
[2024-02-27 10:59:54,186][PyLogger][INFO]: Rank[1]: Epoch[406/500], iter[39350] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50036
[2024-02-27 10:59:54,186][PyLogger][INFO]: Rank[0]: Epoch[406/500], iter[39350] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50038
[2024-02-27 11:00:20,799][PyLogger][INFO]: Rank[1]: Epoch[407/500], iter[39400] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50045
[2024-02-27 11:00:20,800][PyLogger][INFO]: Rank[0]: Epoch[407/500], iter[39400] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50038
[2024-02-27 11:00:46,908][PyLogger][INFO]: Rank[1]: Epoch[407/500], iter[39450] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50049
[2024-02-27 11:00:46,908][PyLogger][INFO]: Rank[0]: Epoch[407/500], iter[39450] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50038
[2024-02-27 11:01:13,567][PyLogger][INFO]: Rank[1]: Epoch[408/500], iter[39500] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50036
[2024-02-27 11:01:13,567][PyLogger][INFO]: Rank[0]: Epoch[408/500], iter[39500] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50036
[2024-02-27 11:01:39,709][PyLogger][INFO]: Rank[0]: Epoch[408/500], iter[39550] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50036
[2024-02-27 11:01:39,709][PyLogger][INFO]: Rank[1]: Epoch[408/500], iter[39550] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50039
[2024-02-27 11:02:06,369][PyLogger][INFO]: Rank[1]: Epoch[409/500], iter[39600] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50039
[2024-02-27 11:02:06,369][PyLogger][INFO]: Rank[0]: Epoch[409/500], iter[39600] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50040
[2024-02-27 11:02:32,497][PyLogger][INFO]: Rank[1]: Epoch[409/500], iter[39650] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50035
[2024-02-27 11:02:32,497][PyLogger][INFO]: Rank[0]: Epoch[409/500], iter[39650] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50035
[2024-02-27 11:02:59,181][PyLogger][INFO]: Rank[0]: Epoch[410/500], iter[39700] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50036
[2024-02-27 11:02:59,181][PyLogger][INFO]: Rank[1]: Epoch[410/500], iter[39700] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50035
[2024-02-27 11:03:25,317][PyLogger][INFO]: Rank[1]: Epoch[410/500], iter[39750] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50036
[2024-02-27 11:03:25,317][PyLogger][INFO]: Rank[0]: Epoch[410/500], iter[39750] - train_accuracy: 1.00000 - val_accuracy: 0.93500 - train_loss: 0.50034 - val_loss: 0.70018 - loss: 0.50039
[2024-02-27 11:04:19,247][PyLogger][INFO]: Rank[1]: Epoch[411/500], iter[39800] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50038
[2024-02-27 11:04:19,247][PyLogger][INFO]: Rank[0]: Epoch[411/500], iter[39800] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50051
[2024-02-27 11:04:45,523][PyLogger][INFO]: Rank[0]: Epoch[411/500], iter[39850] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50036
[2024-02-27 11:04:45,523][PyLogger][INFO]: Rank[1]: Epoch[411/500], iter[39850] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50041
[2024-02-27 11:05:12,169][PyLogger][INFO]: Rank[0]: Epoch[412/500], iter[39900] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50035
[2024-02-27 11:05:12,169][PyLogger][INFO]: Rank[1]: Epoch[412/500], iter[39900] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50039
[2024-02-27 11:05:38,382][PyLogger][INFO]: Rank[1]: Epoch[412/500], iter[39950] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50035
[2024-02-27 11:05:38,382][PyLogger][INFO]: Rank[0]: Epoch[412/500], iter[39950] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50036
[2024-02-27 11:06:05,040][PyLogger][INFO]: Rank[0]: Epoch[413/500], iter[40000] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50039
[2024-02-27 11:06:05,040][PyLogger][INFO]: Rank[1]: Epoch[413/500], iter[40000] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50038
[2024-02-27 11:06:31,325][PyLogger][INFO]: Rank[1]: Epoch[413/500], iter[40050] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50039
[2024-02-27 11:06:31,325][PyLogger][INFO]: Rank[0]: Epoch[413/500], iter[40050] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50039
[2024-02-27 11:06:57,939][PyLogger][INFO]: Rank[0]: Epoch[414/500], iter[40100] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50057
[2024-02-27 11:06:57,939][PyLogger][INFO]: Rank[1]: Epoch[414/500], iter[40100] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50037
[2024-02-27 11:07:24,004][PyLogger][INFO]: Rank[0]: Epoch[414/500], iter[40150] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50036
[2024-02-27 11:07:24,004][PyLogger][INFO]: Rank[1]: Epoch[414/500], iter[40150] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50037
[2024-02-27 11:07:50,638][PyLogger][INFO]: Rank[1]: Epoch[415/500], iter[40200] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50038
[2024-02-27 11:07:50,638][PyLogger][INFO]: Rank[0]: Epoch[415/500], iter[40200] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50036
[2024-02-27 11:08:16,855][PyLogger][INFO]: Rank[1]: Epoch[415/500], iter[40250] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50035
[2024-02-27 11:08:16,855][PyLogger][INFO]: Rank[0]: Epoch[415/500], iter[40250] - train_accuracy: 1.00000 - val_accuracy: 0.93540 - train_loss: 0.50034 - val_loss: 0.69846 - loss: 0.50036
[2024-02-27 11:09:10,893][PyLogger][INFO]: Rank[0]: Epoch[416/500], iter[40300] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50039
[2024-02-27 11:09:10,893][PyLogger][INFO]: Rank[1]: Epoch[416/500], iter[40300] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50036
[2024-02-27 11:09:37,120][PyLogger][INFO]: Rank[1]: Epoch[416/500], iter[40350] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50035
[2024-02-27 11:09:37,120][PyLogger][INFO]: Rank[0]: Epoch[416/500], iter[40350] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50036
[2024-02-27 11:10:03,813][PyLogger][INFO]: Rank[1]: Epoch[417/500], iter[40400] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50040
[2024-02-27 11:10:03,813][PyLogger][INFO]: Rank[0]: Epoch[417/500], iter[40400] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50035
[2024-02-27 11:10:30,631][PyLogger][INFO]: Rank[0]: Epoch[418/500], iter[40450] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50035
[2024-02-27 11:10:30,631][PyLogger][INFO]: Rank[1]: Epoch[418/500], iter[40450] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50037
[2024-02-27 11:10:56,917][PyLogger][INFO]: Rank[0]: Epoch[418/500], iter[40500] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50037
[2024-02-27 11:10:56,917][PyLogger][INFO]: Rank[1]: Epoch[418/500], iter[40500] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50036
[2024-02-27 11:11:23,555][PyLogger][INFO]: Rank[0]: Epoch[419/500], iter[40550] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50036
[2024-02-27 11:11:23,555][PyLogger][INFO]: Rank[1]: Epoch[419/500], iter[40550] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50036
[2024-02-27 11:11:49,745][PyLogger][INFO]: Rank[1]: Epoch[419/500], iter[40600] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50035
[2024-02-27 11:11:49,745][PyLogger][INFO]: Rank[0]: Epoch[419/500], iter[40600] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50039
[2024-02-27 11:12:16,280][PyLogger][INFO]: Rank[1]: Epoch[420/500], iter[40650] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50038
[2024-02-27 11:12:16,280][PyLogger][INFO]: Rank[0]: Epoch[420/500], iter[40650] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50035
[2024-02-27 11:12:42,365][PyLogger][INFO]: Rank[0]: Epoch[420/500], iter[40700] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50038
[2024-02-27 11:12:42,365][PyLogger][INFO]: Rank[1]: Epoch[420/500], iter[40700] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69716 - loss: 0.50043
[2024-02-27 11:13:36,178][PyLogger][INFO]: Rank[1]: Epoch[421/500], iter[40750] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50051
[2024-02-27 11:13:36,178][PyLogger][INFO]: Rank[0]: Epoch[421/500], iter[40750] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50037
[2024-02-27 11:14:02,540][PyLogger][INFO]: Rank[0]: Epoch[421/500], iter[40800] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50043
[2024-02-27 11:14:02,540][PyLogger][INFO]: Rank[1]: Epoch[421/500], iter[40800] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50035
[2024-02-27 11:14:29,115][PyLogger][INFO]: Rank[1]: Epoch[422/500], iter[40850] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50040
[2024-02-27 11:14:29,115][PyLogger][INFO]: Rank[0]: Epoch[422/500], iter[40850] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50037
[2024-02-27 11:14:55,361][PyLogger][INFO]: Rank[0]: Epoch[422/500], iter[40900] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50036
[2024-02-27 11:14:55,361][PyLogger][INFO]: Rank[1]: Epoch[422/500], iter[40900] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50040
[2024-02-27 11:15:22,274][PyLogger][INFO]: Rank[1]: Epoch[423/500], iter[40950] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50064
[2024-02-27 11:15:22,274][PyLogger][INFO]: Rank[0]: Epoch[423/500], iter[40950] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50036
[2024-02-27 11:15:48,419][PyLogger][INFO]: Rank[1]: Epoch[423/500], iter[41000] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50036
[2024-02-27 11:15:48,419][PyLogger][INFO]: Rank[0]: Epoch[423/500], iter[41000] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50038
[2024-02-27 11:16:15,113][PyLogger][INFO]: Rank[0]: Epoch[424/500], iter[41050] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50037
[2024-02-27 11:16:15,113][PyLogger][INFO]: Rank[1]: Epoch[424/500], iter[41050] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50043
[2024-02-27 11:16:41,258][PyLogger][INFO]: Rank[1]: Epoch[424/500], iter[41100] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50038
[2024-02-27 11:16:41,258][PyLogger][INFO]: Rank[0]: Epoch[424/500], iter[41100] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50035
[2024-02-27 11:17:07,953][PyLogger][INFO]: Rank[1]: Epoch[425/500], iter[41150] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50036
[2024-02-27 11:17:07,954][PyLogger][INFO]: Rank[0]: Epoch[425/500], iter[41150] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50045
[2024-02-27 11:17:34,267][PyLogger][INFO]: Rank[1]: Epoch[425/500], iter[41200] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50038
[2024-02-27 11:17:34,267][PyLogger][INFO]: Rank[0]: Epoch[425/500], iter[41200] - train_accuracy: 1.00000 - val_accuracy: 0.93570 - train_loss: 0.50034 - val_loss: 0.69819 - loss: 0.50035
[2024-02-27 11:18:28,120][PyLogger][INFO]: Rank[0]: Epoch[426/500], iter[41250] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50035
[2024-02-27 11:18:28,120][PyLogger][INFO]: Rank[1]: Epoch[426/500], iter[41250] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50034
[2024-02-27 11:18:54,288][PyLogger][INFO]: Rank[1]: Epoch[426/500], iter[41300] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50037
[2024-02-27 11:18:54,288][PyLogger][INFO]: Rank[0]: Epoch[426/500], iter[41300] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50041
[2024-02-27 11:19:20,794][PyLogger][INFO]: Rank[0]: Epoch[427/500], iter[41350] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50042
[2024-02-27 11:19:20,794][PyLogger][INFO]: Rank[1]: Epoch[427/500], iter[41350] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50035
[2024-02-27 11:19:47,089][PyLogger][INFO]: Rank[1]: Epoch[427/500], iter[41400] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50037
[2024-02-27 11:19:47,089][PyLogger][INFO]: Rank[0]: Epoch[427/500], iter[41400] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50038
[2024-02-27 11:20:13,582][PyLogger][INFO]: Rank[1]: Epoch[428/500], iter[41450] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50035
[2024-02-27 11:20:13,582][PyLogger][INFO]: Rank[0]: Epoch[428/500], iter[41450] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50035
[2024-02-27 11:20:39,776][PyLogger][INFO]: Rank[0]: Epoch[428/500], iter[41500] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50034
[2024-02-27 11:20:39,776][PyLogger][INFO]: Rank[1]: Epoch[428/500], iter[41500] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50037
[2024-02-27 11:21:06,446][PyLogger][INFO]: Rank[1]: Epoch[429/500], iter[41550] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50038
[2024-02-27 11:21:06,446][PyLogger][INFO]: Rank[0]: Epoch[429/500], iter[41550] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50034
[2024-02-27 11:21:32,698][PyLogger][INFO]: Rank[0]: Epoch[429/500], iter[41600] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50035
[2024-02-27 11:21:32,698][PyLogger][INFO]: Rank[1]: Epoch[429/500], iter[41600] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50038
[2024-02-27 11:21:59,554][PyLogger][INFO]: Rank[1]: Epoch[430/500], iter[41650] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50036
[2024-02-27 11:21:59,554][PyLogger][INFO]: Rank[0]: Epoch[430/500], iter[41650] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50039
[2024-02-27 11:22:25,769][PyLogger][INFO]: Rank[1]: Epoch[430/500], iter[41700] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50038
[2024-02-27 11:22:25,769][PyLogger][INFO]: Rank[0]: Epoch[430/500], iter[41700] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69677 - loss: 0.50036
[2024-02-27 11:23:19,565][PyLogger][INFO]: Rank[0]: Epoch[431/500], iter[41750] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50037
[2024-02-27 11:23:19,565][PyLogger][INFO]: Rank[1]: Epoch[431/500], iter[41750] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50036
[2024-02-27 11:23:45,778][PyLogger][INFO]: Rank[1]: Epoch[431/500], iter[41800] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50042
[2024-02-27 11:23:45,778][PyLogger][INFO]: Rank[0]: Epoch[431/500], iter[41800] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50039
[2024-02-27 11:24:12,811][PyLogger][INFO]: Rank[1]: Epoch[432/500], iter[41850] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50037
[2024-02-27 11:24:12,811][PyLogger][INFO]: Rank[0]: Epoch[432/500], iter[41850] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50037
[2024-02-27 11:24:39,075][PyLogger][INFO]: Rank[1]: Epoch[432/500], iter[41900] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50042
[2024-02-27 11:24:39,075][PyLogger][INFO]: Rank[0]: Epoch[432/500], iter[41900] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50034
[2024-02-27 11:25:05,846][PyLogger][INFO]: Rank[0]: Epoch[433/500], iter[41950] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50035
[2024-02-27 11:25:05,846][PyLogger][INFO]: Rank[1]: Epoch[433/500], iter[41950] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50811
[2024-02-27 11:25:31,845][PyLogger][INFO]: Rank[1]: Epoch[433/500], iter[42000] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50037
[2024-02-27 11:25:31,845][PyLogger][INFO]: Rank[0]: Epoch[433/500], iter[42000] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50037
[2024-02-27 11:25:58,439][PyLogger][INFO]: Rank[1]: Epoch[434/500], iter[42050] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50035
[2024-02-27 11:25:58,439][PyLogger][INFO]: Rank[0]: Epoch[434/500], iter[42050] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50035
[2024-02-27 11:26:25,080][PyLogger][INFO]: Rank[1]: Epoch[435/500], iter[42100] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50034
[2024-02-27 11:26:25,080][PyLogger][INFO]: Rank[0]: Epoch[435/500], iter[42100] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50034
[2024-02-27 11:26:51,298][PyLogger][INFO]: Rank[1]: Epoch[435/500], iter[42150] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50037
[2024-02-27 11:26:51,298][PyLogger][INFO]: Rank[0]: Epoch[435/500], iter[42150] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69598 - loss: 0.50035
[2024-02-27 11:27:45,378][PyLogger][INFO]: Rank[0]: Epoch[436/500], iter[42200] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50036
[2024-02-27 11:27:45,378][PyLogger][INFO]: Rank[1]: Epoch[436/500], iter[42200] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50035
[2024-02-27 11:28:11,621][PyLogger][INFO]: Rank[1]: Epoch[436/500], iter[42250] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50042
[2024-02-27 11:28:11,621][PyLogger][INFO]: Rank[0]: Epoch[436/500], iter[42250] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50038
[2024-02-27 11:28:38,451][PyLogger][INFO]: Rank[0]: Epoch[437/500], iter[42300] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50036
[2024-02-27 11:28:38,451][PyLogger][INFO]: Rank[1]: Epoch[437/500], iter[42300] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50087
[2024-02-27 11:29:04,633][PyLogger][INFO]: Rank[1]: Epoch[437/500], iter[42350] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50104
[2024-02-27 11:29:04,633][PyLogger][INFO]: Rank[0]: Epoch[437/500], iter[42350] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50035
[2024-02-27 11:29:31,281][PyLogger][INFO]: Rank[1]: Epoch[438/500], iter[42400] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50037
[2024-02-27 11:29:31,281][PyLogger][INFO]: Rank[0]: Epoch[438/500], iter[42400] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50035
[2024-02-27 11:29:57,410][PyLogger][INFO]: Rank[1]: Epoch[438/500], iter[42450] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50035
[2024-02-27 11:29:57,411][PyLogger][INFO]: Rank[0]: Epoch[438/500], iter[42450] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50041
[2024-02-27 11:30:24,167][PyLogger][INFO]: Rank[1]: Epoch[439/500], iter[42500] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50040
[2024-02-27 11:30:24,167][PyLogger][INFO]: Rank[0]: Epoch[439/500], iter[42500] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50036
[2024-02-27 11:30:50,211][PyLogger][INFO]: Rank[0]: Epoch[439/500], iter[42550] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50037
[2024-02-27 11:30:50,211][PyLogger][INFO]: Rank[1]: Epoch[439/500], iter[42550] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50037
[2024-02-27 11:31:16,859][PyLogger][INFO]: Rank[0]: Epoch[440/500], iter[42600] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50035
[2024-02-27 11:31:16,859][PyLogger][INFO]: Rank[1]: Epoch[440/500], iter[42600] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50035
[2024-02-27 11:31:43,088][PyLogger][INFO]: Rank[1]: Epoch[440/500], iter[42650] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50037
[2024-02-27 11:31:43,088][PyLogger][INFO]: Rank[0]: Epoch[440/500], iter[42650] - train_accuracy: 1.00000 - val_accuracy: 0.93610 - train_loss: 0.50034 - val_loss: 0.69541 - loss: 0.50035
[2024-02-27 11:32:37,237][PyLogger][INFO]: Rank[0]: Epoch[441/500], iter[42700] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50046
[2024-02-27 11:32:37,237][PyLogger][INFO]: Rank[1]: Epoch[441/500], iter[42700] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50037
[2024-02-27 11:33:03,539][PyLogger][INFO]: Rank[1]: Epoch[441/500], iter[42750] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50035
[2024-02-27 11:33:03,539][PyLogger][INFO]: Rank[0]: Epoch[441/500], iter[42750] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50035
[2024-02-27 11:33:30,230][PyLogger][INFO]: Rank[0]: Epoch[442/500], iter[42800] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50038
[2024-02-27 11:33:30,230][PyLogger][INFO]: Rank[1]: Epoch[442/500], iter[42800] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50041
[2024-02-27 11:33:56,403][PyLogger][INFO]: Rank[1]: Epoch[442/500], iter[42850] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50043
[2024-02-27 11:33:56,403][PyLogger][INFO]: Rank[0]: Epoch[442/500], iter[42850] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50038
[2024-02-27 11:34:23,519][PyLogger][INFO]: Rank[0]: Epoch[443/500], iter[42900] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50056
[2024-02-27 11:34:23,519][PyLogger][INFO]: Rank[1]: Epoch[443/500], iter[42900] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50034
[2024-02-27 11:34:49,846][PyLogger][INFO]: Rank[1]: Epoch[443/500], iter[42950] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50036
[2024-02-27 11:34:49,846][PyLogger][INFO]: Rank[0]: Epoch[443/500], iter[42950] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50041
[2024-02-27 11:35:16,602][PyLogger][INFO]: Rank[1]: Epoch[444/500], iter[43000] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50035
[2024-02-27 11:35:16,602][PyLogger][INFO]: Rank[0]: Epoch[444/500], iter[43000] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50036
[2024-02-27 11:35:42,952][PyLogger][INFO]: Rank[0]: Epoch[444/500], iter[43050] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50038
[2024-02-27 11:35:42,952][PyLogger][INFO]: Rank[1]: Epoch[444/500], iter[43050] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50037
[2024-02-27 11:36:09,808][PyLogger][INFO]: Rank[1]: Epoch[445/500], iter[43100] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50040
[2024-02-27 11:36:09,808][PyLogger][INFO]: Rank[0]: Epoch[445/500], iter[43100] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50036
[2024-02-27 11:36:35,975][PyLogger][INFO]: Rank[1]: Epoch[445/500], iter[43150] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50037
[2024-02-27 11:36:35,975][PyLogger][INFO]: Rank[0]: Epoch[445/500], iter[43150] - train_accuracy: 1.00000 - val_accuracy: 0.93550 - train_loss: 0.50034 - val_loss: 0.69495 - loss: 0.50037
[2024-02-27 11:37:29,988][PyLogger][INFO]: Rank[1]: Epoch[446/500], iter[43200] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50037
[2024-02-27 11:37:29,988][PyLogger][INFO]: Rank[0]: Epoch[446/500], iter[43200] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50035
[2024-02-27 11:37:56,189][PyLogger][INFO]: Rank[1]: Epoch[446/500], iter[43250] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50036
[2024-02-27 11:37:56,189][PyLogger][INFO]: Rank[0]: Epoch[446/500], iter[43250] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50036
[2024-02-27 11:38:22,988][PyLogger][INFO]: Rank[0]: Epoch[447/500], iter[43300] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50037
[2024-02-27 11:38:22,988][PyLogger][INFO]: Rank[1]: Epoch[447/500], iter[43300] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50035
[2024-02-27 11:38:49,159][PyLogger][INFO]: Rank[1]: Epoch[447/500], iter[43350] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50038
[2024-02-27 11:38:49,160][PyLogger][INFO]: Rank[0]: Epoch[447/500], iter[43350] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50041
[2024-02-27 11:39:16,051][PyLogger][INFO]: Rank[1]: Epoch[448/500], iter[43400] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50036
[2024-02-27 11:39:16,051][PyLogger][INFO]: Rank[0]: Epoch[448/500], iter[43400] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50036
[2024-02-27 11:39:42,176][PyLogger][INFO]: Rank[1]: Epoch[448/500], iter[43450] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50036
[2024-02-27 11:39:42,176][PyLogger][INFO]: Rank[0]: Epoch[448/500], iter[43450] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50042
[2024-02-27 11:40:08,796][PyLogger][INFO]: Rank[1]: Epoch[449/500], iter[43500] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50035
[2024-02-27 11:40:08,797][PyLogger][INFO]: Rank[0]: Epoch[449/500], iter[43500] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50035
[2024-02-27 11:40:35,121][PyLogger][INFO]: Rank[0]: Epoch[449/500], iter[43550] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50034
[2024-02-27 11:40:35,121][PyLogger][INFO]: Rank[1]: Epoch[449/500], iter[43550] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50038
[2024-02-27 11:41:01,846][PyLogger][INFO]: Rank[1]: Epoch[450/500], iter[43600] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50034
[2024-02-27 11:41:01,846][PyLogger][INFO]: Rank[0]: Epoch[450/500], iter[43600] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50051
[2024-02-27 11:41:28,091][PyLogger][INFO]: Rank[1]: Epoch[450/500], iter[43650] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50035
[2024-02-27 11:41:28,091][PyLogger][INFO]: Rank[0]: Epoch[450/500], iter[43650] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69390 - loss: 0.50038
[2024-02-27 11:42:22,313][PyLogger][INFO]: Rank[0]: Epoch[451/500], iter[43700] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50034
[2024-02-27 11:42:22,313][PyLogger][INFO]: Rank[1]: Epoch[451/500], iter[43700] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50041
[2024-02-27 11:42:48,875][PyLogger][INFO]: Rank[1]: Epoch[452/500], iter[43750] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50038
[2024-02-27 11:42:48,875][PyLogger][INFO]: Rank[0]: Epoch[452/500], iter[43750] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50037
[2024-02-27 11:43:15,205][PyLogger][INFO]: Rank[1]: Epoch[452/500], iter[43800] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50035
[2024-02-27 11:43:15,205][PyLogger][INFO]: Rank[0]: Epoch[452/500], iter[43800] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50035
[2024-02-27 11:43:41,954][PyLogger][INFO]: Rank[0]: Epoch[453/500], iter[43850] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50035
[2024-02-27 11:43:41,954][PyLogger][INFO]: Rank[1]: Epoch[453/500], iter[43850] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50035
[2024-02-27 11:44:08,196][PyLogger][INFO]: Rank[1]: Epoch[453/500], iter[43900] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50066
[2024-02-27 11:44:08,196][PyLogger][INFO]: Rank[0]: Epoch[453/500], iter[43900] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50035
[2024-02-27 11:44:35,000][PyLogger][INFO]: Rank[1]: Epoch[454/500], iter[43950] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50036
[2024-02-27 11:44:35,000][PyLogger][INFO]: Rank[0]: Epoch[454/500], iter[43950] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50037
[2024-02-27 11:45:01,227][PyLogger][INFO]: Rank[0]: Epoch[454/500], iter[44000] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50042
[2024-02-27 11:45:01,228][PyLogger][INFO]: Rank[1]: Epoch[454/500], iter[44000] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50036
[2024-02-27 11:45:28,023][PyLogger][INFO]: Rank[1]: Epoch[455/500], iter[44050] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50037
[2024-02-27 11:45:28,023][PyLogger][INFO]: Rank[0]: Epoch[455/500], iter[44050] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50034
[2024-02-27 11:45:54,250][PyLogger][INFO]: Rank[0]: Epoch[455/500], iter[44100] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50043
[2024-02-27 11:45:54,250][PyLogger][INFO]: Rank[1]: Epoch[455/500], iter[44100] - train_accuracy: 1.00000 - val_accuracy: 0.93680 - train_loss: 0.50033 - val_loss: 0.69531 - loss: 0.50034
[2024-02-27 11:46:48,366][PyLogger][INFO]: Rank[0]: Epoch[456/500], iter[44150] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50054
[2024-02-27 11:46:48,366][PyLogger][INFO]: Rank[1]: Epoch[456/500], iter[44150] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50036
[2024-02-27 11:47:14,707][PyLogger][INFO]: Rank[1]: Epoch[456/500], iter[44200] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50037
[2024-02-27 11:47:14,707][PyLogger][INFO]: Rank[0]: Epoch[456/500], iter[44200] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50038
[2024-02-27 11:47:41,415][PyLogger][INFO]: Rank[1]: Epoch[457/500], iter[44250] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50037
[2024-02-27 11:47:41,415][PyLogger][INFO]: Rank[0]: Epoch[457/500], iter[44250] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50054
[2024-02-27 11:48:07,682][PyLogger][INFO]: Rank[0]: Epoch[457/500], iter[44300] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50038
[2024-02-27 11:48:07,682][PyLogger][INFO]: Rank[1]: Epoch[457/500], iter[44300] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50038
[2024-02-27 11:48:34,358][PyLogger][INFO]: Rank[0]: Epoch[458/500], iter[44350] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50037
[2024-02-27 11:48:34,358][PyLogger][INFO]: Rank[1]: Epoch[458/500], iter[44350] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50037
[2024-02-27 11:49:00,518][PyLogger][INFO]: Rank[1]: Epoch[458/500], iter[44400] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50035
[2024-02-27 11:49:00,518][PyLogger][INFO]: Rank[0]: Epoch[458/500], iter[44400] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50034
[2024-02-27 11:49:27,176][PyLogger][INFO]: Rank[1]: Epoch[459/500], iter[44450] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50040
[2024-02-27 11:49:27,176][PyLogger][INFO]: Rank[0]: Epoch[459/500], iter[44450] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50034
[2024-02-27 11:49:53,206][PyLogger][INFO]: Rank[0]: Epoch[459/500], iter[44500] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50037
[2024-02-27 11:49:53,206][PyLogger][INFO]: Rank[1]: Epoch[459/500], iter[44500] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50038
[2024-02-27 11:50:19,972][PyLogger][INFO]: Rank[1]: Epoch[460/500], iter[44550] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50036
[2024-02-27 11:50:19,972][PyLogger][INFO]: Rank[0]: Epoch[460/500], iter[44550] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50035
[2024-02-27 11:50:46,188][PyLogger][INFO]: Rank[1]: Epoch[460/500], iter[44600] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50035
[2024-02-27 11:50:46,188][PyLogger][INFO]: Rank[0]: Epoch[460/500], iter[44600] - train_accuracy: 1.00000 - val_accuracy: 0.93690 - train_loss: 0.50033 - val_loss: 0.69482 - loss: 0.50039
[2024-02-27 11:51:40,101][PyLogger][INFO]: Rank[1]: Epoch[461/500], iter[44650] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50036
[2024-02-27 11:51:40,102][PyLogger][INFO]: Rank[0]: Epoch[461/500], iter[44650] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50038
[2024-02-27 11:52:06,458][PyLogger][INFO]: Rank[0]: Epoch[461/500], iter[44700] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50035
[2024-02-27 11:52:06,458][PyLogger][INFO]: Rank[1]: Epoch[461/500], iter[44700] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50034
[2024-02-27 11:52:33,282][PyLogger][INFO]: Rank[1]: Epoch[462/500], iter[44750] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50037
[2024-02-27 11:52:33,282][PyLogger][INFO]: Rank[0]: Epoch[462/500], iter[44750] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50044
[2024-02-27 11:52:59,537][PyLogger][INFO]: Rank[1]: Epoch[462/500], iter[44800] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50065
[2024-02-27 11:52:59,537][PyLogger][INFO]: Rank[0]: Epoch[462/500], iter[44800] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50035
[2024-02-27 11:53:26,286][PyLogger][INFO]: Rank[1]: Epoch[463/500], iter[44850] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50037
[2024-02-27 11:53:26,286][PyLogger][INFO]: Rank[0]: Epoch[463/500], iter[44850] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50035
[2024-02-27 11:53:52,498][PyLogger][INFO]: Rank[1]: Epoch[463/500], iter[44900] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50036
[2024-02-27 11:53:52,499][PyLogger][INFO]: Rank[0]: Epoch[463/500], iter[44900] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50034
[2024-02-27 11:54:19,095][PyLogger][INFO]: Rank[0]: Epoch[464/500], iter[44950] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50038
[2024-02-27 11:54:19,095][PyLogger][INFO]: Rank[1]: Epoch[464/500], iter[44950] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50035
[2024-02-27 11:54:45,387][PyLogger][INFO]: Rank[1]: Epoch[464/500], iter[45000] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50035
[2024-02-27 11:54:45,387][PyLogger][INFO]: Rank[0]: Epoch[464/500], iter[45000] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50042
[2024-02-27 11:55:12,301][PyLogger][INFO]: Rank[1]: Epoch[465/500], iter[45050] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50036
[2024-02-27 11:55:12,301][PyLogger][INFO]: Rank[0]: Epoch[465/500], iter[45050] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50035
[2024-02-27 11:55:38,433][PyLogger][INFO]: Rank[0]: Epoch[465/500], iter[45100] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50036
[2024-02-27 11:55:38,433][PyLogger][INFO]: Rank[1]: Epoch[465/500], iter[45100] - train_accuracy: 1.00000 - val_accuracy: 0.93630 - train_loss: 0.50033 - val_loss: 0.69520 - loss: 0.50036
[2024-02-27 11:56:32,534][PyLogger][INFO]: Rank[0]: Epoch[466/500], iter[45150] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50041
[2024-02-27 11:56:32,534][PyLogger][INFO]: Rank[1]: Epoch[466/500], iter[45150] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50035
[2024-02-27 11:56:58,826][PyLogger][INFO]: Rank[1]: Epoch[466/500], iter[45200] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50037
[2024-02-27 11:56:58,826][PyLogger][INFO]: Rank[0]: Epoch[466/500], iter[45200] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50037
[2024-02-27 11:57:25,438][PyLogger][INFO]: Rank[1]: Epoch[467/500], iter[45250] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50035
[2024-02-27 11:57:25,438][PyLogger][INFO]: Rank[0]: Epoch[467/500], iter[45250] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50036
[2024-02-27 11:57:52,103][PyLogger][INFO]: Rank[0]: Epoch[468/500], iter[45300] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50037
[2024-02-27 11:57:52,103][PyLogger][INFO]: Rank[1]: Epoch[468/500], iter[45300] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50036
[2024-02-27 11:58:18,346][PyLogger][INFO]: Rank[1]: Epoch[468/500], iter[45350] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50036
[2024-02-27 11:58:18,346][PyLogger][INFO]: Rank[0]: Epoch[468/500], iter[45350] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50038
[2024-02-27 11:58:44,986][PyLogger][INFO]: Rank[0]: Epoch[469/500], iter[45400] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50035
[2024-02-27 11:58:44,986][PyLogger][INFO]: Rank[1]: Epoch[469/500], iter[45400] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50034
[2024-02-27 11:59:11,235][PyLogger][INFO]: Rank[1]: Epoch[469/500], iter[45450] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50037
[2024-02-27 11:59:11,235][PyLogger][INFO]: Rank[0]: Epoch[469/500], iter[45450] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50035
[2024-02-27 11:59:38,056][PyLogger][INFO]: Rank[1]: Epoch[470/500], iter[45500] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50038
[2024-02-27 11:59:38,056][PyLogger][INFO]: Rank[0]: Epoch[470/500], iter[45500] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50036
[2024-02-27 12:00:04,354][PyLogger][INFO]: Rank[1]: Epoch[470/500], iter[45550] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50035
[2024-02-27 12:00:04,354][PyLogger][INFO]: Rank[0]: Epoch[470/500], iter[45550] - train_accuracy: 1.00000 - val_accuracy: 0.93620 - train_loss: 0.50033 - val_loss: 0.69435 - loss: 0.50036
[2024-02-27 12:00:58,279][PyLogger][INFO]: Rank[1]: Epoch[471/500], iter[45600] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50038
[2024-02-27 12:00:58,279][PyLogger][INFO]: Rank[0]: Epoch[471/500], iter[45600] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50033
[2024-02-27 12:01:24,710][PyLogger][INFO]: Rank[1]: Epoch[471/500], iter[45650] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50035
[2024-02-27 12:01:24,710][PyLogger][INFO]: Rank[0]: Epoch[471/500], iter[45650] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50039
[2024-02-27 12:01:51,430][PyLogger][INFO]: Rank[0]: Epoch[472/500], iter[45700] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50036
[2024-02-27 12:01:51,430][PyLogger][INFO]: Rank[1]: Epoch[472/500], iter[45700] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50036
[2024-02-27 12:02:17,581][PyLogger][INFO]: Rank[1]: Epoch[472/500], iter[45750] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50035
[2024-02-27 12:02:17,581][PyLogger][INFO]: Rank[0]: Epoch[472/500], iter[45750] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50035
[2024-02-27 12:02:44,369][PyLogger][INFO]: Rank[1]: Epoch[473/500], iter[45800] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50036
[2024-02-27 12:02:44,369][PyLogger][INFO]: Rank[0]: Epoch[473/500], iter[45800] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50047
[2024-02-27 12:03:10,581][PyLogger][INFO]: Rank[0]: Epoch[473/500], iter[45850] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50034
[2024-02-27 12:03:10,581][PyLogger][INFO]: Rank[1]: Epoch[473/500], iter[45850] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50035
[2024-02-27 12:03:37,337][PyLogger][INFO]: Rank[1]: Epoch[474/500], iter[45900] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50036
[2024-02-27 12:03:37,337][PyLogger][INFO]: Rank[0]: Epoch[474/500], iter[45900] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50034
[2024-02-27 12:04:03,579][PyLogger][INFO]: Rank[1]: Epoch[474/500], iter[45950] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50040
[2024-02-27 12:04:03,579][PyLogger][INFO]: Rank[0]: Epoch[474/500], iter[45950] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50034
[2024-02-27 12:04:30,261][PyLogger][INFO]: Rank[0]: Epoch[475/500], iter[46000] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50039
[2024-02-27 12:04:30,261][PyLogger][INFO]: Rank[1]: Epoch[475/500], iter[46000] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50038
[2024-02-27 12:04:56,454][PyLogger][INFO]: Rank[1]: Epoch[475/500], iter[46050] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50039
[2024-02-27 12:04:56,454][PyLogger][INFO]: Rank[0]: Epoch[475/500], iter[46050] - train_accuracy: 1.00000 - val_accuracy: 0.93650 - train_loss: 0.50033 - val_loss: 0.69399 - loss: 0.50042
[2024-02-27 12:05:50,505][PyLogger][INFO]: Rank[0]: Epoch[476/500], iter[46100] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50037
[2024-02-27 12:05:50,505][PyLogger][INFO]: Rank[1]: Epoch[476/500], iter[46100] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50037
[2024-02-27 12:06:16,634][PyLogger][INFO]: Rank[1]: Epoch[476/500], iter[46150] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50038
[2024-02-27 12:06:16,634][PyLogger][INFO]: Rank[0]: Epoch[476/500], iter[46150] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50037
[2024-02-27 12:06:43,455][PyLogger][INFO]: Rank[1]: Epoch[477/500], iter[46200] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50035
[2024-02-27 12:06:43,455][PyLogger][INFO]: Rank[0]: Epoch[477/500], iter[46200] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50037
[2024-02-27 12:07:09,683][PyLogger][INFO]: Rank[0]: Epoch[477/500], iter[46250] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50036
[2024-02-27 12:07:09,683][PyLogger][INFO]: Rank[1]: Epoch[477/500], iter[46250] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50034
[2024-02-27 12:07:36,408][PyLogger][INFO]: Rank[1]: Epoch[478/500], iter[46300] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50036
[2024-02-27 12:07:36,408][PyLogger][INFO]: Rank[0]: Epoch[478/500], iter[46300] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50036
[2024-02-27 12:08:02,492][PyLogger][INFO]: Rank[0]: Epoch[478/500], iter[46350] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50042
[2024-02-27 12:08:02,492][PyLogger][INFO]: Rank[1]: Epoch[478/500], iter[46350] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50036
[2024-02-27 12:08:29,261][PyLogger][INFO]: Rank[1]: Epoch[479/500], iter[46400] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50034
[2024-02-27 12:08:29,261][PyLogger][INFO]: Rank[0]: Epoch[479/500], iter[46400] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50038
[2024-02-27 12:08:55,399][PyLogger][INFO]: Rank[0]: Epoch[479/500], iter[46450] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50039
[2024-02-27 12:08:55,399][PyLogger][INFO]: Rank[1]: Epoch[479/500], iter[46450] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50035
[2024-02-27 12:09:22,120][PyLogger][INFO]: Rank[1]: Epoch[480/500], iter[46500] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50035
[2024-02-27 12:09:22,120][PyLogger][INFO]: Rank[0]: Epoch[480/500], iter[46500] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50036
[2024-02-27 12:09:48,324][PyLogger][INFO]: Rank[0]: Epoch[480/500], iter[46550] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50036
[2024-02-27 12:09:48,324][PyLogger][INFO]: Rank[1]: Epoch[480/500], iter[46550] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69389 - loss: 0.50035
[2024-02-27 12:10:42,487][PyLogger][INFO]: Rank[0]: Epoch[481/500], iter[46600] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50046
[2024-02-27 12:10:42,487][PyLogger][INFO]: Rank[1]: Epoch[481/500], iter[46600] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50037
[2024-02-27 12:11:08,772][PyLogger][INFO]: Rank[0]: Epoch[481/500], iter[46650] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50036
[2024-02-27 12:11:08,772][PyLogger][INFO]: Rank[1]: Epoch[481/500], iter[46650] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50034
[2024-02-27 12:11:35,586][PyLogger][INFO]: Rank[1]: Epoch[482/500], iter[46700] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50037
[2024-02-27 12:11:35,586][PyLogger][INFO]: Rank[0]: Epoch[482/500], iter[46700] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50034
[2024-02-27 12:12:01,974][PyLogger][INFO]: Rank[1]: Epoch[482/500], iter[46750] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50036
[2024-02-27 12:12:01,974][PyLogger][INFO]: Rank[0]: Epoch[482/500], iter[46750] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50038
[2024-02-27 12:12:28,786][PyLogger][INFO]: Rank[1]: Epoch[483/500], iter[46800] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50036
[2024-02-27 12:12:28,786][PyLogger][INFO]: Rank[0]: Epoch[483/500], iter[46800] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50035
[2024-02-27 12:12:54,931][PyLogger][INFO]: Rank[1]: Epoch[483/500], iter[46850] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50040
[2024-02-27 12:12:54,931][PyLogger][INFO]: Rank[0]: Epoch[483/500], iter[46850] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50036
[2024-02-27 12:13:22,111][PyLogger][INFO]: Rank[1]: Epoch[484/500], iter[46900] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50048
[2024-02-27 12:13:22,111][PyLogger][INFO]: Rank[0]: Epoch[484/500], iter[46900] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50034
[2024-02-27 12:13:48,768][PyLogger][INFO]: Rank[1]: Epoch[485/500], iter[46950] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50042
[2024-02-27 12:13:48,768][PyLogger][INFO]: Rank[0]: Epoch[485/500], iter[46950] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50034
[2024-02-27 12:14:14,884][PyLogger][INFO]: Rank[0]: Epoch[485/500], iter[47000] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50038
[2024-02-27 12:14:14,884][PyLogger][INFO]: Rank[1]: Epoch[485/500], iter[47000] - train_accuracy: 1.00000 - val_accuracy: 0.93660 - train_loss: 0.50033 - val_loss: 0.69457 - loss: 0.50051
[2024-02-27 12:15:08,935][PyLogger][INFO]: Rank[0]: Epoch[486/500], iter[47050] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50040
[2024-02-27 12:15:08,935][PyLogger][INFO]: Rank[1]: Epoch[486/500], iter[47050] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50035
[2024-02-27 12:15:35,168][PyLogger][INFO]: Rank[1]: Epoch[486/500], iter[47100] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50045
[2024-02-27 12:15:35,168][PyLogger][INFO]: Rank[0]: Epoch[486/500], iter[47100] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50033
[2024-02-27 12:16:02,317][PyLogger][INFO]: Rank[1]: Epoch[487/500], iter[47150] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50039
[2024-02-27 12:16:02,317][PyLogger][INFO]: Rank[0]: Epoch[487/500], iter[47150] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50035
[2024-02-27 12:16:28,556][PyLogger][INFO]: Rank[1]: Epoch[487/500], iter[47200] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50037
[2024-02-27 12:16:28,556][PyLogger][INFO]: Rank[0]: Epoch[487/500], iter[47200] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50264
[2024-02-27 12:16:55,421][PyLogger][INFO]: Rank[1]: Epoch[488/500], iter[47250] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50036
[2024-02-27 12:16:55,421][PyLogger][INFO]: Rank[0]: Epoch[488/500], iter[47250] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50035
[2024-02-27 12:17:21,716][PyLogger][INFO]: Rank[1]: Epoch[488/500], iter[47300] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50035
[2024-02-27 12:17:21,716][PyLogger][INFO]: Rank[0]: Epoch[488/500], iter[47300] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50042
[2024-02-27 12:17:48,835][PyLogger][INFO]: Rank[1]: Epoch[489/500], iter[47350] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50039
[2024-02-27 12:17:48,835][PyLogger][INFO]: Rank[0]: Epoch[489/500], iter[47350] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50035
[2024-02-27 12:18:15,066][PyLogger][INFO]: Rank[1]: Epoch[489/500], iter[47400] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50037
[2024-02-27 12:18:15,066][PyLogger][INFO]: Rank[0]: Epoch[489/500], iter[47400] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50038
[2024-02-27 12:18:41,839][PyLogger][INFO]: Rank[1]: Epoch[490/500], iter[47450] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50036
[2024-02-27 12:18:41,839][PyLogger][INFO]: Rank[0]: Epoch[490/500], iter[47450] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50036
[2024-02-27 12:19:08,060][PyLogger][INFO]: Rank[1]: Epoch[490/500], iter[47500] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50038
[2024-02-27 12:19:08,060][PyLogger][INFO]: Rank[0]: Epoch[490/500], iter[47500] - train_accuracy: 1.00000 - val_accuracy: 0.93590 - train_loss: 0.50033 - val_loss: 0.69446 - loss: 0.50037
[2024-02-27 12:20:02,095][PyLogger][INFO]: Rank[1]: Epoch[491/500], iter[47550] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50035
[2024-02-27 12:20:02,095][PyLogger][INFO]: Rank[0]: Epoch[491/500], iter[47550] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50036
[2024-02-27 12:20:28,274][PyLogger][INFO]: Rank[0]: Epoch[491/500], iter[47600] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50036
[2024-02-27 12:20:28,274][PyLogger][INFO]: Rank[1]: Epoch[491/500], iter[47600] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50036
[2024-02-27 12:20:54,979][PyLogger][INFO]: Rank[1]: Epoch[492/500], iter[47650] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50035
[2024-02-27 12:20:54,979][PyLogger][INFO]: Rank[0]: Epoch[492/500], iter[47650] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50035
[2024-02-27 12:21:21,297][PyLogger][INFO]: Rank[0]: Epoch[492/500], iter[47700] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50037
[2024-02-27 12:21:21,297][PyLogger][INFO]: Rank[1]: Epoch[492/500], iter[47700] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50037
[2024-02-27 12:21:48,045][PyLogger][INFO]: Rank[1]: Epoch[493/500], iter[47750] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50039
[2024-02-27 12:21:48,046][PyLogger][INFO]: Rank[0]: Epoch[493/500], iter[47750] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50040
[2024-02-27 12:22:14,329][PyLogger][INFO]: Rank[0]: Epoch[493/500], iter[47800] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50173
[2024-02-27 12:22:14,329][PyLogger][INFO]: Rank[1]: Epoch[493/500], iter[47800] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50038
[2024-02-27 12:22:41,008][PyLogger][INFO]: Rank[1]: Epoch[494/500], iter[47850] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50034
[2024-02-27 12:22:41,008][PyLogger][INFO]: Rank[0]: Epoch[494/500], iter[47850] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50038
[2024-02-27 12:23:07,381][PyLogger][INFO]: Rank[1]: Epoch[494/500], iter[47900] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50035
[2024-02-27 12:23:07,381][PyLogger][INFO]: Rank[0]: Epoch[494/500], iter[47900] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50036
[2024-02-27 12:23:34,253][PyLogger][INFO]: Rank[1]: Epoch[495/500], iter[47950] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50038
[2024-02-27 12:23:34,253][PyLogger][INFO]: Rank[0]: Epoch[495/500], iter[47950] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50035
[2024-02-27 12:24:00,448][PyLogger][INFO]: Rank[0]: Epoch[495/500], iter[48000] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50034
[2024-02-27 12:24:00,448][PyLogger][INFO]: Rank[1]: Epoch[495/500], iter[48000] - train_accuracy: 1.00000 - val_accuracy: 0.93640 - train_loss: 0.50033 - val_loss: 0.69402 - loss: 0.50036
[2024-02-27 12:24:54,377][PyLogger][INFO]: Rank[0]: Epoch[496/500], iter[48050] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50038
[2024-02-27 12:24:54,377][PyLogger][INFO]: Rank[1]: Epoch[496/500], iter[48050] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50041
[2024-02-27 12:25:20,512][PyLogger][INFO]: Rank[1]: Epoch[496/500], iter[48100] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50037
[2024-02-27 12:25:20,512][PyLogger][INFO]: Rank[0]: Epoch[496/500], iter[48100] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50037
[2024-02-27 12:25:47,075][PyLogger][INFO]: Rank[0]: Epoch[497/500], iter[48150] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50038
[2024-02-27 12:25:47,075][PyLogger][INFO]: Rank[1]: Epoch[497/500], iter[48150] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50039
[2024-02-27 12:26:13,254][PyLogger][INFO]: Rank[1]: Epoch[497/500], iter[48200] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50034
[2024-02-27 12:26:13,254][PyLogger][INFO]: Rank[0]: Epoch[497/500], iter[48200] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50043
[2024-02-27 12:26:40,070][PyLogger][INFO]: Rank[1]: Epoch[498/500], iter[48250] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50033
[2024-02-27 12:26:40,070][PyLogger][INFO]: Rank[0]: Epoch[498/500], iter[48250] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50036
[2024-02-27 12:27:06,272][PyLogger][INFO]: Rank[1]: Epoch[498/500], iter[48300] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50036
[2024-02-27 12:27:06,272][PyLogger][INFO]: Rank[0]: Epoch[498/500], iter[48300] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50037
[2024-02-27 12:27:32,986][PyLogger][INFO]: Rank[0]: Epoch[499/500], iter[48350] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50039
[2024-02-27 12:27:32,986][PyLogger][INFO]: Rank[1]: Epoch[499/500], iter[48350] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50037
[2024-02-27 12:27:59,173][PyLogger][INFO]: Rank[0]: Epoch[499/500], iter[48400] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50035
[2024-02-27 12:27:59,173][PyLogger][INFO]: Rank[1]: Epoch[499/500], iter[48400] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50035
[2024-02-27 12:28:25,849][PyLogger][INFO]: Rank[0]: Epoch[500/500], iter[48450] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50037
[2024-02-27 12:28:25,849][PyLogger][INFO]: Rank[1]: Epoch[500/500], iter[48450] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50039
[2024-02-27 12:28:52,176][PyLogger][INFO]: Rank[1]: Epoch[500/500], iter[48500] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50037
[2024-02-27 12:28:52,176][PyLogger][INFO]: Rank[0]: Epoch[500/500], iter[48500] - train_accuracy: 1.00000 - val_accuracy: 0.93600 - train_loss: 0.50033 - val_loss: 0.69412 - loss: 0.50037
Files already downloaded and verified
Files already downloaded and verified
2024-02-27 12:29:21,865 ignite.distributed.launcher.Parallel INFO: End of run
2024-02-27 12:29:28,688 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:29:28,688 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:29:28,688 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14e9de8ba3e0>' in 2 processes
2024-02-27 12:29:39,291 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:29:39,756 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14671e1f0a40>}
[2024-02-27 12:29:40,902][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:29:40,902][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:29:40,902][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:29:40,902][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:29:40,904][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:29:40,904][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:29:40,904][PyLogger][INFO]: World size: 2
[2024-02-27 12:29:40,904][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:29:40,904][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:29:40,904][PyLogger][INFO]: World size: 2
[2024-02-27 12:29:44,181][PyLogger][INFO]: Rank[0]: val_accuracy: 0.10000
[2024-02-27 12:29:44,181][PyLogger][INFO]: Rank[1]: val_accuracy: 0.10000
2024-02-27 12:29:45,614 ignite.distributed.launcher.Parallel INFO: End of run
new_size=2 done.
2024-02-27 12:29:49,864 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:29:49,864 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:29:49,864 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x154ff825e3e0>' in 2 processes
2024-02-27 12:29:58,389 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:29:58,806 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1511a90248f0>}
[2024-02-27 12:29:58,923][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:29:58,924][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:29:58,924][PyLogger][INFO]: World size: 2
[2024-02-27 12:29:58,928][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:29:58,928][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:29:58,928][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:29:58,928][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:29:58,929][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:29:58,929][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:29:58,929][PyLogger][INFO]: World size: 2
[2024-02-27 12:30:02,323][PyLogger][INFO]: Rank[0]: val_accuracy: 0.10000
[2024-02-27 12:30:02,323][PyLogger][INFO]: Rank[1]: val_accuracy: 0.10000
2024-02-27 12:30:03,867 ignite.distributed.launcher.Parallel INFO: End of run
new_size=3 done.
2024-02-27 12:30:09,320 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:30:09,320 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:30:09,320 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14ed5a3c63e0>' in 2 processes
2024-02-27 12:30:20,943 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:30:21,375 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x146f1b5d4ce0>}
[2024-02-27 12:30:21,497][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:30:21,497][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:30:21,497][PyLogger][INFO]: World size: 2
[2024-02-27 12:30:21,498][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:30:21,499][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:30:21,499][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:30:21,499][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:30:21,500][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:30:21,500][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:30:21,500][PyLogger][INFO]: World size: 2
[2024-02-27 12:30:24,922][PyLogger][INFO]: Rank[1]: val_accuracy: 0.10000
[2024-02-27 12:30:24,922][PyLogger][INFO]: Rank[0]: val_accuracy: 0.10000
2024-02-27 12:30:26,481 ignite.distributed.launcher.Parallel INFO: End of run
new_size=4 done.
2024-02-27 12:30:32,221 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:30:32,221 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:30:32,221 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x149c2890e3e0>' in 2 processes
2024-02-27 12:30:40,766 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:30:41,169 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x153991dec380>}
[2024-02-27 12:30:41,282][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:30:41,282][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:30:41,282][PyLogger][INFO]: World size: 2
[2024-02-27 12:30:41,287][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:30:41,287][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:30:41,287][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:30:41,287][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:30:41,288][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:30:41,289][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:30:41,289][PyLogger][INFO]: World size: 2
[2024-02-27 12:30:44,851][PyLogger][INFO]: Rank[1]: val_accuracy: 0.10000
[2024-02-27 12:30:44,851][PyLogger][INFO]: Rank[0]: val_accuracy: 0.10000
2024-02-27 12:30:46,288 ignite.distributed.launcher.Parallel INFO: End of run
new_size=5 done.
2024-02-27 12:30:52,102 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:30:52,102 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:30:52,102 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x15106cd463e0>' in 2 processes
2024-02-27 12:31:03,115 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:31:03,529 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x153efd0b3b90>}
[2024-02-27 12:31:03,641][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:31:03,642][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:31:03,642][PyLogger][INFO]: World size: 2
[2024-02-27 12:31:03,645][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:31:03,645][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:31:03,645][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:31:03,646][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:31:03,646][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:31:03,646][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:31:03,647][PyLogger][INFO]: World size: 2
[2024-02-27 12:31:07,254][PyLogger][INFO]: Rank[0]: val_accuracy: 0.10000
[2024-02-27 12:31:07,254][PyLogger][INFO]: Rank[1]: val_accuracy: 0.10000
2024-02-27 12:31:08,701 ignite.distributed.launcher.Parallel INFO: End of run
new_size=6 done.
2024-02-27 12:31:14,500 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:31:14,500 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:31:14,500 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14b65d56e3e0>' in 2 processes
2024-02-27 12:31:26,001 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:31:26,469 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14f623ef4b90>}
[2024-02-27 12:31:26,591][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:31:26,591][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:31:26,591][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:31:26,591][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:31:26,592][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:31:26,592][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:31:26,592][PyLogger][INFO]: World size: 2
[2024-02-27 12:31:26,594][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:31:26,594][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:31:26,594][PyLogger][INFO]: World size: 2
[2024-02-27 12:31:30,261][PyLogger][INFO]: Rank[1]: val_accuracy: 0.10000
[2024-02-27 12:31:30,261][PyLogger][INFO]: Rank[0]: val_accuracy: 0.10000
2024-02-27 12:31:31,705 ignite.distributed.launcher.Parallel INFO: End of run
new_size=7 done.
2024-02-27 12:31:35,817 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:31:35,817 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:31:35,817 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1545187ce3e0>' in 2 processes
2024-02-27 12:31:45,636 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:31:46,083 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14d74782c200>}
[2024-02-27 12:31:46,197][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:31:46,197][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:31:46,197][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:31:46,197][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:31:46,198][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:31:46,198][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:31:46,198][PyLogger][INFO]: World size: 2
[2024-02-27 12:31:46,205][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:31:46,205][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:31:46,205][PyLogger][INFO]: World size: 2
[2024-02-27 12:31:49,734][PyLogger][INFO]: Rank[0]: val_accuracy: 0.10000
[2024-02-27 12:31:49,734][PyLogger][INFO]: Rank[1]: val_accuracy: 0.10000
2024-02-27 12:31:51,192 ignite.distributed.launcher.Parallel INFO: End of run
new_size=8 done.
2024-02-27 12:31:56,600 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:31:56,600 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:31:56,600 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x15211bcee3e0>' in 2 processes
2024-02-27 12:32:06,653 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:32:07,070 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14d042e8c200>}
[2024-02-27 12:32:07,185][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:32:07,185][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:32:07,185][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:32:07,185][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:32:07,186][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:32:07,187][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:32:07,187][PyLogger][INFO]: World size: 2
[2024-02-27 12:32:07,187][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:32:07,187][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:32:07,187][PyLogger][INFO]: World size: 2
[2024-02-27 12:32:10,770][PyLogger][INFO]: Rank[1]: val_accuracy: 0.10550
[2024-02-27 12:32:10,770][PyLogger][INFO]: Rank[0]: val_accuracy: 0.10550
2024-02-27 12:32:11,826 ignite.distributed.launcher.Parallel INFO: End of run
new_size=9 done.
2024-02-27 12:32:17,552 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:32:17,552 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:32:17,552 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1485adbf63e0>' in 2 processes
2024-02-27 12:32:28,461 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:32:28,876 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x152481b8d3a0>}
[2024-02-27 12:32:28,988][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:32:28,988][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:32:28,988][PyLogger][INFO]: World size: 2
[2024-02-27 12:32:28,994][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:32:28,994][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:32:28,994][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:32:28,994][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:32:28,995][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:32:28,996][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:32:28,996][PyLogger][INFO]: World size: 2
[2024-02-27 12:32:32,632][PyLogger][INFO]: Rank[1]: val_accuracy: 0.10690
[2024-02-27 12:32:32,632][PyLogger][INFO]: Rank[0]: val_accuracy: 0.10690
2024-02-27 12:32:34,110 ignite.distributed.launcher.Parallel INFO: End of run
new_size=10 done.
2024-02-27 12:32:39,864 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:32:39,864 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:32:39,864 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14a2b27de3e0>' in 2 processes
2024-02-27 12:32:51,379 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:32:51,784 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1518051ace30>}
[2024-02-27 12:32:51,901][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:32:51,901][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:32:51,901][PyLogger][INFO]: World size: 2
[2024-02-27 12:32:51,905][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:32:51,905][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:32:51,905][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:32:51,905][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:32:51,906][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:32:51,906][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:32:51,906][PyLogger][INFO]: World size: 2
[2024-02-27 12:32:55,606][PyLogger][INFO]: Rank[0]: val_accuracy: 0.11960
[2024-02-27 12:32:55,606][PyLogger][INFO]: Rank[1]: val_accuracy: 0.11960
2024-02-27 12:32:57,034 ignite.distributed.launcher.Parallel INFO: End of run
new_size=11 done.
2024-02-27 12:33:01,045 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:33:01,045 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:33:01,045 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x147c1c9a63e0>' in 2 processes
2024-02-27 12:33:10,877 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:33:11,291 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x145fdabc4e00>}
[2024-02-27 12:33:11,401][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:33:11,402][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:33:11,402][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:33:11,402][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:33:11,404][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:33:11,404][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:33:11,404][PyLogger][INFO]: World size: 2
[2024-02-27 12:33:11,415][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:33:11,415][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:33:11,415][PyLogger][INFO]: World size: 2
[2024-02-27 12:33:15,077][PyLogger][INFO]: Rank[0]: val_accuracy: 0.12370
[2024-02-27 12:33:15,077][PyLogger][INFO]: Rank[1]: val_accuracy: 0.12370
2024-02-27 12:33:16,522 ignite.distributed.launcher.Parallel INFO: End of run
new_size=12 done.
2024-02-27 12:33:22,190 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:33:22,190 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:33:22,190 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14e55a65a3e0>' in 2 processes
2024-02-27 12:33:31,960 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:33:32,392 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14948f8f7680>}
[2024-02-27 12:33:32,503][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:33:32,503][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:33:32,503][PyLogger][INFO]: World size: 2
[2024-02-27 12:33:32,512][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:33:32,512][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:33:32,512][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:33:32,512][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:33:32,513][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:33:32,513][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:33:32,514][PyLogger][INFO]: World size: 2
[2024-02-27 12:33:36,208][PyLogger][INFO]: Rank[1]: val_accuracy: 0.13720
[2024-02-27 12:33:36,208][PyLogger][INFO]: Rank[0]: val_accuracy: 0.13720
2024-02-27 12:33:37,267 ignite.distributed.launcher.Parallel INFO: End of run
new_size=13 done.
2024-02-27 12:33:42,687 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:33:42,687 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:33:42,687 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x145ff6f7e3e0>' in 2 processes
2024-02-27 12:33:53,649 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:33:54,088 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1513a2318380>}
[2024-02-27 12:33:54,205][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:33:54,205][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:33:54,206][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:33:54,206][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:33:54,207][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:33:54,207][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:33:54,207][PyLogger][INFO]: World size: 2
[2024-02-27 12:33:54,209][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:33:54,209][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:33:54,209][PyLogger][INFO]: World size: 2
[2024-02-27 12:33:57,899][PyLogger][INFO]: Rank[0]: val_accuracy: 0.15220
[2024-02-27 12:33:57,899][PyLogger][INFO]: Rank[1]: val_accuracy: 0.15220
2024-02-27 12:33:59,369 ignite.distributed.launcher.Parallel INFO: End of run
new_size=14 done.
2024-02-27 12:34:05,361 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:34:05,361 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:34:05,361 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x154e5acae3e0>' in 2 processes
2024-02-27 12:34:15,314 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:34:15,720 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x153cb5940e60>}
[2024-02-27 12:34:15,835][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:34:15,835][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:34:15,835][PyLogger][INFO]: World size: 2
[2024-02-27 12:34:15,838][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:34:15,838][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:34:15,838][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:34:15,838][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:34:15,839][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:34:15,839][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:34:15,839][PyLogger][INFO]: World size: 2
[2024-02-27 12:34:19,634][PyLogger][INFO]: Rank[1]: val_accuracy: 0.22590
[2024-02-27 12:34:19,634][PyLogger][INFO]: Rank[0]: val_accuracy: 0.22590
2024-02-27 12:34:21,087 ignite.distributed.launcher.Parallel INFO: End of run
new_size=15 done.
2024-02-27 12:34:27,592 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:34:27,592 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:34:27,592 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14ca6439e3e0>' in 2 processes
2024-02-27 12:34:37,529 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:34:37,963 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x153a23110ad0>}
[2024-02-27 12:34:38,078][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:34:38,078][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:34:38,078][PyLogger][INFO]: World size: 2
[2024-02-27 12:34:38,082][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:34:38,082][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:34:38,083][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:34:38,083][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:34:38,084][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:34:38,084][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:34:38,084][PyLogger][INFO]: World size: 2
[2024-02-27 12:34:41,885][PyLogger][INFO]: Rank[0]: val_accuracy: 0.25660
[2024-02-27 12:34:41,885][PyLogger][INFO]: Rank[1]: val_accuracy: 0.25660
2024-02-27 12:34:43,362 ignite.distributed.launcher.Parallel INFO: End of run
new_size=16 done.
2024-02-27 12:34:49,333 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:34:49,333 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:34:49,333 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x153377c223e0>' in 2 processes
2024-02-27 12:34:59,365 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:34:59,809 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x145aa4e082c0>}
[2024-02-27 12:34:59,924][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:34:59,924][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:34:59,924][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:34:59,924][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:34:59,926][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:34:59,926][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:34:59,926][PyLogger][INFO]: World size: 2
[2024-02-27 12:34:59,928][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:34:59,928][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:34:59,928][PyLogger][INFO]: World size: 2
[2024-02-27 12:35:03,907][PyLogger][INFO]: Rank[1]: val_accuracy: 0.60410
[2024-02-27 12:35:03,907][PyLogger][INFO]: Rank[0]: val_accuracy: 0.60410
2024-02-27 12:35:05,400 ignite.distributed.launcher.Parallel INFO: End of run
new_size=17 done.
2024-02-27 12:35:10,947 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:35:10,947 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:35:10,947 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x153c27d663e0>' in 2 processes
2024-02-27 12:35:21,556 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:35:21,986 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14602ee09d00>}
[2024-02-27 12:35:22,099][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:35:22,099][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:35:22,099][PyLogger][INFO]: World size: 2
[2024-02-27 12:35:22,105][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:35:22,105][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:35:22,105][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:35:22,105][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:35:22,106][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:35:22,106][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:35:22,106][PyLogger][INFO]: World size: 2
[2024-02-27 12:35:26,046][PyLogger][INFO]: Rank[1]: val_accuracy: 0.66190
[2024-02-27 12:35:26,046][PyLogger][INFO]: Rank[0]: val_accuracy: 0.66190
2024-02-27 12:35:27,106 ignite.distributed.launcher.Parallel INFO: End of run
new_size=18 done.
2024-02-27 12:35:33,281 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:35:33,281 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:35:33,281 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1480eaff23e0>' in 2 processes
2024-02-27 12:35:44,462 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:35:44,897 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x149efca42540>}
[2024-02-27 12:35:45,020][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:35:45,020][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:35:45,020][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:35:45,020][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:35:45,024][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:35:45,024][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:35:45,024][PyLogger][INFO]: World size: 2
[2024-02-27 12:35:45,036][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:35:45,036][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:35:45,036][PyLogger][INFO]: World size: 2
[2024-02-27 12:35:49,112][PyLogger][INFO]: Rank[0]: val_accuracy: 0.73100
[2024-02-27 12:35:49,112][PyLogger][INFO]: Rank[1]: val_accuracy: 0.73100
2024-02-27 12:35:50,192 ignite.distributed.launcher.Parallel INFO: End of run
new_size=19 done.
2024-02-27 12:35:55,335 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:35:55,335 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:35:55,335 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x15168cec23e0>' in 2 processes
2024-02-27 12:36:05,948 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:36:06,397 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14eed0d00920>}
[2024-02-27 12:36:06,511][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:36:06,511][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:36:06,511][PyLogger][INFO]: World size: 2
[2024-02-27 12:36:06,514][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:36:06,514][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:36:06,514][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:36:06,514][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:36:06,515][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:36:06,515][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:36:06,515][PyLogger][INFO]: World size: 2
[2024-02-27 12:36:10,647][PyLogger][INFO]: Rank[1]: val_accuracy: 0.76540
[2024-02-27 12:36:10,647][PyLogger][INFO]: Rank[0]: val_accuracy: 0.76540
2024-02-27 12:36:12,245 ignite.distributed.launcher.Parallel INFO: End of run
new_size=20 done.
2024-02-27 12:36:17,465 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:36:17,465 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:36:17,465 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x152122ad23e0>' in 2 processes
2024-02-27 12:36:25,245 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:36:25,649 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14796cd98ec0>}
[2024-02-27 12:36:25,761][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:36:25,761][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:36:25,761][PyLogger][INFO]: World size: 2
[2024-02-27 12:36:25,767][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:36:25,767][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:36:25,767][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:36:25,767][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:36:25,768][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:36:25,768][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:36:25,768][PyLogger][INFO]: World size: 2
[2024-02-27 12:36:29,951][PyLogger][INFO]: Rank[1]: val_accuracy: 0.79400
[2024-02-27 12:36:29,951][PyLogger][INFO]: Rank[0]: val_accuracy: 0.79400
2024-02-27 12:36:31,427 ignite.distributed.launcher.Parallel INFO: End of run
new_size=21 done.
2024-02-27 12:36:35,890 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:36:35,890 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:36:35,890 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1482db3663e0>' in 2 processes
2024-02-27 12:36:45,038 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:36:45,452 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14a55d254e30>}
[2024-02-27 12:36:45,568][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:36:45,568][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:36:45,568][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:36:45,568][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:36:45,569][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:36:45,569][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:36:45,569][PyLogger][INFO]: World size: 2
[2024-02-27 12:36:45,571][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:36:45,571][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:36:45,571][PyLogger][INFO]: World size: 2
[2024-02-27 12:36:49,790][PyLogger][INFO]: Rank[0]: val_accuracy: 0.81700
[2024-02-27 12:36:49,790][PyLogger][INFO]: Rank[1]: val_accuracy: 0.81700
2024-02-27 12:36:51,324 ignite.distributed.launcher.Parallel INFO: End of run
new_size=22 done.
2024-02-27 12:36:56,694 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:36:56,694 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:36:56,694 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x152eb6e923e0>' in 2 processes
2024-02-27 12:37:05,474 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:37:05,881 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14d1fd8f5940>}
[2024-02-27 12:37:05,992][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:37:05,992][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:37:05,992][PyLogger][INFO]: World size: 2
[2024-02-27 12:37:05,996][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:37:05,996][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:37:05,996][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:37:05,996][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:37:05,998][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:37:05,998][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:37:05,998][PyLogger][INFO]: World size: 2
[2024-02-27 12:37:10,314][PyLogger][INFO]: Rank[0]: val_accuracy: 0.83740
[2024-02-27 12:37:10,314][PyLogger][INFO]: Rank[1]: val_accuracy: 0.83740
2024-02-27 12:37:11,812 ignite.distributed.launcher.Parallel INFO: End of run
new_size=23 done.
2024-02-27 12:37:17,334 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:37:17,334 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:37:17,334 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1513fa1263e0>' in 2 processes
2024-02-27 12:37:26,967 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:37:27,393 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14c1fae189b0>}
[2024-02-27 12:37:27,510][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:37:27,510][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:37:27,510][PyLogger][INFO]: World size: 2
[2024-02-27 12:37:27,512][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:37:27,512][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:37:27,512][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:37:27,512][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:37:27,513][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:37:27,513][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:37:27,513][PyLogger][INFO]: World size: 2
[2024-02-27 12:37:31,963][PyLogger][INFO]: Rank[0]: val_accuracy: 0.86210
[2024-02-27 12:37:31,963][PyLogger][INFO]: Rank[1]: val_accuracy: 0.86210
2024-02-27 12:37:33,471 ignite.distributed.launcher.Parallel INFO: End of run
new_size=24 done.
2024-02-27 12:37:38,956 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:37:38,956 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:37:38,956 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14e2a63ea3e0>' in 2 processes
2024-02-27 12:37:49,260 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:37:49,680 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x149b25158320>}
[2024-02-27 12:37:49,799][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:37:49,799][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:37:49,799][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:37:49,799][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:37:49,801][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:37:49,801][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:37:49,801][PyLogger][INFO]: World size: 2
[2024-02-27 12:37:49,801][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:37:49,801][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:37:49,801][PyLogger][INFO]: World size: 2
[2024-02-27 12:37:54,260][PyLogger][INFO]: Rank[0]: val_accuracy: 0.86520
[2024-02-27 12:37:54,260][PyLogger][INFO]: Rank[1]: val_accuracy: 0.86520
2024-02-27 12:37:55,790 ignite.distributed.launcher.Parallel INFO: End of run
new_size=25 done.
2024-02-27 12:38:00,884 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:38:00,884 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:38:00,884 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14e5f12be3e0>' in 2 processes
2024-02-27 12:38:10,360 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:38:10,776 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x148a067c1b80>}
[2024-02-27 12:38:10,894][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:38:10,894][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:38:10,894][PyLogger][INFO]: World size: 2
[2024-02-27 12:38:10,895][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:38:10,895][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:38:10,895][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:38:10,896][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:38:10,897][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:38:10,897][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:38:10,897][PyLogger][INFO]: World size: 2
[2024-02-27 12:38:15,491][PyLogger][INFO]: Rank[0]: val_accuracy: 0.88170
[2024-02-27 12:38:15,491][PyLogger][INFO]: Rank[1]: val_accuracy: 0.88170
2024-02-27 12:38:17,025 ignite.distributed.launcher.Parallel INFO: End of run
new_size=26 done.
2024-02-27 12:38:21,967 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:38:21,967 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:38:21,967 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1519a18063e0>' in 2 processes
2024-02-27 12:38:31,806 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:38:32,256 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x154986a880e0>}
[2024-02-27 12:38:32,368][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:38:32,369][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:38:32,369][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:38:32,369][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:38:32,370][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:38:32,370][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:38:32,370][PyLogger][INFO]: World size: 2
[2024-02-27 12:38:32,373][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:38:32,373][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:38:32,373][PyLogger][INFO]: World size: 2
[2024-02-27 12:38:36,996][PyLogger][INFO]: Rank[0]: val_accuracy: 0.89400
[2024-02-27 12:38:36,996][PyLogger][INFO]: Rank[1]: val_accuracy: 0.89400
2024-02-27 12:38:38,501 ignite.distributed.launcher.Parallel INFO: End of run
new_size=27 done.
2024-02-27 12:38:44,187 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:38:44,187 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:38:44,187 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x148312bfe3e0>' in 2 processes
2024-02-27 12:38:53,304 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:38:53,797 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14f83d694200>}
[2024-02-27 12:38:53,917][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:38:53,917][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:38:53,917][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:38:53,918][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:38:53,918][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:38:53,919][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:38:53,919][PyLogger][INFO]: World size: 2
[2024-02-27 12:38:53,919][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:38:53,919][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:38:53,919][PyLogger][INFO]: World size: 2
[2024-02-27 12:38:58,587][PyLogger][INFO]: Rank[0]: val_accuracy: 0.90590
[2024-02-27 12:38:58,587][PyLogger][INFO]: Rank[1]: val_accuracy: 0.90590
2024-02-27 12:39:00,129 ignite.distributed.launcher.Parallel INFO: End of run
new_size=28 done.
2024-02-27 12:39:05,267 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:39:05,267 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:39:05,267 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14876796a3e0>' in 2 processes
2024-02-27 12:39:14,930 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:39:15,340 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x146aaa448560>}
[2024-02-27 12:39:15,458][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:39:15,459][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:39:15,459][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:39:15,459][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:39:15,460][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:39:15,460][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:39:15,460][PyLogger][INFO]: World size: 2
[2024-02-27 12:39:15,469][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:39:15,469][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:39:15,469][PyLogger][INFO]: World size: 2
[2024-02-27 12:39:20,319][PyLogger][INFO]: Rank[0]: val_accuracy: 0.91250
[2024-02-27 12:39:20,319][PyLogger][INFO]: Rank[1]: val_accuracy: 0.91250
2024-02-27 12:39:21,846 ignite.distributed.launcher.Parallel INFO: End of run
new_size=29 done.
2024-02-27 12:39:27,110 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:39:27,110 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:39:27,110 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14bef59ce3e0>' in 2 processes
2024-02-27 12:39:36,723 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:39:37,148 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x155232640bc0>}
[2024-02-27 12:39:37,265][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:39:37,265][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:39:37,265][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:39:37,265][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:39:37,266][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:39:37,266][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:39:37,266][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:39:37,266][PyLogger][INFO]: World size: 2
[2024-02-27 12:39:37,266][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:39:37,266][PyLogger][INFO]: World size: 2
[2024-02-27 12:39:42,115][PyLogger][INFO]: Rank[1]: val_accuracy: 0.92360
[2024-02-27 12:39:42,115][PyLogger][INFO]: Rank[0]: val_accuracy: 0.92360
2024-02-27 12:39:43,640 ignite.distributed.launcher.Parallel INFO: End of run
new_size=30 done.
2024-02-27 12:39:49,436 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:39:49,436 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:39:49,436 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14e659b4a3e0>' in 2 processes
2024-02-27 12:39:58,834 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:39:59,282 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1533b54623c0>}
[2024-02-27 12:39:59,384][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:39:59,385][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:39:59,385][PyLogger][INFO]: World size: 2
[2024-02-27 12:39:59,398][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:39:59,398][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:39:59,398][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:39:59,398][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:39:59,399][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:39:59,399][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:39:59,399][PyLogger][INFO]: World size: 2
[2024-02-27 12:40:04,533][PyLogger][INFO]: Rank[1]: val_accuracy: 0.93460
[2024-02-27 12:40:04,533][PyLogger][INFO]: Rank[0]: val_accuracy: 0.93460
2024-02-27 12:40:05,634 ignite.distributed.launcher.Parallel INFO: End of run
new_size=31 done.
2024-02-27 12:40:10,869 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:40:10,869 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:40:10,869 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x150a76a4a3e0>' in 2 processes
2024-02-27 12:40:21,376 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:40:21,794 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14eb4d198b00>}
[2024-02-27 12:40:21,912][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:40:21,912][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:40:21,912][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:40:21,912][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:40:21,913][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:40:21,913][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:40:21,913][PyLogger][INFO]: World size: 2
[2024-02-27 12:40:21,917][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:40:21,917][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:40:21,917][PyLogger][INFO]: World size: 2
[2024-02-27 12:40:27,111][PyLogger][INFO]: Rank[0]: val_accuracy: 0.93530
[2024-02-27 12:40:27,111][PyLogger][INFO]: Rank[1]: val_accuracy: 0.93530
2024-02-27 12:40:28,613 ignite.distributed.launcher.Parallel INFO: End of run
new_size=32 done.
2024-02-27 12:40:32,688 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:40:32,688 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:40:32,688 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14be2cc323e0>' in 2 processes
2024-02-27 12:40:41,848 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:40:42,286 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14665b020c80>}
[2024-02-27 12:40:42,403][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:40:42,404][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:40:42,404][PyLogger][INFO]: World size: 2
[2024-02-27 12:40:42,406][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:40:42,406][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:40:42,406][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:40:42,406][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:40:42,407][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:40:42,407][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:40:42,407][PyLogger][INFO]: World size: 2
[2024-02-27 12:40:48,280][PyLogger][INFO]: Rank[0]: val_accuracy: 0.92770
[2024-02-27 12:40:48,280][PyLogger][INFO]: Rank[1]: val_accuracy: 0.92770
2024-02-27 12:40:49,901 ignite.distributed.launcher.Parallel INFO: End of run
new_size=33 done.
2024-02-27 12:40:56,065 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:40:56,065 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:40:56,065 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x146634f523e0>' in 2 processes
2024-02-27 12:41:07,429 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:41:07,856 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x145cbaf7cd70>}
[2024-02-27 12:41:07,972][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:41:07,972][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:41:07,972][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:41:07,972][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:41:07,973][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:41:07,974][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:41:07,974][PyLogger][INFO]: World size: 2
[2024-02-27 12:41:07,980][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:41:07,980][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:41:07,980][PyLogger][INFO]: World size: 2
[2024-02-27 12:41:13,869][PyLogger][INFO]: Rank[1]: val_accuracy: 0.92170
[2024-02-27 12:41:13,869][PyLogger][INFO]: Rank[0]: val_accuracy: 0.92170
2024-02-27 12:41:15,453 ignite.distributed.launcher.Parallel INFO: End of run
new_size=34 done.
2024-02-27 12:41:21,305 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:41:21,305 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:41:21,305 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x151af2b463e0>' in 2 processes
2024-02-27 12:41:29,297 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:41:29,701 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x148a1ee78f20>}
[2024-02-27 12:41:29,819][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:41:29,820][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:41:29,820][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:41:29,820][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:41:29,821][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:41:29,821][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:41:29,821][PyLogger][INFO]: World size: 2
[2024-02-27 12:41:29,821][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:41:29,822][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:41:29,822][PyLogger][INFO]: World size: 2
[2024-02-27 12:41:35,714][PyLogger][INFO]: Rank[0]: val_accuracy: 0.91060
[2024-02-27 12:41:35,714][PyLogger][INFO]: Rank[1]: val_accuracy: 0.91060
2024-02-27 12:41:37,297 ignite.distributed.launcher.Parallel INFO: End of run
new_size=35 done.
2024-02-27 12:41:42,277 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:41:42,277 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:41:42,277 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x15382d4063e0>' in 2 processes
2024-02-27 12:41:51,387 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:41:51,799 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14c2ae6b02f0>}
[2024-02-27 12:41:51,914][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:41:51,914][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:41:51,914][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:41:51,914][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:41:51,916][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:41:51,916][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:41:51,916][PyLogger][INFO]: World size: 2
[2024-02-27 12:41:51,921][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:41:51,921][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:41:51,921][PyLogger][INFO]: World size: 2
[2024-02-27 12:41:57,864][PyLogger][INFO]: Rank[0]: val_accuracy: 0.89250
[2024-02-27 12:41:57,864][PyLogger][INFO]: Rank[1]: val_accuracy: 0.89250
2024-02-27 12:41:59,432 ignite.distributed.launcher.Parallel INFO: End of run
new_size=36 done.
2024-02-27 12:42:05,313 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:42:05,313 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:42:05,313 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x148432d023e0>' in 2 processes
2024-02-27 12:42:16,087 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:42:16,506 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1509f772c6e0>}
[2024-02-27 12:42:16,621][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:42:16,621][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:42:16,622][PyLogger][INFO]: World size: 2
[2024-02-27 12:42:16,624][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:42:16,624][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:42:16,624][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:42:16,624][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:42:16,625][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:42:16,625][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:42:16,625][PyLogger][INFO]: World size: 2
[2024-02-27 12:42:21,790][PyLogger][INFO]: Rank[0]: val_accuracy: 0.13370
[2024-02-27 12:42:21,790][PyLogger][INFO]: Rank[1]: val_accuracy: 0.13370
2024-02-27 12:42:22,904 ignite.distributed.launcher.Parallel INFO: End of run
new_size=2 done.
2024-02-27 12:42:27,082 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:42:27,082 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:42:27,082 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1512903b23e0>' in 2 processes
2024-02-27 12:42:35,753 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:42:36,153 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14ddcbf406e0>}
[2024-02-27 12:42:36,265][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:42:36,265][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:42:36,266][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:42:36,266][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:42:36,266][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:42:36,267][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:42:36,267][PyLogger][INFO]: World size: 2
[2024-02-27 12:42:36,271][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:42:36,272][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:42:36,272][PyLogger][INFO]: World size: 2
[2024-02-27 12:42:41,456][PyLogger][INFO]: Rank[0]: val_accuracy: 0.16470
[2024-02-27 12:42:41,456][PyLogger][INFO]: Rank[1]: val_accuracy: 0.16470
2024-02-27 12:42:43,003 ignite.distributed.launcher.Parallel INFO: End of run
new_size=3 done.
2024-02-27 12:42:48,422 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:42:48,422 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:42:48,422 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x145b798863e0>' in 2 processes
2024-02-27 12:42:59,604 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:43:00,042 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x150e65498c80>}
[2024-02-27 12:43:00,154][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:43:00,155][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:43:00,155][PyLogger][INFO]: World size: 2
[2024-02-27 12:43:00,160][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:43:00,160][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:43:00,160][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:43:00,160][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:43:00,161][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:43:00,161][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:43:00,161][PyLogger][INFO]: World size: 2
[2024-02-27 12:43:05,374][PyLogger][INFO]: Rank[1]: val_accuracy: 0.18330
[2024-02-27 12:43:05,374][PyLogger][INFO]: Rank[0]: val_accuracy: 0.18330
2024-02-27 12:43:06,933 ignite.distributed.launcher.Parallel INFO: End of run
new_size=4 done.
2024-02-27 12:43:13,249 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:43:13,249 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:43:13,249 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1534ea75a3e0>' in 2 processes
2024-02-27 12:43:23,837 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:43:24,272 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14a3f7159b80>}
[2024-02-27 12:43:24,383][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:43:24,383][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:43:24,383][PyLogger][INFO]: World size: 2
[2024-02-27 12:43:24,389][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:43:24,389][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:43:24,389][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:43:24,390][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:43:24,391][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:43:24,391][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:43:24,391][PyLogger][INFO]: World size: 2
[2024-02-27 12:43:29,672][PyLogger][INFO]: Rank[1]: val_accuracy: 0.17830
[2024-02-27 12:43:29,672][PyLogger][INFO]: Rank[0]: val_accuracy: 0.17830
2024-02-27 12:43:31,222 ignite.distributed.launcher.Parallel INFO: End of run
new_size=5 done.
2024-02-27 12:43:36,540 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:43:36,540 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:43:36,540 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14943921e3e0>' in 2 processes
2024-02-27 12:43:46,643 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:43:47,063 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x145c5d73c380>}
[2024-02-27 12:43:47,178][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:43:47,178][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:43:47,178][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:43:47,178][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:43:47,179][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:43:47,179][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:43:47,179][PyLogger][INFO]: World size: 2
[2024-02-27 12:43:47,183][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:43:47,183][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:43:47,183][PyLogger][INFO]: World size: 2
[2024-02-27 12:43:52,327][PyLogger][INFO]: Rank[0]: val_accuracy: 0.18050
[2024-02-27 12:43:52,327][PyLogger][INFO]: Rank[1]: val_accuracy: 0.18050
2024-02-27 12:43:53,875 ignite.distributed.launcher.Parallel INFO: End of run
new_size=6 done.
2024-02-27 12:43:59,600 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:43:59,600 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:43:59,600 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14c4cd0163e0>' in 2 processes
2024-02-27 12:44:10,621 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:44:11,055 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14fdd31ae5a0>}
[2024-02-27 12:44:11,173][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:44:11,173][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:44:11,173][PyLogger][INFO]: World size: 2
[2024-02-27 12:44:11,175][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:44:11,175][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:44:11,175][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:44:11,175][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:44:11,176][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:44:11,176][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:44:11,176][PyLogger][INFO]: World size: 2
[2024-02-27 12:44:16,371][PyLogger][INFO]: Rank[0]: val_accuracy: 0.19510
[2024-02-27 12:44:16,371][PyLogger][INFO]: Rank[1]: val_accuracy: 0.19510
2024-02-27 12:44:17,899 ignite.distributed.launcher.Parallel INFO: End of run
new_size=7 done.
2024-02-27 12:44:23,854 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:44:23,854 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:44:23,855 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x150d51a8a3e0>' in 2 processes
2024-02-27 12:44:33,564 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:44:33,965 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14ee2f5c4e00>}
[2024-02-27 12:44:34,075][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:44:34,075][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:44:34,075][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:44:34,075][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:44:34,076][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:44:34,076][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:44:34,076][PyLogger][INFO]: World size: 2
[2024-02-27 12:44:34,085][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:44:34,086][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:44:34,086][PyLogger][INFO]: World size: 2
[2024-02-27 12:44:39,181][PyLogger][INFO]: Rank[0]: val_accuracy: 0.19300
[2024-02-27 12:44:39,181][PyLogger][INFO]: Rank[1]: val_accuracy: 0.19300
2024-02-27 12:44:40,829 ignite.distributed.launcher.Parallel INFO: End of run
new_size=8 done.
2024-02-27 12:44:46,489 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:44:46,489 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:44:46,489 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14f39a18e3e0>' in 2 processes
2024-02-27 12:44:55,862 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:44:56,391 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14dd78cc42c0>}
[2024-02-27 12:44:56,506][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:44:56,507][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:44:56,507][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:44:56,507][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:44:56,508][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:44:56,509][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:44:56,509][PyLogger][INFO]: World size: 2
[2024-02-27 12:44:56,509][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:44:56,510][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:44:56,510][PyLogger][INFO]: World size: 2
[2024-02-27 12:45:01,754][PyLogger][INFO]: Rank[1]: val_accuracy: 0.23520
[2024-02-27 12:45:01,754][PyLogger][INFO]: Rank[0]: val_accuracy: 0.23520
2024-02-27 12:45:03,302 ignite.distributed.launcher.Parallel INFO: End of run
new_size=9 done.
2024-02-27 12:45:08,518 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:45:08,518 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:45:08,518 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14c9bc0663e0>' in 2 processes
2024-02-27 12:45:18,190 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:45:18,637 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x151ed88001a0>}
[2024-02-27 12:45:18,751][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:45:18,751][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:45:18,751][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:45:18,751][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:45:18,752][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:45:18,753][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:45:18,753][PyLogger][INFO]: World size: 2
[2024-02-27 12:45:18,754][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:45:18,754][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:45:18,754][PyLogger][INFO]: World size: 2
[2024-02-27 12:45:23,951][PyLogger][INFO]: Rank[0]: val_accuracy: 0.25890
[2024-02-27 12:45:23,952][PyLogger][INFO]: Rank[1]: val_accuracy: 0.25890
2024-02-27 12:45:25,077 ignite.distributed.launcher.Parallel INFO: End of run
new_size=10 done.
2024-02-27 12:45:30,823 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:45:30,823 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:45:30,823 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14a9141de3e0>' in 2 processes
2024-02-27 12:45:40,132 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:45:40,537 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14c53d420140>}
[2024-02-27 12:45:40,655][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:45:40,655][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:45:40,655][PyLogger][INFO]: World size: 2
[2024-02-27 12:45:40,658][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:45:40,658][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:45:40,658][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:45:40,658][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:45:40,659][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:45:40,659][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:45:40,659][PyLogger][INFO]: World size: 2
[2024-02-27 12:45:45,756][PyLogger][INFO]: Rank[1]: val_accuracy: 0.28990
[2024-02-27 12:45:45,756][PyLogger][INFO]: Rank[0]: val_accuracy: 0.28990
2024-02-27 12:45:47,323 ignite.distributed.launcher.Parallel INFO: End of run
new_size=11 done.
2024-02-27 12:45:52,728 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:45:52,728 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:45:52,728 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14b1beea63e0>' in 2 processes
2024-02-27 12:46:03,280 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:46:03,707 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x152083a18b60>}
[2024-02-27 12:46:03,825][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:46:03,825][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:46:03,825][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:46:03,825][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:46:03,826][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:46:03,826][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:46:03,826][PyLogger][INFO]: World size: 2
[2024-02-27 12:46:03,837][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:46:03,837][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:46:03,837][PyLogger][INFO]: World size: 2
[2024-02-27 12:46:09,102][PyLogger][INFO]: Rank[1]: val_accuracy: 0.30080
[2024-02-27 12:46:09,102][PyLogger][INFO]: Rank[0]: val_accuracy: 0.30080
2024-02-27 12:46:10,682 ignite.distributed.launcher.Parallel INFO: End of run
new_size=12 done.
2024-02-27 12:46:16,186 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:46:16,186 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:46:16,186 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14720eb363e0>' in 2 processes
2024-02-27 12:46:27,260 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:46:27,709 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14585a488f50>}
[2024-02-27 12:46:27,825][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:46:27,825][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:46:27,825][PyLogger][INFO]: World size: 2
[2024-02-27 12:46:27,830][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:46:27,830][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:46:27,830][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:46:27,830][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:46:27,831][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:46:27,831][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:46:27,831][PyLogger][INFO]: World size: 2
[2024-02-27 12:46:33,092][PyLogger][INFO]: Rank[0]: val_accuracy: 0.30760
[2024-02-27 12:46:33,092][PyLogger][INFO]: Rank[1]: val_accuracy: 0.30760
2024-02-27 12:46:34,626 ignite.distributed.launcher.Parallel INFO: End of run
new_size=13 done.
2024-02-27 12:46:40,338 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:46:40,338 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:46:40,338 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x153adba923e0>' in 2 processes
2024-02-27 12:46:51,213 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:46:51,642 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14ad9c680680>}
[2024-02-27 12:46:51,761][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:46:51,761][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:46:51,761][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:46:51,761][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:46:51,762][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:46:51,762][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:46:51,762][PyLogger][INFO]: World size: 2
[2024-02-27 12:46:51,764][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:46:51,765][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:46:51,765][PyLogger][INFO]: World size: 2
[2024-02-27 12:46:56,911][PyLogger][INFO]: Rank[0]: val_accuracy: 0.34070
[2024-02-27 12:46:56,911][PyLogger][INFO]: Rank[1]: val_accuracy: 0.34070
2024-02-27 12:46:58,420 ignite.distributed.launcher.Parallel INFO: End of run
new_size=14 done.
2024-02-27 12:47:04,112 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:47:04,112 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:47:04,112 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1522c79023e0>' in 2 processes
2024-02-27 12:47:14,571 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:47:14,982 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x1531c1b9cb00>}
[2024-02-27 12:47:15,100][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:47:15,100][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:47:15,100][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:47:15,100][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:47:15,102][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:47:15,102][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:47:15,102][PyLogger][INFO]: World size: 2
[2024-02-27 12:47:15,102][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:47:15,102][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:47:15,102][PyLogger][INFO]: World size: 2
[2024-02-27 12:47:20,325][PyLogger][INFO]: Rank[1]: val_accuracy: 0.36150
[2024-02-27 12:47:20,325][PyLogger][INFO]: Rank[0]: val_accuracy: 0.36150
2024-02-27 12:47:21,883 ignite.distributed.launcher.Parallel INFO: End of run
new_size=15 done.
2024-02-27 12:47:26,725 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:47:26,725 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:47:26,725 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x15041919e3e0>' in 2 processes
2024-02-27 12:47:34,632 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:47:35,059 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x154af7708ec0>}
[2024-02-27 12:47:35,177][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:47:35,177][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:47:35,177][PyLogger][INFO]: World size: 2
[2024-02-27 12:47:35,180][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:47:35,180][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:47:35,181][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:47:35,181][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:47:35,182][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:47:35,182][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:47:35,182][PyLogger][INFO]: World size: 2
[2024-02-27 12:47:40,320][PyLogger][INFO]: Rank[1]: val_accuracy: 0.40050
[2024-02-27 12:47:40,320][PyLogger][INFO]: Rank[0]: val_accuracy: 0.40050
2024-02-27 12:47:41,839 ignite.distributed.launcher.Parallel INFO: End of run
new_size=16 done.
2024-02-27 12:47:46,663 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:47:46,663 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:47:46,663 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14ffc90b63e0>' in 2 processes
2024-02-27 12:47:55,845 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:47:56,280 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x146f81f38e00>}
[2024-02-27 12:47:56,400][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:47:56,400][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:47:56,401][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:47:56,401][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:47:56,401][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:47:56,401][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:47:56,401][PyLogger][INFO]: World size: 2
[2024-02-27 12:47:56,402][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:47:56,402][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:47:56,402][PyLogger][INFO]: World size: 2
[2024-02-27 12:48:01,576][PyLogger][INFO]: Rank[0]: val_accuracy: 0.40630
[2024-02-27 12:48:01,577][PyLogger][INFO]: Rank[1]: val_accuracy: 0.40630
2024-02-27 12:48:03,121 ignite.distributed.launcher.Parallel INFO: End of run
new_size=17 done.
2024-02-27 12:48:08,322 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:48:08,322 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:48:08,322 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x148223e563e0>' in 2 processes
2024-02-27 12:48:19,196 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:48:19,617 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14a087b5c740>}
[2024-02-27 12:48:19,733][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:48:19,733][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:48:19,733][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:48:19,733][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:48:19,734][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:48:19,734][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:48:19,734][PyLogger][INFO]: World size: 2
[2024-02-27 12:48:19,740][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:48:19,741][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:48:19,741][PyLogger][INFO]: World size: 2
[2024-02-27 12:48:24,913][PyLogger][INFO]: Rank[1]: val_accuracy: 0.40870
[2024-02-27 12:48:24,913][PyLogger][INFO]: Rank[0]: val_accuracy: 0.40870
2024-02-27 12:48:26,443 ignite.distributed.launcher.Parallel INFO: End of run
new_size=18 done.
2024-02-27 12:48:31,781 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:48:31,781 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:48:31,781 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14e4220f63e0>' in 2 processes
2024-02-27 12:48:42,534 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:48:42,972 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14ac83dce7e0>}
[2024-02-27 12:48:43,087][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:48:43,087][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:48:43,087][PyLogger][INFO]: World size: 2
[2024-02-27 12:48:43,089][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:48:43,089][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:48:43,089][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:48:43,090][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:48:43,091][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:48:43,091][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:48:43,091][PyLogger][INFO]: World size: 2
[2024-02-27 12:48:48,224][PyLogger][INFO]: Rank[0]: val_accuracy: 0.43760
[2024-02-27 12:48:48,224][PyLogger][INFO]: Rank[1]: val_accuracy: 0.43760
2024-02-27 12:48:49,905 ignite.distributed.launcher.Parallel INFO: End of run
new_size=19 done.
2024-02-27 12:48:55,509 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:48:55,509 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:48:55,509 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x15554a0a63e0>' in 2 processes
2024-02-27 12:49:05,733 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:49:06,144 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14a0f039c9e0>}
[2024-02-27 12:49:06,256][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:49:06,256][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:49:06,256][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:49:06,256][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:49:06,257][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:49:06,257][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:49:06,257][PyLogger][INFO]: World size: 2
[2024-02-27 12:49:06,261][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:49:06,261][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:49:06,261][PyLogger][INFO]: World size: 2
[2024-02-27 12:49:11,585][PyLogger][INFO]: Rank[0]: val_accuracy: 0.43220
[2024-02-27 12:49:11,585][PyLogger][INFO]: Rank[1]: val_accuracy: 0.43220
2024-02-27 12:49:12,717 ignite.distributed.launcher.Parallel INFO: End of run
new_size=20 done.
2024-02-27 12:49:17,774 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:49:17,774 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:49:17,774 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x15182a2423e0>' in 2 processes
2024-02-27 12:49:25,934 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:49:26,336 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14a119810680>}
[2024-02-27 12:49:26,449][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:49:26,449][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:49:26,449][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:49:26,449][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:49:26,450][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:49:26,450][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:49:26,450][PyLogger][INFO]: World size: 2
[2024-02-27 12:49:26,455][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:49:26,455][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:49:26,455][PyLogger][INFO]: World size: 2
[2024-02-27 12:49:31,723][PyLogger][INFO]: Rank[0]: val_accuracy: 0.49670
[2024-02-27 12:49:31,724][PyLogger][INFO]: Rank[1]: val_accuracy: 0.49670
2024-02-27 12:49:33,247 ignite.distributed.launcher.Parallel INFO: End of run
new_size=21 done.
2024-02-27 12:49:37,513 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:49:37,513 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:49:37,513 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x145fd287e3e0>' in 2 processes
2024-02-27 12:49:46,193 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:49:46,633 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14c255407e90>}
[2024-02-27 12:49:46,751][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:49:46,751][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:49:46,751][PyLogger][INFO]: World size: 2
[2024-02-27 12:49:46,755][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:49:46,755][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:49:46,755][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:49:46,755][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:49:46,756][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:49:46,756][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:49:46,756][PyLogger][INFO]: World size: 2
[2024-02-27 12:49:51,925][PyLogger][INFO]: Rank[1]: val_accuracy: 0.51040
[2024-02-27 12:49:51,925][PyLogger][INFO]: Rank[0]: val_accuracy: 0.51040
2024-02-27 12:49:53,475 ignite.distributed.launcher.Parallel INFO: End of run
new_size=22 done.
2024-02-27 12:49:58,881 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:49:58,881 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:49:58,881 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1535652ce3e0>' in 2 processes
2024-02-27 12:50:06,752 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:50:07,175 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x147f0e8ed880>}
[2024-02-27 12:50:07,292][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:50:07,292][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:50:07,293][PyLogger][INFO]: World size: 2
[2024-02-27 12:50:07,295][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:50:07,295][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:50:07,295][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:50:07,295][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:50:07,296][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:50:07,296][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:50:07,296][PyLogger][INFO]: World size: 2
[2024-02-27 12:50:12,513][PyLogger][INFO]: Rank[0]: val_accuracy: 0.55650
[2024-02-27 12:50:12,513][PyLogger][INFO]: Rank[1]: val_accuracy: 0.55650
2024-02-27 12:50:14,054 ignite.distributed.launcher.Parallel INFO: End of run
new_size=23 done.
2024-02-27 12:50:18,963 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:50:18,963 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:50:18,963 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x151296cea3e0>' in 2 processes
2024-02-27 12:50:28,405 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:50:28,818 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x151f29e29400>}
[2024-02-27 12:50:28,933][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:50:28,933][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:50:28,933][PyLogger][INFO]: World size: 2
[2024-02-27 12:50:28,938][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:50:28,938][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:50:28,938][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:50:28,938][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:50:28,940][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:50:28,940][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:50:28,940][PyLogger][INFO]: World size: 2
[2024-02-27 12:50:34,108][PyLogger][INFO]: Rank[1]: val_accuracy: 0.62270
[2024-02-27 12:50:34,108][PyLogger][INFO]: Rank[0]: val_accuracy: 0.62270
2024-02-27 12:50:35,657 ignite.distributed.launcher.Parallel INFO: End of run
new_size=24 done.
2024-02-27 12:50:40,929 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:50:40,929 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:50:40,929 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14810bbda3e0>' in 2 processes
2024-02-27 12:50:51,555 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:50:51,984 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14dbd6a7c740>}
[2024-02-27 12:50:52,098][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:50:52,098][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:50:52,098][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:50:52,098][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:50:52,099][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:50:52,099][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:50:52,099][PyLogger][INFO]: World size: 2
[2024-02-27 12:50:52,101][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:50:52,102][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:50:52,102][PyLogger][INFO]: World size: 2
[2024-02-27 12:50:57,302][PyLogger][INFO]: Rank[1]: val_accuracy: 0.66130
[2024-02-27 12:50:57,302][PyLogger][INFO]: Rank[0]: val_accuracy: 0.66130
2024-02-27 12:50:58,934 ignite.distributed.launcher.Parallel INFO: End of run
new_size=25 done.
2024-02-27 12:51:04,240 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:51:04,240 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:51:04,240 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x155027fc63e0>' in 2 processes
2024-02-27 12:51:14,063 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:51:14,483 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14b0f6004ce0>}
[2024-02-27 12:51:14,593][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:51:14,593][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:51:14,593][PyLogger][INFO]: World size: 2
[2024-02-27 12:51:14,603][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:51:14,603][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:51:14,603][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:51:14,603][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:51:14,604][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:51:14,604][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:51:14,604][PyLogger][INFO]: World size: 2
[2024-02-27 12:51:19,822][PyLogger][INFO]: Rank[0]: val_accuracy: 0.74130
[2024-02-27 12:51:19,822][PyLogger][INFO]: Rank[1]: val_accuracy: 0.74130
2024-02-27 12:51:21,385 ignite.distributed.launcher.Parallel INFO: End of run
new_size=26 done.
2024-02-27 12:51:26,224 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:51:26,224 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:51:26,224 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x146a31ad23e0>' in 2 processes
2024-02-27 12:51:35,506 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:51:35,905 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14e4e32582c0>}
[2024-02-27 12:51:36,023][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:51:36,023][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:51:36,023][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:51:36,023][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:51:36,024][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:51:36,024][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:51:36,025][PyLogger][INFO]: World size: 2
[2024-02-27 12:51:36,025][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:51:36,025][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:51:36,025][PyLogger][INFO]: World size: 2
[2024-02-27 12:51:41,242][PyLogger][INFO]: Rank[1]: val_accuracy: 0.80500
[2024-02-27 12:51:41,242][PyLogger][INFO]: Rank[0]: val_accuracy: 0.80500
2024-02-27 12:51:42,779 ignite.distributed.launcher.Parallel INFO: End of run
new_size=27 done.
2024-02-27 12:51:46,982 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:51:46,982 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:51:46,982 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14bfdacfa3e0>' in 2 processes
2024-02-27 12:51:56,461 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:51:56,891 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14b9f9365880>}
[2024-02-27 12:51:57,010][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:51:57,010][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:51:57,010][PyLogger][INFO]: World size: 2
[2024-02-27 12:51:57,013][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:51:57,013][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:51:57,013][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:51:57,013][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:51:57,014][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:51:57,014][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:51:57,014][PyLogger][INFO]: World size: 2
[2024-02-27 12:52:02,226][PyLogger][INFO]: Rank[0]: val_accuracy: 0.86430
[2024-02-27 12:52:02,226][PyLogger][INFO]: Rank[1]: val_accuracy: 0.86430
2024-02-27 12:52:03,809 ignite.distributed.launcher.Parallel INFO: End of run
new_size=28 done.
2024-02-27 12:52:09,636 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:52:09,636 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:52:09,636 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14fd1bf223e0>' in 2 processes
2024-02-27 12:52:19,475 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:52:19,887 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14cbcc210a40>}
[2024-02-27 12:52:20,006][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:52:20,006][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:52:20,006][PyLogger][INFO]: World size: 2
[2024-02-27 12:52:20,007][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:52:20,007][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:52:20,007][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:52:20,007][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:52:20,008][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:52:20,008][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:52:20,008][PyLogger][INFO]: World size: 2
[2024-02-27 12:52:25,175][PyLogger][INFO]: Rank[1]: val_accuracy: 0.89170
[2024-02-27 12:52:25,175][PyLogger][INFO]: Rank[0]: val_accuracy: 0.89170
2024-02-27 12:52:26,270 ignite.distributed.launcher.Parallel INFO: End of run
new_size=29 done.
2024-02-27 12:52:30,639 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:52:30,639 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:52:30,639 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x146b9f3c23e0>' in 2 processes
2024-02-27 12:52:39,412 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:52:39,850 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14c82f368e60>}
[2024-02-27 12:52:39,964][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:52:39,964][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:52:39,964][PyLogger][INFO]: World size: 2
[2024-02-27 12:52:39,965][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:52:39,965][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:52:39,965][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:52:39,965][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:52:39,966][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:52:39,966][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:52:39,967][PyLogger][INFO]: World size: 2
[2024-02-27 12:52:45,134][PyLogger][INFO]: Rank[0]: val_accuracy: 0.91380
[2024-02-27 12:52:45,135][PyLogger][INFO]: Rank[1]: val_accuracy: 0.91380
2024-02-27 12:52:46,249 ignite.distributed.launcher.Parallel INFO: End of run
new_size=30 done.
2024-02-27 12:52:51,218 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:52:51,218 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:52:51,218 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1518943663e0>' in 2 processes
2024-02-27 12:53:01,042 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:53:01,464 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x150d3cd62e40>}
[2024-02-27 12:53:01,577][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:53:01,577][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:53:01,577][PyLogger][INFO]: World size: 2
[2024-02-27 12:53:01,582][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:53:01,582][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:53:01,583][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:53:01,583][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:53:01,584][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:53:01,584][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:53:01,584][PyLogger][INFO]: World size: 2
[2024-02-27 12:53:06,774][PyLogger][INFO]: Rank[1]: val_accuracy: 0.93520
[2024-02-27 12:53:06,774][PyLogger][INFO]: Rank[0]: val_accuracy: 0.93520
2024-02-27 12:53:08,349 ignite.distributed.launcher.Parallel INFO: End of run
new_size=31 done.
2024-02-27 12:53:14,257 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:53:14,257 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:53:14,257 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x1546d4bd23e0>' in 2 processes
2024-02-27 12:53:23,718 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:53:24,115 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14ad680d2930>}
[2024-02-27 12:53:24,233][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:53:24,233][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:53:24,233][PyLogger][INFO]: World size: 2
[2024-02-27 12:53:24,235][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:53:24,235][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:53:24,235][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:53:24,235][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:53:24,236][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:53:24,236][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:53:24,236][PyLogger][INFO]: World size: 2
[2024-02-27 12:53:29,426][PyLogger][INFO]: Rank[0]: val_accuracy: 0.93530
[2024-02-27 12:53:29,426][PyLogger][INFO]: Rank[1]: val_accuracy: 0.93530
2024-02-27 12:53:30,536 ignite.distributed.launcher.Parallel INFO: End of run
new_size=32 done.
2024-02-27 12:53:35,771 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:53:35,771 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:53:35,772 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x149106e363e0>' in 2 processes
2024-02-27 12:53:46,099 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:53:46,509 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14e94c01d940>}
[2024-02-27 12:53:46,626][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:53:46,626][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:53:46,626][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:53:46,626][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:53:46,627][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:53:46,627][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:53:46,627][PyLogger][INFO]: World size: 2
[2024-02-27 12:53:46,631][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:53:46,631][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:53:46,631][PyLogger][INFO]: World size: 2
[2024-02-27 12:53:51,826][PyLogger][INFO]: Rank[0]: val_accuracy: 0.93520
[2024-02-27 12:53:51,826][PyLogger][INFO]: Rank[1]: val_accuracy: 0.93520
2024-02-27 12:53:53,472 ignite.distributed.launcher.Parallel INFO: End of run
new_size=33 done.
2024-02-27 12:53:58,565 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:53:58,565 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:53:58,565 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x15170fafe3e0>' in 2 processes
2024-02-27 12:54:07,728 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:54:08,146 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14f1cb7f00b0>}
[2024-02-27 12:54:08,257][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:54:08,257][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:54:08,257][PyLogger][INFO]: World size: 2
[2024-02-27 12:54:08,265][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:54:08,265][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:54:08,265][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:54:08,265][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:54:08,266][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:54:08,266][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:54:08,266][PyLogger][INFO]: World size: 2
[2024-02-27 12:54:13,467][PyLogger][INFO]: Rank[1]: val_accuracy: 0.91380
[2024-02-27 12:54:13,467][PyLogger][INFO]: Rank[0]: val_accuracy: 0.91380
2024-02-27 12:54:14,597 ignite.distributed.launcher.Parallel INFO: End of run
new_size=34 done.
2024-02-27 12:54:20,649 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:54:20,649 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:54:20,649 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14bb27c6e3e0>' in 2 processes
2024-02-27 12:54:30,974 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:54:31,391 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14b54b9e4e30>}
[2024-02-27 12:54:31,511][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:54:31,511][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:54:31,511][PyLogger][INFO]: World size: 2
[2024-02-27 12:54:31,512][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:54:31,512][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:54:31,512][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:54:31,512][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:54:31,513][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:54:31,513][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:54:31,513][PyLogger][INFO]: World size: 2
[2024-02-27 12:54:36,740][PyLogger][INFO]: Rank[1]: val_accuracy: 0.93520
[2024-02-27 12:54:36,740][PyLogger][INFO]: Rank[0]: val_accuracy: 0.93520
2024-02-27 12:54:38,368 ignite.distributed.launcher.Parallel INFO: End of run
new_size=35 done.
2024-02-27 12:54:43,079 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'nccl'
2024-02-27 12:54:43,080 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: 
	nproc_per_node: 2
	nnodes: 1
	node_rank: 0
2024-02-27 12:54:43,080 ignite.distributed.launcher.Parallel INFO: Spawn function '<function evaluation at 0x14e8f90be3e0>' in 2 processes
2024-02-27 12:54:53,244 ignite.distributed.auto.auto_model INFO: Apply torch DistributedDataParallel on model, device id: 0
2024-02-27 12:54:53,674 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': 
	{'batch_size': 512, 'num_workers': 0, 'pin_memory': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x14d12dcec140>}
[2024-02-27 12:54:53,788][PyLogger][INFO]: GPU device[1]: Tesla P100-SXM2-16GB
[2024-02-27 12:54:53,788][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:54:53,788][PyLogger][INFO]: World size: 2
[2024-02-27 12:54:53,794][PyLogger][INFO]: PyTorch version: 2.2.0
[2024-02-27 12:54:53,794][PyLogger][INFO]: Ignite version: 0.4.13
[2024-02-27 12:54:53,794][PyLogger][INFO]: GPU device[0]: Tesla P100-SXM2-16GB
[2024-02-27 12:54:53,794][PyLogger][INFO]: CUDA version: 12.1
[2024-02-27 12:54:53,795][PyLogger][INFO]: CUDNN version: 8902
[2024-02-27 12:54:53,795][PyLogger][INFO]: Distributed backend: nccl
[2024-02-27 12:54:53,795][PyLogger][INFO]: World size: 2
[2024-02-27 12:54:58,987][PyLogger][INFO]: Rank[1]: val_accuracy: 0.86430
[2024-02-27 12:54:58,987][PyLogger][INFO]: Rank[0]: val_accuracy: 0.86430
2024-02-27 12:55:00,640 ignite.distributed.launcher.Parallel INFO: End of run
new_size=36 done.
